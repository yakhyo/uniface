{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#uniface","title":"UniFace","text":"<p>A lightweight, production-ready face analysis library built on ONNX Runtime</p> <p> </p> <p>Get Started View on GitHub</p>"},{"location":"#face-detection","title":"Face Detection","text":"<p>ONNX-optimized RetinaFace, SCRFD, and YOLOv5-Face models with 5-point landmarks.</p>"},{"location":"#face-recognition","title":"Face Recognition","text":"<p>ArcFace, MobileFace, and SphereFace embeddings for identity verification.</p>"},{"location":"#landmarks","title":"Landmarks","text":"<p>Accurate 106-point facial landmark localization for detailed face analysis.</p>"},{"location":"#attributes","title":"Attributes","text":"<p>Age, gender, race (FairFace), and emotion detection from faces.</p>"},{"location":"#face-parsing","title":"Face Parsing","text":"<p>BiSeNet semantic segmentation with 19 facial component classes.</p>"},{"location":"#gaze-estimation","title":"Gaze Estimation","text":"<p>Real-time gaze direction prediction with MobileGaze models.</p>"},{"location":"#anti-spoofing","title":"Anti-Spoofing","text":"<p>Face liveness detection with MiniFASNet to prevent fraud.</p>"},{"location":"#privacy","title":"Privacy","text":"<p>Face anonymization with 5 blur methods for privacy protection.</p>"},{"location":"#installation","title":"Installation","text":"StandardGPU (CUDA)From Source <pre><code>pip install uniface\n</code></pre> <pre><code>pip install uniface[gpu]\n</code></pre> <pre><code>git clone https://github.com/yakhyo/uniface.git\ncd uniface\npip install -e .\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":""},{"location":"#quickstart","title":"Quickstart","text":"<p>Get up and running in 5 minutes with common use cases.</p> <p>Quickstart Guide \u2192</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Step-by-step examples for common workflows.</p> <p>View Tutorials \u2192</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Explore individual modules and their APIs.</p> <p>Browse API \u2192</p>"},{"location":"#guides","title":"Guides","text":"<p>Learn about the architecture and design principles.</p> <p>Read Guides \u2192</p>"},{"location":"#license","title":"License","text":"<p>UniFace is released under the MIT License.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for contributing to UniFace!</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<pre><code># Clone\ngit clone https://github.com/yakhyo/uniface.git\ncd uniface\n\n# Install dev dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use Ruff for formatting:</p> <pre><code>ruff format .\nruff check . --fix\n</code></pre> <p>Guidelines:</p> <ul> <li>Line length: 120</li> <li>Python 3.11+ type hints</li> <li>Google-style docstrings</li> </ul>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code>pip install pre-commit\npre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Write tests for new features</li> <li>Ensure tests pass</li> <li>Submit PR with clear description</li> </ol>"},{"location":"contributing/#adding-new-models","title":"Adding New Models","text":"<ol> <li>Create model class in appropriate submodule</li> <li>Add weight constants to <code>uniface/constants.py</code></li> <li>Export in <code>__init__.py</code> files</li> <li>Write tests in <code>tests/</code></li> <li>Add example in <code>tools/</code> or notebooks</li> </ol>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue on GitHub.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers all installation options for UniFace.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.11 or higher</li> <li>Operating Systems: macOS, Linux, Windows</li> </ul>"},{"location":"installation/#quick-install","title":"Quick Install","text":"<p>The simplest way to install UniFace:</p> <pre><code>pip install uniface\n</code></pre> <p>This installs the CPU version with all core dependencies.</p>"},{"location":"installation/#platform-specific-installation","title":"Platform-Specific Installation","text":""},{"location":"installation/#macos-apple-silicon-m1m2m3m4","title":"macOS (Apple Silicon - M1/M2/M3/M4)","text":"<p>For Apple Silicon Macs, the standard installation automatically includes ARM64 optimizations:</p> <pre><code>pip install uniface\n</code></pre> <p>Native Performance</p> <p>The base <code>onnxruntime</code> package has native Apple Silicon support with ARM64 optimizations built-in since version 1.13+. No additional configuration needed.</p> <p>Verify ARM64 installation:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64\n</code></pre>"},{"location":"installation/#linuxwindows-with-nvidia-gpu","title":"Linux/Windows with NVIDIA GPU","text":"<p>For CUDA acceleration on NVIDIA GPUs:</p> <pre><code>pip install uniface[gpu]\n</code></pre> <p>Requirements:</p> <ul> <li>CUDA 11.x or 12.x</li> <li>cuDNN 8.x</li> </ul> <p>CUDA Compatibility</p> <p>See ONNX Runtime GPU requirements for detailed compatibility matrix.</p> <p>Verify GPU installation:</p> <pre><code>import onnxruntime as ort\nprint(\"Available providers:\", ort.get_available_providers())\n# Should include: 'CUDAExecutionProvider'\n</code></pre>"},{"location":"installation/#cpu-only-all-platforms","title":"CPU-Only (All Platforms)","text":"<pre><code>pip install uniface\n</code></pre> <p>Works on all platforms with automatic CPU fallback.</p>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"<p>For development or the latest features:</p> <pre><code>git clone https://github.com/yakhyo/uniface.git\ncd uniface\npip install -e .\n</code></pre> <p>With development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>UniFace has minimal dependencies:</p> Package Purpose <code>numpy</code> Array operations <code>opencv-python</code> Image processing <code>onnxruntime</code> Model inference <code>requests</code> Model download <code>tqdm</code> Progress bars"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation:</p> <pre><code>import uniface\nprint(f\"UniFace version: {uniface.__version__}\")\n\n# Check available ONNX providers\nimport onnxruntime as ort\nprint(f\"Available providers: {ort.get_available_providers()}\")\n\n# Quick test\nfrom uniface import RetinaFace\ndetector = RetinaFace()\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you're using Python 3.11+:</p> <pre><code>python --version\n# Should show: Python 3.11.x or higher\n</code></pre>"},{"location":"installation/#model-download-issues","title":"Model Download Issues","text":"<p>Models are automatically downloaded on first use. If downloads fail:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\n# Manually download a model\nmodel_path = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Model downloaded to: {model_path}\")\n</code></pre>"},{"location":"installation/#performance-issues-on-mac","title":"Performance Issues on Mac","text":"<p>Verify you're using the ARM64 build (not x86_64 via Rosetta):</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64 (not x86_64)\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Get started in 5 minutes</li> <li>Execution Providers - Hardware acceleration setup</li> </ul>"},{"location":"license-attribution/","title":"Licenses &amp; Attribution","text":""},{"location":"license-attribution/#uniface-license","title":"UniFace License","text":"<p>UniFace is released under the MIT License.</p>"},{"location":"license-attribution/#model-credits","title":"Model Credits","text":"Model Source License RetinaFace yakhyo/retinaface-pytorch MIT SCRFD InsightFace MIT YOLOv5-Face yakhyo/yolov5-face-onnx-inference GPL-3.0 ArcFace InsightFace MIT MobileFace yakhyo/face-recognition MIT SphereFace yakhyo/face-recognition MIT BiSeNet yakhyo/face-parsing MIT MobileGaze yakhyo/gaze-estimation MIT MiniFASNet yakhyo/face-anti-spoofing Apache-2.0 FairFace yakhyo/fairface-onnx CC BY 4.0"},{"location":"models/","title":"Model Zoo","text":"<p>Complete guide to all available models, their performance characteristics, and selection criteria.</p>"},{"location":"models/#face-detection-models","title":"Face Detection Models","text":""},{"location":"models/#retinaface-family","title":"RetinaFace Family","text":"<p>RetinaFace models are trained on the WIDER FACE dataset and provide excellent accuracy-speed tradeoffs.</p> Model Name Params Size Easy Medium Hard Use Case <code>MNET_025</code> 0.4M 1.7MB 88.48% 87.02% 80.61% Mobile/Edge devices <code>MNET_050</code> 1.0M 2.6MB 89.42% 87.97% 82.40% Mobile/Edge devices <code>MNET_V1</code> 3.5M 3.8MB 90.59% 89.14% 84.13% Balanced mobile <code>MNET_V2</code> 3.2M 3.5MB 91.70% 91.03% 86.60% Default <code>RESNET18</code> 11.7M 27MB 92.50% 91.02% 86.63% Server/High accuracy <code>RESNET34</code> 24.8M 56MB 94.16% 93.12% 88.90% Maximum accuracy <p>Accuracy &amp; Benchmarks</p> <p>Accuracy: WIDER FACE validation set (Easy/Medium/Hard subsets) - from RetinaFace paper</p> <p>Speed: Benchmark on your own hardware using <code>python tools/detection.py --source &lt;image&gt; --iterations 100</code></p>"},{"location":"models/#scrfd-family","title":"SCRFD Family","text":"<p>SCRFD (Sample and Computation Redistribution for Efficient Face Detection) models offer state-of-the-art speed-accuracy tradeoffs.</p> Model Name Params Size Easy Medium Hard Use Case <code>SCRFD_500M</code> 0.6M 2.5MB 90.57% 88.12% 68.51% Real-time applications <code>SCRFD_10G</code> 4.2M 17MB 95.16% 93.87% 83.05% High accuracy + speed <p>Accuracy &amp; Benchmarks</p> <p>Accuracy: WIDER FACE validation set - from SCRFD paper</p> <p>Speed: Benchmark on your own hardware using <code>python tools/detection.py --source &lt;image&gt; --iterations 100</code></p>"},{"location":"models/#yolov5-face-family","title":"YOLOv5-Face Family","text":"<p>YOLOv5-Face models provide excellent detection accuracy with 5-point facial landmarks, optimized for real-time applications.</p> Model Name Size Easy Medium Hard Use Case <code>YOLOV5N</code> 11MB 93.61% 91.52% 80.53% Lightweight/Mobile <code>YOLOV5S</code> 28MB 94.33% 92.61% 83.15% Real-time + accuracy <code>YOLOV5M</code> 82MB 95.30% 93.76% 85.28% High accuracy <p>Accuracy &amp; Benchmarks</p> <p>Accuracy: WIDER FACE validation set - from YOLOv5-Face paper</p> <p>Speed: Benchmark on your own hardware using <code>python tools/detection.py --source &lt;image&gt; --iterations 100</code></p> <p>Fixed Input Size</p> <p>All YOLOv5-Face models use a fixed input size of 640\u00d7640. Models exported to ONNX from deepcam-cn/yolov5-face.</p>"},{"location":"models/#face-recognition-models","title":"Face Recognition Models","text":""},{"location":"models/#arcface","title":"ArcFace","text":"<p>State-of-the-art face recognition using additive angular margin loss.</p> Model Name Backbone Params Size Use Case <code>MNET</code> MobileNet 2.0M 8MB Balanced (recommended) <code>RESNET</code> ResNet50 43.6M 166MB Maximum accuracy <p>Training Data</p> <p>Dataset: Trained on MS1M-V2 (5.8M images, 85K identities)</p> <p>Accuracy: Benchmark on your own dataset or use standard face verification benchmarks</p>"},{"location":"models/#mobileface","title":"MobileFace","text":"<p>Lightweight face recognition optimized for mobile devices.</p> Model Name Backbone Params Size LFW CALFW CPLFW AgeDB-30 Use Case <code>MNET_025</code> MobileNetV1 0.25 0.36M 1MB 98.76% 92.02% 82.37% 90.02% Ultra-lightweight <code>MNET_V2</code> MobileNetV2 2.29M 4MB 99.55% 94.87% 86.89% 95.16% Mobile/Edge <code>MNET_V3_SMALL</code> MobileNetV3-S 1.25M 3MB 99.30% 93.77% 85.29% 92.79% Mobile optimized <code>MNET_V3_LARGE</code> MobileNetV3-L 3.52M 10MB 99.53% 94.56% 86.79% 95.13% Balanced mobile <p>Training Data</p> <p>Dataset: Trained on MS1M-V2 (5.8M images, 85K identities)</p> <p>Accuracy: Evaluated on LFW, CALFW, CPLFW, and AgeDB-30 benchmarks</p> <p>Use Case</p> <p>These models are lightweight alternatives to ArcFace for resource-constrained environments.</p>"},{"location":"models/#sphereface","title":"SphereFace","text":"<p>Face recognition using angular softmax loss.</p> Model Name Backbone Params Size LFW CALFW CPLFW AgeDB-30 Use Case <code>SPHERE20</code> Sphere20 24.5M 50MB 99.67% 95.61% 88.75% 96.58% Research/Comparison <code>SPHERE36</code> Sphere36 34.6M 92MB 99.72% 95.64% 89.92% 96.83% Research/Comparison <p>Training Data</p> <p>Dataset: Trained on MS1M-V2 (5.8M images, 85K identities)</p> <p>Accuracy: Evaluated on LFW, CALFW, CPLFW, and AgeDB-30 benchmarks</p> <p>Architecture</p> <p>SphereFace uses angular softmax loss, an earlier approach before ArcFace. These models provide good accuracy with moderate resource requirements.</p>"},{"location":"models/#facial-landmark-models","title":"Facial Landmark Models","text":""},{"location":"models/#106-point-landmark-detection","title":"106-Point Landmark Detection","text":"<p>High-precision facial landmark localization.</p> Model Name Points Params Size Use Case <code>2D106</code> 106 3.7M 14MB Face alignment, analysis <p>Landmark Groups:</p> Group Points Count Face contour 0-32 33 points Eyebrows 33-50 18 points Nose 51-62 12 points Eyes 63-86 24 points Mouth 87-105 19 points"},{"location":"models/#attribute-analysis-models","title":"Attribute Analysis Models","text":""},{"location":"models/#age-gender-detection","title":"Age &amp; Gender Detection","text":"Model Name Attributes Params Size Use Case <code>AgeGender</code> Age, Gender 2.1M 8MB General purpose <p>Training Data</p> <p>Dataset: Trained on CelebA</p> <p>Accuracy Note</p> <p>Accuracy varies by demographic and image quality. Test on your specific use case.</p>"},{"location":"models/#fairface-attributes","title":"FairFace Attributes","text":"Model Name Attributes Params Size Use Case <code>FairFace</code> Race, Gender, Age Group - 44MB Balanced demographic prediction <p>Training Data</p> <p>Dataset: Trained on FairFace dataset with balanced demographics</p> <p>Equitable Predictions</p> <p>FairFace provides more equitable predictions across different racial and gender groups.</p> <p>Race Categories (7): White, Black, Latino Hispanic, East Asian, Southeast Asian, Indian, Middle Eastern</p> <p>Age Groups (9): 0-2, 3-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70+</p>"},{"location":"models/#emotion-detection","title":"Emotion Detection","text":"Model Name Classes Params Size Use Case <code>AFFECNET7</code> 7 0.5M 2MB 7-class emotion <code>AFFECNET8</code> 8 0.5M 2MB 8-class emotion <p>Classes (7): Neutral, Happy, Sad, Surprise, Fear, Disgust, Anger</p> <p>Classes (8): Above + Contempt</p> <p>Training Data</p> <p>Dataset: Trained on AffectNet</p> <p>Accuracy Note</p> <p>Emotion detection accuracy depends heavily on facial expression clarity and cultural context.</p>"},{"location":"models/#gaze-estimation-models","title":"Gaze Estimation Models","text":""},{"location":"models/#mobilegaze-family","title":"MobileGaze Family","text":"<p>Real-time gaze direction prediction models trained on Gaze360 dataset. Returns pitch (vertical) and yaw (horizontal) angles in radians.</p> Model Name Params Size MAE* Use Case <code>RESNET18</code> 11.7M 43 MB 12.84 Balanced accuracy/speed <code>RESNET34</code> 24.8M 81.6 MB 11.33 Default <code>RESNET50</code> 25.6M 91.3 MB 11.34 High accuracy <code>MOBILENET_V2</code> 3.5M 9.59 MB 13.07 Mobile/Edge devices <code>MOBILEONE_S0</code> 2.1M 4.8 MB 12.58 Lightweight/Real-time <p>*MAE (Mean Absolute Error) in degrees on Gaze360 test set - lower is better</p> <p>Training Data</p> <p>Dataset: Trained on Gaze360 (indoor/outdoor scenes with diverse head poses)</p> <p>Training: 200 epochs with classification-based approach (binned angles)</p> <p>Input Requirements</p> <p>Requires face crop as input. Use face detection first to obtain bounding boxes.</p>"},{"location":"models/#face-parsing-models","title":"Face Parsing Models","text":""},{"location":"models/#bisenet-family","title":"BiSeNet Family","text":"<p>BiSeNet (Bilateral Segmentation Network) models for semantic face parsing. Segments face images into 19 facial component classes.</p> Model Name Params Size Classes Use Case <code>RESNET18</code> 13.3M 50.7 MB 19 Default <code>RESNET34</code> 24.1M 89.2 MB 19 Higher accuracy <p>Training Data</p> <p>Dataset: Trained on CelebAMask-HQ</p> <p>Architecture: BiSeNet with ResNet backbone</p> <p>Input Size: 512\u00d7512 (automatically resized)</p> <p>19 Facial Component Classes:</p> # Class # Class # Class 1 Background 8 Left Ear 15 Neck 2 Skin 9 Right Ear 16 Neck Lace 3 Left Eyebrow 10 Ear Ring 17 Cloth 4 Right Eyebrow 11 Nose 18 Hair 5 Left Eye 12 Mouth 19 Hat 6 Right Eye 13 Upper Lip 7 Eye Glasses 14 Lower Lip <p>Applications:</p> <ul> <li>Face makeup and beauty applications</li> <li>Virtual try-on systems</li> <li>Face editing and manipulation</li> <li>Facial feature extraction</li> <li>Portrait segmentation</li> </ul> <p>Input Requirements</p> <p>Input should be a cropped face image. For full pipeline, use face detection first to obtain face crops.</p>"},{"location":"models/#anti-spoofing-models","title":"Anti-Spoofing Models","text":""},{"location":"models/#minifasnet-family","title":"MiniFASNet Family","text":"<p>Lightweight face anti-spoofing models for liveness detection. Detect if a face is real (live) or fake (photo, video replay, mask).</p> Model Name Size Scale Use Case <code>V1SE</code> 1.2 MB 4.0 Squeeze-and-excitation variant <code>V2</code> 1.2 MB 2.7 Default <p>Output Format</p> <p>Output: Returns <code>SpoofingResult(is_real, confidence)</code> where is_real: True=Real, False=Fake</p> <p>Input Requirements</p> <p>Requires face bounding box from a detector. Use with RetinaFace, SCRFD, or YOLOv5Face.</p>"},{"location":"models/#model-management","title":"Model Management","text":"<p>Models are automatically downloaded and cached on first use.</p> <ul> <li>Cache location: <code>~/.uniface/models/</code></li> <li>Verification: Models are verified with SHA-256 checksums</li> <li>Manual download: Use <code>python tools/download_model.py</code> to pre-download models</li> </ul>"},{"location":"models/#references","title":"References","text":""},{"location":"models/#model-training-architectures","title":"Model Training &amp; Architectures","text":"<ul> <li>RetinaFace Training: yakhyo/retinaface-pytorch - PyTorch implementation and training code</li> <li>YOLOv5-Face Original: deepcam-cn/yolov5-face - Original PyTorch implementation</li> <li>YOLOv5-Face ONNX: yakhyo/yolov5-face-onnx-inference - ONNX inference implementation</li> <li>Face Recognition Training: yakhyo/face-recognition - ArcFace, MobileFace, SphereFace training code</li> <li>Gaze Estimation Training: yakhyo/gaze-estimation - MobileGaze training code and pretrained weights</li> <li>Face Parsing Training: yakhyo/face-parsing - BiSeNet training code and pretrained weights</li> <li>Face Anti-Spoofing: yakhyo/face-anti-spoofing - MiniFASNet ONNX inference (weights from minivision-ai/Silent-Face-Anti-Spoofing)</li> <li>FairFace: yakhyo/fairface-onnx - FairFace ONNX inference for race, gender, age prediction</li> <li>InsightFace: deepinsight/insightface - Model architectures and pretrained weights</li> </ul>"},{"location":"models/#papers","title":"Papers","text":"<ul> <li>RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild</li> <li>SCRFD: Sample and Computation Redistribution for Efficient Face Detection</li> <li>YOLOv5-Face: YOLO5Face: Why Reinventing a Face Detector</li> <li>ArcFace: Additive Angular Margin Loss for Deep Face Recognition</li> <li>SphereFace: Deep Hypersphere Embedding for Face Recognition</li> <li>BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Get up and running with UniFace in 5 minutes. This guide covers the most common use cases.</p>"},{"location":"quickstart/#face-detection","title":"Face Detection","text":"<p>Detect faces in an image:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\n\n# Load image\nimage = cv2.imread(\"photo.jpg\")\n\n# Initialize detector (models auto-download on first use)\ndetector = RetinaFace()\n\n# Detect faces\nfaces = detector.detect(image)\n\n# Print results\nfor i, face in enumerate(faces):\n    print(f\"Face {i+1}:\")\n    print(f\"  Confidence: {face.confidence:.2f}\")\n    print(f\"  BBox: {face.bbox}\")\n    print(f\"  Landmarks: {len(face.landmarks)} points\")\n</code></pre> <p>Output:</p> <pre><code>Face 1:\n  Confidence: 0.99\n  BBox: [120.5, 85.3, 245.8, 210.6]\n  Landmarks: 5 points\n</code></pre>"},{"location":"quickstart/#visualize-detections","title":"Visualize Detections","text":"<p>Draw bounding boxes and landmarks:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.visualization import draw_detections\n\n# Detect faces\ndetector = RetinaFace()\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\n# Extract visualization data\nbboxes = [f.bbox for f in faces]\nscores = [f.confidence for f in faces]\nlandmarks = [f.landmarks for f in faces]\n\n# Draw on image\ndraw_detections(\n    image=image,\n    bboxes=bboxes,\n    scores=scores,\n    landmarks=landmarks,\n    vis_threshold=0.6,\n)\n\n# Save result\ncv2.imwrite(\"output.jpg\", image)\n</code></pre>"},{"location":"quickstart/#face-recognition","title":"Face Recognition","text":"<p>Compare two faces:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, ArcFace\n\n# Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Load two images\nimage1 = cv2.imread(\"person1.jpg\")\nimage2 = cv2.imread(\"person2.jpg\")\n\n# Detect faces\nfaces1 = detector.detect(image1)\nfaces2 = detector.detect(image2)\n\nif faces1 and faces2:\n    # Extract embeddings\n    emb1 = recognizer.get_normalized_embedding(image1, faces1[0].landmarks)\n    emb2 = recognizer.get_normalized_embedding(image2, faces2[0].landmarks)\n\n    # Compute similarity (cosine similarity)\n    similarity = np.dot(emb1, emb2.T)[0][0]\n\n    # Interpret result\n    if similarity &gt; 0.6:\n        print(f\"Same person (similarity: {similarity:.3f})\")\n    else:\n        print(f\"Different people (similarity: {similarity:.3f})\")\n</code></pre> <p>Similarity Thresholds</p> <ul> <li><code>&gt; 0.6</code>: Same person (high confidence)</li> <li><code>0.4 - 0.6</code>: Uncertain (manual review)</li> <li><code>&lt; 0.4</code>: Different people</li> </ul>"},{"location":"quickstart/#age-gender-detection","title":"Age &amp; Gender Detection","text":"<pre><code>import cv2\nfrom uniface import RetinaFace, AgeGender\n\n# Initialize models\ndetector = RetinaFace()\nage_gender = AgeGender()\n\n# Load image\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\n# Predict attributes\nfor i, face in enumerate(faces):\n    result = age_gender.predict(image, face.bbox)\n    print(f\"Face {i+1}: {result.sex}, {result.age} years old\")\n</code></pre> <p>Output:</p> <pre><code>Face 1: Male, 32 years old\nFace 2: Female, 28 years old\n</code></pre>"},{"location":"quickstart/#fairface-attributes","title":"FairFace Attributes","text":"<p>Detect race, gender, and age group:</p> <pre><code>import cv2\nfrom uniface import RetinaFace, FairFace\n\ndetector = RetinaFace()\nfairface = FairFace()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    result = fairface.predict(image, face.bbox)\n    print(f\"Face {i+1}: {result.sex}, {result.age_group}, {result.race}\")\n</code></pre> <p>Output:</p> <pre><code>Face 1: Male, 30-39, East Asian\nFace 2: Female, 20-29, White\n</code></pre>"},{"location":"quickstart/#facial-landmarks-106-points","title":"Facial Landmarks (106 Points)","text":"<pre><code>import cv2\nfrom uniface import RetinaFace, Landmark106\n\ndetector = RetinaFace()\nlandmarker = Landmark106()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nif faces:\n    landmarks = landmarker.get_landmarks(image, faces[0].bbox)\n    print(f\"Detected {len(landmarks)} landmarks\")\n\n    # Draw landmarks\n    for x, y in landmarks.astype(int):\n        cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n\n    cv2.imwrite(\"landmarks.jpg\", image)\n</code></pre>"},{"location":"quickstart/#gaze-estimation","title":"Gaze Estimation","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, MobileGaze\nfrom uniface.visualization import draw_gaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    x1, y1, x2, y2 = map(int, face.bbox[:4])\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        result = gaze_estimator.estimate(face_crop)\n        print(f\"Face {i+1}: pitch={np.degrees(result.pitch):.1f}\u00b0, yaw={np.degrees(result.yaw):.1f}\u00b0\")\n\n        # Draw gaze direction\n        draw_gaze(image, face.bbox, result.pitch, result.yaw)\n\ncv2.imwrite(\"gaze_output.jpg\", image)\n</code></pre>"},{"location":"quickstart/#face-parsing","title":"Face Parsing","text":"<p>Segment face into semantic components:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface.parsing import BiSeNet\nfrom uniface.visualization import vis_parsing_maps\n\nparser = BiSeNet()\n\n# Load face image (already cropped)\nface_image = cv2.imread(\"face.jpg\")\n\n# Parse face into 19 components\nmask = parser.parse(face_image)\n\n# Visualize with overlay\nface_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\nvis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\nprint(f\"Detected {len(np.unique(mask))} facial components\")\n</code></pre>"},{"location":"quickstart/#face-anonymization","title":"Face Anonymization","text":"<p>Blur faces for privacy protection:</p> <pre><code>from uniface.privacy import anonymize_faces\nimport cv2\n\n# One-liner: automatic detection and blurring\nimage = cv2.imread(\"group_photo.jpg\")\nanonymized = anonymize_faces(image, method='pixelate')\ncv2.imwrite(\"anonymized.jpg\", anonymized)\n</code></pre> <p>Manual control:</p> <pre><code>from uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian', blur_strength=5.0)\n\nfaces = detector.detect(image)\nanonymized = blurrer.anonymize(image, faces)\n</code></pre> <p>Available methods:</p> Method Description <code>pixelate</code> Blocky effect (news media standard) <code>gaussian</code> Smooth, natural blur <code>blackout</code> Solid color boxes (maximum privacy) <code>elliptical</code> Soft oval blur (natural face shape) <code>median</code> Edge-preserving blur"},{"location":"quickstart/#face-anti-spoofing","title":"Face Anti-Spoofing","text":"<p>Detect real vs. fake faces:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    result = spoofer.predict(image, face.bbox)\n    label = 'Real' if result.is_real else 'Fake'\n    print(f\"Face {i+1}: {label} ({result.confidence:.1%})\")\n</code></pre>"},{"location":"quickstart/#webcam-demo","title":"Webcam Demo","text":"<p>Real-time face detection:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.visualization import draw_detections\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\nprint(\"Press 'q' to quit\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    bboxes = [f.bbox for f in faces]\n    scores = [f.confidence for f in faces]\n    landmarks = [f.landmarks for f in faces]\n    draw_detections(image=frame, bboxes=bboxes, scores=scores, landmarks=landmarks)\n\n    cv2.imshow(\"UniFace - Press 'q' to quit\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"quickstart/#model-selection","title":"Model Selection","text":"<p>For detailed model comparisons, benchmarks, and selection guidance, see the Model Zoo.</p> <p>Quick recommendations:</p> Task Recommended Model Alternative Detection (balanced) <code>RetinaFace</code> (MNET_V2) <code>YOLOv5Face</code> (YOLOV5S) Detection (speed) <code>RetinaFace</code> (MNET_025) <code>SCRFD</code> (SCRFD_500M) Detection (accuracy) <code>SCRFD</code> (SCRFD_10G) <code>RetinaFace</code> (RESNET34) Recognition <code>ArcFace</code> (MNET) <code>MobileFace</code> (MNET_V2) Gaze <code>MobileGaze</code> (RESNET34) <code>MobileGaze</code> (MOBILEONE_S0) Parsing <code>BiSeNet</code> (RESNET18) <code>BiSeNet</code> (RESNET34)"},{"location":"quickstart/#common-issues","title":"Common Issues","text":""},{"location":"quickstart/#models-not-downloading","title":"Models Not Downloading","text":"<pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\n# Manually download a model\nmodel_path = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Model downloaded to: {model_path}\")\n</code></pre>"},{"location":"quickstart/#check-hardware-acceleration","title":"Check Hardware Acceleration","text":"<pre><code>import onnxruntime as ort\nprint(\"Available providers:\", ort.get_available_providers())\n\n# macOS M-series should show: ['CoreMLExecutionProvider', ...]\n# NVIDIA GPU should show: ['CUDAExecutionProvider', ...]\n</code></pre>"},{"location":"quickstart/#slow-performance-on-mac","title":"Slow Performance on Mac","text":"<p>Verify you're using the ARM64 build of Python:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64 (not x86_64)\n</code></pre>"},{"location":"quickstart/#import-errors","title":"Import Errors","text":"<pre><code># Correct imports\nfrom uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\nfrom uniface.landmark import Landmark106\n\n# Also works (re-exported at package level)\nfrom uniface import RetinaFace, ArcFace, Landmark106\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Model Zoo - All models, benchmarks, and selection guide</li> <li>API Reference - Explore individual modules and their APIs</li> <li>Tutorials - Step-by-step examples for common workflows</li> <li>Guides - Learn about the architecture and design principles</li> </ul>"},{"location":"concepts/coordinate-systems/","title":"Coordinate Systems","text":"<p>This page explains the coordinate formats used in UniFace.</p>"},{"location":"concepts/coordinate-systems/#image-coordinates","title":"Image Coordinates","text":"<p>All coordinates use pixel-based, top-left origin:</p> <pre><code>(0, 0) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba x (width)\n   \u2502\n   \u2502    Image\n   \u2502\n   \u25bc\n   y (height)\n</code></pre>"},{"location":"concepts/coordinate-systems/#bounding-box-format","title":"Bounding Box Format","text":"<p>Bounding boxes use <code>[x1, y1, x2, y2]</code> format (top-left and bottom-right corners):</p> <pre><code>(x1, y1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                     \u2502\n    \u2502      Face           \u2502\n    \u2502                     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 (x2, y2)\n</code></pre>"},{"location":"concepts/coordinate-systems/#accessing-coordinates","title":"Accessing Coordinates","text":"<pre><code>face = faces[0]\n\n# Direct access\nx1, y1, x2, y2 = face.bbox\n\n# As properties\nbbox_xyxy = face.bbox_xyxy  # [x1, y1, x2, y2]\nbbox_xywh = face.bbox_xywh  # [x1, y1, width, height]\n</code></pre>"},{"location":"concepts/coordinate-systems/#conversion","title":"Conversion","text":"<pre><code>import numpy as np\n\n# xyxy \u2192 xywh\ndef xyxy_to_xywh(bbox):\n    x1, y1, x2, y2 = bbox\n    return np.array([x1, y1, x2 - x1, y2 - y1])\n\n# xywh \u2192 xyxy\ndef xywh_to_xyxy(bbox):\n    x, y, w, h = bbox\n    return np.array([x, y, x + w, y + h])\n</code></pre>"},{"location":"concepts/coordinate-systems/#landmarks","title":"Landmarks","text":""},{"location":"concepts/coordinate-systems/#5-point-landmarks-detection","title":"5-Point Landmarks (Detection)","text":"<p>Returned by all detection models:</p> <pre><code>landmarks = face.landmarks  # Shape: (5, 2)\n</code></pre> Index Point 0 Left Eye 1 Right Eye 2 Nose Tip 3 Left Mouth Corner 4 Right Mouth Corner <pre><code>      0 \u25cf           \u25cf 1\n\n            \u25cf 2\n\n        3 \u25cf     \u25cf 4\n</code></pre>"},{"location":"concepts/coordinate-systems/#106-point-landmarks","title":"106-Point Landmarks","text":"<p>Returned by <code>Landmark106</code>:</p> <pre><code>from uniface import Landmark106\n\nlandmarker = Landmark106()\nlandmarks = landmarker.get_landmarks(image, face.bbox)\n# Shape: (106, 2)\n</code></pre> <p>Landmark Groups:</p> Range Group Points 0-32 Face Contour 33 33-50 Eyebrows 18 51-62 Nose 12 63-86 Eyes 24 87-105 Mouth 19"},{"location":"concepts/coordinate-systems/#face-crop","title":"Face Crop","text":"<p>To crop a face from an image:</p> <pre><code>def crop_face(image, bbox, margin=0):\n    \"\"\"Crop face with optional margin.\"\"\"\n    h, w = image.shape[:2]\n    x1, y1, x2, y2 = map(int, bbox)\n\n    # Add margin\n    if margin &gt; 0:\n        bw, bh = x2 - x1, y2 - y1\n        x1 = max(0, x1 - int(bw * margin))\n        y1 = max(0, y1 - int(bh * margin))\n        x2 = min(w, x2 + int(bw * margin))\n        y2 = min(h, y2 + int(bh * margin))\n\n    return image[y1:y2, x1:x2]\n\n# Usage\nface_crop = crop_face(image, face.bbox, margin=0.1)\n</code></pre>"},{"location":"concepts/coordinate-systems/#gaze-angles","title":"Gaze Angles","text":"<p>Gaze estimation returns pitch and yaw in radians:</p> <pre><code>result = gaze_estimator.estimate(face_crop)\n\n# Angles in radians\npitch = result.pitch  # Vertical: + = up, - = down\nyaw = result.yaw      # Horizontal: + = right, - = left\n\n# Convert to degrees\nimport numpy as np\npitch_deg = np.degrees(pitch)\nyaw_deg = np.degrees(yaw)\n</code></pre> <p>Angle Reference:</p> <pre><code>          pitch = +90\u00b0 (up)\n               \u2502\n               \u2502\nyaw = -90\u00b0 \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500 yaw = +90\u00b0\n(left)         \u2502      (right)\n               \u2502\n          pitch = -90\u00b0 (down)\n</code></pre>"},{"location":"concepts/coordinate-systems/#face-alignment","title":"Face Alignment","text":"<p>Face alignment uses 5-point landmarks to normalize face orientation:</p> <pre><code>from uniface import face_alignment\n\n# Align face to standard template\naligned_face = face_alignment(image, face.landmarks)\n# Output: 112x112 aligned face image\n</code></pre> <p>The alignment transforms faces to a canonical pose for better recognition accuracy.</p>"},{"location":"concepts/coordinate-systems/#next-steps","title":"Next Steps","text":"<ul> <li>Inputs &amp; Outputs - Data types reference</li> <li>Recognition Module - Face recognition details</li> </ul>"},{"location":"concepts/execution-providers/","title":"Execution Providers","text":"<p>UniFace uses ONNX Runtime for model inference, which supports multiple hardware acceleration backends.</p>"},{"location":"concepts/execution-providers/#automatic-provider-selection","title":"Automatic Provider Selection","text":"<p>UniFace automatically selects the optimal execution provider based on available hardware:</p> <pre><code>from uniface import RetinaFace\n\n# Automatically uses best available provider\ndetector = RetinaFace()\n</code></pre> <p>Priority order:</p> <ol> <li>CUDAExecutionProvider - NVIDIA GPU</li> <li>CoreMLExecutionProvider - Apple Silicon</li> <li>CPUExecutionProvider - Fallback</li> </ol>"},{"location":"concepts/execution-providers/#check-available-providers","title":"Check Available Providers","text":"<pre><code>import onnxruntime as ort\n\nproviders = ort.get_available_providers()\nprint(\"Available providers:\", providers)\n</code></pre> <p>Example outputs:</p> macOS (Apple Silicon)Linux (NVIDIA GPU)Windows (CPU) <pre><code>['CoreMLExecutionProvider', 'CPUExecutionProvider']\n</code></pre> <pre><code>['CUDAExecutionProvider', 'CPUExecutionProvider']\n</code></pre> <pre><code>['CPUExecutionProvider']\n</code></pre>"},{"location":"concepts/execution-providers/#platform-specific-setup","title":"Platform-Specific Setup","text":""},{"location":"concepts/execution-providers/#apple-silicon-m1m2m3m4","title":"Apple Silicon (M1/M2/M3/M4)","text":"<p>No additional setup required. ARM64 optimizations are built into <code>onnxruntime</code>:</p> <pre><code>pip install uniface\n</code></pre> <p>Verify ARM64:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64\n</code></pre> <p>Performance</p> <p>Apple Silicon Macs use CoreML acceleration automatically, providing excellent performance for face analysis tasks.</p>"},{"location":"concepts/execution-providers/#nvidia-gpu-cuda","title":"NVIDIA GPU (CUDA)","text":"<p>Install with GPU support:</p> <pre><code>pip install uniface[gpu]\n</code></pre> <p>Requirements:</p> <ul> <li>CUDA 11.x or 12.x</li> <li>cuDNN 8.x</li> <li>Compatible NVIDIA driver</li> </ul> <p>Verify CUDA:</p> <pre><code>import onnxruntime as ort\n\nif 'CUDAExecutionProvider' in ort.get_available_providers():\n    print(\"CUDA is available!\")\nelse:\n    print(\"CUDA not available, using CPU\")\n</code></pre>"},{"location":"concepts/execution-providers/#cpu-fallback","title":"CPU Fallback","text":"<p>CPU execution is always available:</p> <pre><code>pip install uniface\n</code></pre> <p>Works on all platforms without additional configuration.</p>"},{"location":"concepts/execution-providers/#internal-api","title":"Internal API","text":"<p>For advanced use cases, you can access the provider utilities:</p> <pre><code>from uniface.onnx_utils import get_available_providers, create_onnx_session\n\n# Check available providers\nproviders = get_available_providers()\nprint(f\"Available: {providers}\")\n\n# Models use create_onnx_session() internally\n# which auto-selects the best provider\n</code></pre>"},{"location":"concepts/execution-providers/#performance-tips","title":"Performance Tips","text":""},{"location":"concepts/execution-providers/#1-use-gpu-when-available","title":"1. Use GPU When Available","text":"<p>For batch processing or real-time applications, GPU acceleration provides significant speedups:</p> <pre><code>pip install uniface[gpu]\n</code></pre>"},{"location":"concepts/execution-providers/#2-optimize-input-size","title":"2. Optimize Input Size","text":"<p>Smaller input sizes are faster but may reduce accuracy:</p> <pre><code>from uniface import RetinaFace\n\n# Faster, lower accuracy\ndetector = RetinaFace(input_size=(320, 320))\n\n# Balanced (default)\ndetector = RetinaFace(input_size=(640, 640))\n</code></pre>"},{"location":"concepts/execution-providers/#3-batch-processing","title":"3. Batch Processing","text":"<p>Process multiple images to maximize GPU utilization:</p> <pre><code># Process images in batch (GPU-efficient)\nfor image_path in image_paths:\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n    # ...\n</code></pre>"},{"location":"concepts/execution-providers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/execution-providers/#cuda-not-detected","title":"CUDA Not Detected","text":"<ol> <li> <p>Verify CUDA installation:    <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Check CUDA version compatibility with ONNX Runtime</p> </li> <li> <p>Reinstall with GPU support:    <pre><code>pip uninstall onnxruntime onnxruntime-gpu\npip install uniface[gpu]\n</code></pre></p> </li> </ol>"},{"location":"concepts/execution-providers/#slow-performance-on-mac","title":"Slow Performance on Mac","text":"<p>Verify you're using ARM64 Python (not Rosetta):</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64 (not x86_64)\n</code></pre>"},{"location":"concepts/execution-providers/#next-steps","title":"Next Steps","text":"<ul> <li>Model Cache &amp; Offline - Model management</li> <li>Thresholds &amp; Calibration - Tuning parameters</li> </ul>"},{"location":"concepts/inputs-outputs/","title":"Inputs &amp; Outputs","text":"<p>This page describes the data types used throughout UniFace.</p>"},{"location":"concepts/inputs-outputs/#input-images","title":"Input: Images","text":"<p>All models accept NumPy arrays in BGR format (OpenCV default):</p> <pre><code>import cv2\n\n# Load image (BGR format)\nimage = cv2.imread(\"photo.jpg\")\nprint(f\"Shape: {image.shape}\")  # (H, W, 3)\nprint(f\"Dtype: {image.dtype}\")  # uint8\n</code></pre> <p>Color Format</p> <p>UniFace expects BGR format (OpenCV default). If using PIL or other libraries, convert first:</p> <pre><code>from PIL import Image\nimport numpy as np\n\npil_image = Image.open(\"photo.jpg\")\nbgr_image = np.array(pil_image)[:, :, ::-1]  # RGB \u2192 BGR\n</code></pre>"},{"location":"concepts/inputs-outputs/#output-face-dataclass","title":"Output: Face Dataclass","text":"<p>Detection returns a list of <code>Face</code> objects:</p> <pre><code>from dataclasses import dataclass\nimport numpy as np\n\n@dataclass\nclass Face:\n    # Required (from detection)\n    bbox: np.ndarray        # [x1, y1, x2, y2]\n    confidence: float       # 0.0 to 1.0\n    landmarks: np.ndarray   # (5, 2) or (106, 2)\n\n    # Optional (enriched by analyzers)\n    embedding: np.ndarray | None = None\n    gender: int | None = None           # 0=Female, 1=Male\n    age: int | None = None              # Years\n    age_group: str | None = None        # \"20-29\", etc.\n    race: str | None = None             # \"East Asian\", etc.\n    emotion: str | None = None          # \"Happy\", etc.\n    emotion_confidence: float | None = None\n</code></pre>"},{"location":"concepts/inputs-outputs/#properties","title":"Properties","text":"<pre><code>face = faces[0]\n\n# Bounding box formats\nface.bbox_xyxy  # [x1, y1, x2, y2] - same as bbox\nface.bbox_xywh  # [x1, y1, width, height]\n\n# Gender as string\nface.sex  # \"Female\" or \"Male\" (None if not predicted)\n</code></pre>"},{"location":"concepts/inputs-outputs/#methods","title":"Methods","text":"<pre><code># Compute similarity with another face\nsimilarity = face1.compute_similarity(face2)\n\n# Convert to dictionary\nface_dict = face.to_dict()\n</code></pre>"},{"location":"concepts/inputs-outputs/#result-types","title":"Result Types","text":""},{"location":"concepts/inputs-outputs/#gazeresult","title":"GazeResult","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass GazeResult:\n    pitch: float  # Vertical angle (radians), + = up\n    yaw: float    # Horizontal angle (radians), + = right\n</code></pre> <p>Usage:</p> <pre><code>import numpy as np\n\nresult = gaze_estimator.estimate(face_crop)\nprint(f\"Pitch: {np.degrees(result.pitch):.1f}\u00b0\")\nprint(f\"Yaw: {np.degrees(result.yaw):.1f}\u00b0\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#spoofingresult","title":"SpoofingResult","text":"<pre><code>@dataclass(frozen=True)\nclass SpoofingResult:\n    is_real: bool      # True = real, False = fake\n    confidence: float  # 0.0 to 1.0\n</code></pre> <p>Usage:</p> <pre><code>result = spoofer.predict(image, face.bbox)\nlabel = \"Real\" if result.is_real else \"Fake\"\nprint(f\"{label}: {result.confidence:.1%}\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#attributeresult","title":"AttributeResult","text":"<pre><code>@dataclass(frozen=True)\nclass AttributeResult:\n    gender: int              # 0=Female, 1=Male\n    age: int | None          # Years (AgeGender model)\n    age_group: str | None    # \"20-29\" (FairFace model)\n    race: str | None         # Race label (FairFace model)\n\n    @property\n    def sex(self) -&gt; str:\n        return \"Female\" if self.gender == 0 else \"Male\"\n</code></pre> <p>Usage:</p> <pre><code># AgeGender model\nresult = age_gender.predict(image, face.bbox)\nprint(f\"{result.sex}, {result.age} years old\")\n\n# FairFace model\nresult = fairface.predict(image, face.bbox)\nprint(f\"{result.sex}, {result.age_group}, {result.race}\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#emotionresult","title":"EmotionResult","text":"<pre><code>@dataclass(frozen=True)\nclass EmotionResult:\n    emotion: str       # \"Happy\", \"Sad\", etc.\n    confidence: float  # 0.0 to 1.0\n</code></pre>"},{"location":"concepts/inputs-outputs/#embeddings","title":"Embeddings","text":"<p>Face recognition models return normalized 512-dimensional embeddings:</p> <pre><code>embedding = recognizer.get_normalized_embedding(image, landmarks)\nprint(f\"Shape: {embedding.shape}\")  # (1, 512)\nprint(f\"Norm: {np.linalg.norm(embedding):.4f}\")  # ~1.0\n</code></pre>"},{"location":"concepts/inputs-outputs/#similarity-computation","title":"Similarity Computation","text":"<pre><code>from uniface import compute_similarity\n\nsimilarity = compute_similarity(embedding1, embedding2)\n# Returns: float between -1 and 1 (cosine similarity)\n</code></pre>"},{"location":"concepts/inputs-outputs/#parsing-masks","title":"Parsing Masks","text":"<p>Face parsing returns a segmentation mask:</p> <pre><code>mask = parser.parse(face_image)\nprint(f\"Shape: {mask.shape}\")  # (H, W)\nprint(f\"Classes: {np.unique(mask)}\")  # [0, 1, 2, ...]\n</code></pre> <p>19 Classes:</p> ID Class ID Class 0 Background 10 Ear Ring 1 Skin 11 Nose 2 Left Eyebrow 12 Mouth 3 Right Eyebrow 13 Upper Lip 4 Left Eye 14 Lower Lip 5 Right Eye 15 Neck 6 Eye Glasses 16 Neck Lace 7 Left Ear 17 Cloth 8 Right Ear 18 Hair 9 Hat"},{"location":"concepts/inputs-outputs/#next-steps","title":"Next Steps","text":"<ul> <li>Coordinate Systems - Bbox and landmark formats</li> <li>Thresholds &amp; Calibration - Tuning confidence thresholds</li> </ul>"},{"location":"concepts/model-cache-offline/","title":"Model Cache &amp; Offline Use","text":"<p>UniFace automatically downloads and caches models. This page explains how model management works.</p>"},{"location":"concepts/model-cache-offline/#automatic-download","title":"Automatic Download","text":"<p>Models are downloaded on first use:</p> <pre><code>from uniface import RetinaFace\n\n# First run: downloads model to cache\ndetector = RetinaFace()  # ~3.5 MB download\n\n# Subsequent runs: loads from cache\ndetector = RetinaFace()  # Instant\n</code></pre>"},{"location":"concepts/model-cache-offline/#cache-location","title":"Cache Location","text":"<p>Default cache directory:</p> <pre><code>~/.uniface/models/\n</code></pre> <p>Example structure:</p> <pre><code>~/.uniface/models/\n\u251c\u2500\u2500 retinaface_mv2.onnx\n\u251c\u2500\u2500 w600k_mbf.onnx\n\u251c\u2500\u2500 2d106det.onnx\n\u251c\u2500\u2500 gaze_resnet34.onnx\n\u251c\u2500\u2500 parsing_resnet18.onnx\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"concepts/model-cache-offline/#custom-cache-directory","title":"Custom Cache Directory","text":"<p>Specify a custom cache location:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\n# Download to custom directory\nmodel_path = verify_model_weights(\n    RetinaFaceWeights.MNET_V2,\n    root='./my_models'\n)\nprint(f\"Model at: {model_path}\")\n</code></pre>"},{"location":"concepts/model-cache-offline/#pre-download-models","title":"Pre-Download Models","text":"<p>Download models before deployment:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import (\n    RetinaFaceWeights,\n    ArcFaceWeights,\n    AgeGenderWeights,\n)\n\n# Download all needed models\nmodels = [\n    RetinaFaceWeights.MNET_V2,\n    ArcFaceWeights.MNET,\n    AgeGenderWeights.DEFAULT,\n]\n\nfor model in models:\n    path = verify_model_weights(model)\n    print(f\"Downloaded: {path}\")\n</code></pre> <p>Or use the CLI tool:</p> <pre><code>python tools/download_model.py\n</code></pre>"},{"location":"concepts/model-cache-offline/#offline-use","title":"Offline Use","text":"<p>For air-gapped or offline environments:</p>"},{"location":"concepts/model-cache-offline/#1-pre-download-models","title":"1. Pre-download models","text":"<p>On a connected machine:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\npath = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Copy from: {path}\")\n</code></pre>"},{"location":"concepts/model-cache-offline/#2-copy-to-target-machine","title":"2. Copy to target machine","text":"<pre><code># Copy the entire cache directory\nscp -r ~/.uniface/models/ user@offline-machine:~/.uniface/models/\n</code></pre>"},{"location":"concepts/model-cache-offline/#3-use-normally","title":"3. Use normally","text":"<pre><code># Models load from local cache\nfrom uniface import RetinaFace\ndetector = RetinaFace()  # No network required\n</code></pre>"},{"location":"concepts/model-cache-offline/#model-verification","title":"Model Verification","text":"<p>Models are verified with SHA-256 checksums:</p> <pre><code>from uniface.constants import MODEL_SHA256, RetinaFaceWeights\n\n# Check expected checksum\nexpected = MODEL_SHA256[RetinaFaceWeights.MNET_V2]\nprint(f\"Expected SHA256: {expected}\")\n</code></pre> <p>If a model fails verification, it's re-downloaded automatically.</p>"},{"location":"concepts/model-cache-offline/#available-models","title":"Available Models","text":""},{"location":"concepts/model-cache-offline/#detection-models","title":"Detection Models","text":"Model Size Download RetinaFace MNET_025 1.7 MB \u2705 RetinaFace MNET_V2 3.5 MB \u2705 RetinaFace RESNET34 56 MB \u2705 SCRFD 500M 2.5 MB \u2705 SCRFD 10G 17 MB \u2705 YOLOv5n-Face 11 MB \u2705 YOLOv5s-Face 28 MB \u2705 YOLOv5m-Face 82 MB \u2705"},{"location":"concepts/model-cache-offline/#recognition-models","title":"Recognition Models","text":"Model Size Download ArcFace MNET 8 MB \u2705 ArcFace RESNET 166 MB \u2705 MobileFace MNET_V2 4 MB \u2705 SphereFace SPHERE20 50 MB \u2705"},{"location":"concepts/model-cache-offline/#other-models","title":"Other Models","text":"Model Size Download Landmark106 14 MB \u2705 AgeGender 8 MB \u2705 FairFace 44 MB \u2705 Gaze ResNet34 82 MB \u2705 BiSeNet ResNet18 51 MB \u2705 MiniFASNet V2 1.2 MB \u2705"},{"location":"concepts/model-cache-offline/#clear-cache","title":"Clear Cache","text":"<p>Remove cached models:</p> <pre><code># Remove all cached models\nrm -rf ~/.uniface/models/\n\n# Remove specific model\nrm ~/.uniface/models/retinaface_mv2.onnx\n</code></pre> <p>Models will be re-downloaded on next use.</p>"},{"location":"concepts/model-cache-offline/#environment-variables","title":"Environment Variables","text":"<p>Set custom cache location via environment variable:</p> <pre><code>export UNIFACE_CACHE_DIR=/path/to/custom/cache\n</code></pre> <pre><code>import os\nos.environ['UNIFACE_CACHE_DIR'] = '/path/to/custom/cache'\n\nfrom uniface import RetinaFace\ndetector = RetinaFace()  # Uses custom cache\n</code></pre>"},{"location":"concepts/model-cache-offline/#next-steps","title":"Next Steps","text":"<ul> <li>Thresholds &amp; Calibration - Tune model parameters</li> <li>Detection Module - Detection model details</li> </ul>"},{"location":"concepts/overview/","title":"Overview","text":"<p>UniFace is designed as a modular, production-ready face analysis library. This page explains the architecture and design principles.</p>"},{"location":"concepts/overview/#architecture","title":"Architecture","text":"<p>UniFace follows a modular architecture where each face analysis task is handled by a dedicated module:</p> <pre><code>graph TB\n    subgraph Input\n        IMG[Image/Frame]\n    end\n\n    subgraph Detection\n        DET[RetinaFace / SCRFD / YOLOv5Face]\n    end\n\n    subgraph Analysis\n        REC[Recognition]\n        LMK[Landmarks]\n        ATTR[Attributes]\n        GAZE[Gaze]\n        PARSE[Parsing]\n        SPOOF[Anti-Spoofing]\n        PRIV[Privacy]\n    end\n\n    subgraph Output\n        FACE[Face Objects]\n    end\n\n    IMG --&gt; DET\n    DET --&gt; REC\n    DET --&gt; LMK\n    DET --&gt; ATTR\n    DET --&gt; GAZE\n    DET --&gt; PARSE\n    DET --&gt; SPOOF\n    DET --&gt; PRIV\n    REC --&gt; FACE\n    LMK --&gt; FACE\n    ATTR --&gt; FACE</code></pre>"},{"location":"concepts/overview/#design-principles","title":"Design Principles","text":""},{"location":"concepts/overview/#1-onnx-first","title":"1. ONNX-First","text":"<p>All models use ONNX Runtime for inference:</p> <ul> <li>Cross-platform: Same models work on macOS, Linux, Windows</li> <li>Hardware acceleration: Automatic selection of optimal provider</li> <li>Production-ready: No Python-only dependencies for inference</li> </ul>"},{"location":"concepts/overview/#2-minimal-dependencies","title":"2. Minimal Dependencies","text":"<p>Core dependencies are kept minimal:</p> <pre><code>numpy         # Array operations\nopencv-python # Image processing\nonnxruntime   # Model inference\nrequests      # Model download\ntqdm          # Progress bars\n</code></pre>"},{"location":"concepts/overview/#3-simple-api","title":"3. Simple API","text":"<p>Factory functions and direct instantiation:</p> <pre><code># Factory function\ndetector = create_detector('retinaface')\n\n# Direct instantiation (recommended)\nfrom uniface import RetinaFace\ndetector = RetinaFace()\n</code></pre>"},{"location":"concepts/overview/#4-type-safety","title":"4. Type Safety","text":"<p>Full type hints throughout:</p> <pre><code>def detect(self, image: np.ndarray) -&gt; list[Face]:\n    ...\n</code></pre>"},{"location":"concepts/overview/#module-structure","title":"Module Structure","text":"<pre><code>uniface/\n\u251c\u2500\u2500 detection/      # Face detection (RetinaFace, SCRFD, YOLOv5Face)\n\u251c\u2500\u2500 recognition/    # Face recognition (ArcFace, MobileFace, SphereFace)\n\u251c\u2500\u2500 landmark/       # 106-point landmarks\n\u251c\u2500\u2500 attribute/      # Age, gender, emotion, race\n\u251c\u2500\u2500 parsing/        # Face semantic segmentation\n\u251c\u2500\u2500 gaze/           # Gaze estimation\n\u251c\u2500\u2500 spoofing/       # Anti-spoofing\n\u251c\u2500\u2500 privacy/        # Face anonymization\n\u251c\u2500\u2500 types.py        # Dataclasses (Face, GazeResult, etc.)\n\u251c\u2500\u2500 constants.py    # Model weights and URLs\n\u251c\u2500\u2500 model_store.py  # Model download and caching\n\u251c\u2500\u2500 onnx_utils.py   # ONNX Runtime utilities\n\u2514\u2500\u2500 visualization.py # Drawing utilities\n</code></pre>"},{"location":"concepts/overview/#workflow","title":"Workflow","text":"<p>A typical face analysis workflow:</p> <pre><code>import cv2\nfrom uniface import RetinaFace, ArcFace, AgeGender\n\n# 1. Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\nage_gender = AgeGender()\n\n# 2. Load image\nimage = cv2.imread(\"photo.jpg\")\n\n# 3. Detect faces\nfaces = detector.detect(image)\n\n# 4. Analyze each face\nfor face in faces:\n    # Recognition embedding\n    embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n    # Attributes\n    attrs = age_gender.predict(image, face.bbox)\n\n    print(f\"Face: {attrs.sex}, {attrs.age} years\")\n</code></pre>"},{"location":"concepts/overview/#faceanalyzer","title":"FaceAnalyzer","text":"<p>For convenience, <code>FaceAnalyzer</code> combines multiple modules:</p> <pre><code>from uniface import FaceAnalyzer\n\nanalyzer = FaceAnalyzer(\n    detect=True,\n    recognize=True,\n    attributes=True\n)\n\nfaces = analyzer.analyze(image)\nfor face in faces:\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n    print(f\"Embedding: {face.embedding.shape}\")\n</code></pre>"},{"location":"concepts/overview/#model-lifecycle","title":"Model Lifecycle","text":"<ol> <li>First use: Model is downloaded from GitHub releases</li> <li>Cached: Stored in <code>~/.uniface/models/</code></li> <li>Verified: SHA-256 checksum validation</li> <li>Loaded: ONNX Runtime session created</li> <li>Inference: Hardware-accelerated execution</li> </ol> <pre><code># Models auto-download on first use\ndetector = RetinaFace()  # Downloads if not cached\n\n# Or manually pre-download\nfrom uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\npath = verify_model_weights(RetinaFaceWeights.MNET_V2)\n</code></pre>"},{"location":"concepts/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Inputs &amp; Outputs - Understand data types</li> <li>Execution Providers - Hardware acceleration</li> <li>Detection Module - Start with face detection</li> <li>Image Pipeline Recipe - Complete workflow</li> </ul>"},{"location":"concepts/thresholds-calibration/","title":"Thresholds &amp; Calibration","text":"<p>This page explains how to tune detection and recognition thresholds for your use case.</p>"},{"location":"concepts/thresholds-calibration/#detection-thresholds","title":"Detection Thresholds","text":""},{"location":"concepts/thresholds-calibration/#confidence-threshold","title":"Confidence Threshold","text":"<p>Controls minimum confidence for face detection:</p> <pre><code>from uniface import RetinaFace\n\n# Default (balanced)\ndetector = RetinaFace(confidence_threshold=0.5)\n\n# High precision (fewer false positives)\ndetector = RetinaFace(confidence_threshold=0.8)\n\n# High recall (catch more faces)\ndetector = RetinaFace(confidence_threshold=0.3)\n</code></pre> <p>Guidelines:</p> Threshold Use Case 0.3 - 0.4 Maximum recall (research, analysis) 0.5 - 0.6 Balanced (default, general use) 0.7 - 0.9 High precision (production, security)"},{"location":"concepts/thresholds-calibration/#nms-threshold","title":"NMS Threshold","text":"<p>Non-Maximum Suppression removes overlapping detections:</p> <pre><code># Default\ndetector = RetinaFace(nms_threshold=0.4)\n\n# Stricter (fewer overlapping boxes)\ndetector = RetinaFace(nms_threshold=0.3)\n\n# Looser (for crowded scenes)\ndetector = RetinaFace(nms_threshold=0.5)\n</code></pre>"},{"location":"concepts/thresholds-calibration/#input-size","title":"Input Size","text":"<p>Affects detection accuracy and speed:</p> <pre><code># Faster, lower accuracy\ndetector = RetinaFace(input_size=(320, 320))\n\n# Balanced (default)\ndetector = RetinaFace(input_size=(640, 640))\n\n# Higher accuracy, slower\ndetector = RetinaFace(input_size=(1280, 1280))\n</code></pre> <p>Dynamic Size</p> <p>For RetinaFace, enable dynamic input for variable image sizes: <pre><code>detector = RetinaFace(dynamic_size=True)\n</code></pre></p>"},{"location":"concepts/thresholds-calibration/#recognition-thresholds","title":"Recognition Thresholds","text":""},{"location":"concepts/thresholds-calibration/#similarity-threshold","title":"Similarity Threshold","text":"<p>For identity verification (same person check):</p> <pre><code>import numpy as np\nfrom uniface import compute_similarity\n\nsimilarity = compute_similarity(embedding1, embedding2)\n\n# Threshold interpretation\nif similarity &gt; 0.6:\n    print(\"Same person (high confidence)\")\nelif similarity &gt; 0.4:\n    print(\"Uncertain (manual review)\")\nelse:\n    print(\"Different people\")\n</code></pre> <p>Recommended thresholds:</p> Threshold Decision False Accept Rate 0.4 Low security Higher FAR 0.5 Balanced Moderate FAR 0.6 High security Lower FAR 0.7 Very strict Very low FAR"},{"location":"concepts/thresholds-calibration/#calibration-for-your-dataset","title":"Calibration for Your Dataset","text":"<p>Test on your data to find optimal thresholds:</p> <pre><code>import numpy as np\n\ndef calibrate_threshold(same_pairs, diff_pairs, recognizer, detector):\n    \"\"\"Find optimal threshold for your dataset.\"\"\"\n    same_scores = []\n    diff_scores = []\n\n    # Compute similarities for same-person pairs\n    for img1_path, img2_path in same_pairs:\n        img1 = cv2.imread(img1_path)\n        img2 = cv2.imread(img2_path)\n\n        faces1 = detector.detect(img1)\n        faces2 = detector.detect(img2)\n\n        if faces1 and faces2:\n            emb1 = recognizer.get_normalized_embedding(img1, faces1[0].landmarks)\n            emb2 = recognizer.get_normalized_embedding(img2, faces2[0].landmarks)\n            same_scores.append(np.dot(emb1, emb2.T)[0][0])\n\n    # Compute similarities for different-person pairs\n    for img1_path, img2_path in diff_pairs:\n        # ... similar process\n        diff_scores.append(similarity)\n\n    # Find optimal threshold\n    thresholds = np.arange(0.3, 0.8, 0.05)\n    best_threshold = 0.5\n    best_accuracy = 0\n\n    for thresh in thresholds:\n        tp = sum(1 for s in same_scores if s &gt;= thresh)\n        tn = sum(1 for s in diff_scores if s &lt; thresh)\n        accuracy = (tp + tn) / (len(same_scores) + len(diff_scores))\n\n        if accuracy &gt; best_accuracy:\n            best_accuracy = accuracy\n            best_threshold = thresh\n\n    return best_threshold, best_accuracy\n</code></pre>"},{"location":"concepts/thresholds-calibration/#anti-spoofing-thresholds","title":"Anti-Spoofing Thresholds","text":"<p>The MiniFASNet model returns a confidence score:</p> <pre><code>from uniface.spoofing import MiniFASNet\n\nspoofer = MiniFASNet()\nresult = spoofer.predict(image, face.bbox)\n\n# Default threshold (0.5)\nif result.is_real:  # confidence &gt; 0.5\n    print(\"Real face\")\n\n# Custom threshold for high security\nSPOOF_THRESHOLD = 0.7\nif result.confidence &gt; SPOOF_THRESHOLD:\n    print(\"Real face (high confidence)\")\nelse:\n    print(\"Potentially fake\")\n</code></pre>"},{"location":"concepts/thresholds-calibration/#attribute-model-confidence","title":"Attribute Model Confidence","text":""},{"location":"concepts/thresholds-calibration/#emotion","title":"Emotion","text":"<pre><code>result = emotion_predictor.predict(image, landmarks)\n\n# Filter low-confidence predictions\nif result.confidence &gt; 0.6:\n    print(f\"Emotion: {result.emotion}\")\nelse:\n    print(\"Uncertain emotion\")\n</code></pre>"},{"location":"concepts/thresholds-calibration/#visualization-threshold","title":"Visualization Threshold","text":"<p>For drawing detections, filter by confidence:</p> <pre><code>from uniface.visualization import draw_detections\n\n# Only draw high-confidence detections\nbboxes = [f.bbox for f in faces if f.confidence &gt; 0.7]\nscores = [f.confidence for f in faces if f.confidence &gt; 0.7]\nlandmarks = [f.landmarks for f in faces if f.confidence &gt; 0.7]\n\ndraw_detections(\n    image=image,\n    bboxes=bboxes,\n    scores=scores,\n    landmarks=landmarks,\n    vis_threshold=0.6  # Additional visualization filter\n)\n</code></pre>"},{"location":"concepts/thresholds-calibration/#summary","title":"Summary","text":"Parameter Default Range Lower = Higher = <code>confidence_threshold</code> 0.5 0.1-0.9 More detections Fewer false positives <code>nms_threshold</code> 0.4 0.1-0.7 Fewer overlaps More overlapping boxes Similarity threshold 0.6 0.3-0.8 More matches (FAR\u2191) Fewer matches (FRR\u2191) Spoof confidence 0.5 0.3-0.9 More \"real\" Stricter liveness"},{"location":"concepts/thresholds-calibration/#next-steps","title":"Next Steps","text":"<ul> <li>Detection Module - Detection model options</li> <li>Recognition Module - Recognition model options</li> </ul>"},{"location":"modules/attributes/","title":"Attributes","text":"<p>Facial attribute analysis for age, gender, race, and emotion detection.</p>"},{"location":"modules/attributes/#available-models","title":"Available Models","text":"Model Attributes Size Notes AgeGender Age, Gender 8 MB Exact age prediction FairFace Gender, Age Group, Race 44 MB Balanced demographics Emotion 7-8 emotions 2 MB Requires PyTorch"},{"location":"modules/attributes/#agegender","title":"AgeGender","text":"<p>Predicts exact age and binary gender.</p>"},{"location":"modules/attributes/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace, AgeGender\n\ndetector = RetinaFace()\nage_gender = AgeGender()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = age_gender.predict(image, face.bbox)\n    print(f\"Gender: {result.sex}\")  # \"Female\" or \"Male\"\n    print(f\"Age: {result.age} years\")\n</code></pre>"},{"location":"modules/attributes/#output","title":"Output","text":"<pre><code># AttributeResult fields\nresult.gender     # 0=Female, 1=Male\nresult.sex        # \"Female\" or \"Male\" (property)\nresult.age        # int, age in years\nresult.age_group  # None (not provided by this model)\nresult.race       # None (not provided by this model)\n</code></pre>"},{"location":"modules/attributes/#fairface","title":"FairFace","text":"<p>Predicts gender, age group, and race with balanced demographics.</p>"},{"location":"modules/attributes/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace, FairFace\n\ndetector = RetinaFace()\nfairface = FairFace()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = fairface.predict(image, face.bbox)\n    print(f\"Gender: {result.sex}\")\n    print(f\"Age Group: {result.age_group}\")\n    print(f\"Race: {result.race}\")\n</code></pre>"},{"location":"modules/attributes/#output_1","title":"Output","text":"<pre><code># AttributeResult fields\nresult.gender     # 0=Female, 1=Male\nresult.sex        # \"Female\" or \"Male\"\nresult.age        # None (not provided by this model)\nresult.age_group  # \"20-29\", \"30-39\", etc.\nresult.race       # Race/ethnicity label\n</code></pre>"},{"location":"modules/attributes/#race-categories","title":"Race Categories","text":"Label White Black Latino Hispanic East Asian Southeast Asian Indian Middle Eastern"},{"location":"modules/attributes/#age-groups","title":"Age Groups","text":"Group 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 70+"},{"location":"modules/attributes/#emotion","title":"Emotion","text":"<p>Predicts facial emotions. Requires PyTorch.</p> <p>Optional Dependency</p> <p>Emotion detection requires PyTorch. Install with: <pre><code>pip install torch\n</code></pre></p>"},{"location":"modules/attributes/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace\nfrom uniface.attribute import Emotion\nfrom uniface.constants import DDAMFNWeights\n\ndetector = RetinaFace()\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET7)\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = emotion.predict(image, face.landmarks)\n    print(f\"Emotion: {result.emotion}\")\n    print(f\"Confidence: {result.confidence:.2%}\")\n</code></pre>"},{"location":"modules/attributes/#emotion-classes","title":"Emotion Classes","text":"7-Class (AFFECNET7)8-Class (AFFECNET8) Label Neutral Happy Sad Surprise Fear Disgust Anger Label Neutral Happy Sad Surprise Fear Disgust Anger Contempt"},{"location":"modules/attributes/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.attribute import Emotion\nfrom uniface.constants import DDAMFNWeights\n\n# 7-class emotion\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET7)\n\n# 8-class emotion\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET8)\n</code></pre>"},{"location":"modules/attributes/#combining-models","title":"Combining Models","text":""},{"location":"modules/attributes/#full-attribute-analysis","title":"Full Attribute Analysis","text":"<pre><code>from uniface import RetinaFace, AgeGender, FairFace\n\ndetector = RetinaFace()\nage_gender = AgeGender()\nfairface = FairFace()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    # Get exact age from AgeGender\n    ag_result = age_gender.predict(image, face.bbox)\n\n    # Get race from FairFace\n    ff_result = fairface.predict(image, face.bbox)\n\n    print(f\"Gender: {ag_result.sex}\")\n    print(f\"Exact Age: {ag_result.age}\")\n    print(f\"Age Group: {ff_result.age_group}\")\n    print(f\"Race: {ff_result.race}\")\n</code></pre>"},{"location":"modules/attributes/#using-faceanalyzer","title":"Using FaceAnalyzer","text":"<pre><code>from uniface import FaceAnalyzer\n\nanalyzer = FaceAnalyzer(\n    detect=True,\n    recognize=False,\n    attributes=True  # Uses AgeGender\n)\n\nfaces = analyzer.analyze(image)\n\nfor face in faces:\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n</code></pre>"},{"location":"modules/attributes/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef draw_attributes(image, face, result):\n    \"\"\"Draw attributes on image.\"\"\"\n    x1, y1, x2, y2 = map(int, face.bbox)\n\n    # Draw bounding box\n    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    # Build label\n    label = f\"{result.sex}\"\n    if result.age:\n        label += f\", {result.age}y\"\n    if result.age_group:\n        label += f\", {result.age_group}\"\n    if result.race:\n        label += f\", {result.race}\"\n\n    # Draw label\n    cv2.putText(\n        image, label, (x1, y1 - 10),\n        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n    )\n\n    return image\n\n# Usage\nfor face in faces:\n    result = age_gender.predict(image, face.bbox)\n    image = draw_attributes(image, face, result)\n\ncv2.imwrite(\"attributes.jpg\", image)\n</code></pre>"},{"location":"modules/attributes/#accuracy-notes","title":"Accuracy Notes","text":"<p>Model Limitations</p> <ul> <li>AgeGender: Trained on CelebA; accuracy varies by demographic</li> <li>FairFace: Trained for balanced demographics; better cross-racial accuracy</li> <li>Emotion: Accuracy depends on facial expression clarity</li> </ul> <p>Always test on your specific use case and consider cultural context.</p>"},{"location":"modules/attributes/#next-steps","title":"Next Steps","text":"<ul> <li>Parsing - Face semantic segmentation</li> <li>Gaze - Gaze estimation</li> <li>Image Pipeline Recipe - Complete workflow</li> </ul>"},{"location":"modules/detection/","title":"Detection","text":"<p>Face detection is the first step in any face analysis pipeline. UniFace provides three detection models.</p>"},{"location":"modules/detection/#available-models","title":"Available Models","text":"Model Backbone Size WIDER FACE (Easy/Medium/Hard) Best For RetinaFace MobileNet V2 3.5 MB 91.7% / 91.0% / 86.6% Balanced (recommended) SCRFD SCRFD-10G 17 MB 95.2% / 93.9% / 83.1% High accuracy YOLOv5-Face YOLOv5s 28 MB 94.3% / 92.6% / 83.2% Real-time"},{"location":"modules/detection/#retinaface","title":"RetinaFace","text":"<p>The recommended detector for most use cases.</p>"},{"location":"modules/detection/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace\n\ndetector = RetinaFace()\nfaces = detector.detect(image)\n\nfor face in faces:\n    print(f\"Confidence: {face.confidence:.2f}\")\n    print(f\"BBox: {face.bbox}\")\n    print(f\"Landmarks: {face.landmarks.shape}\")  # (5, 2)\n</code></pre>"},{"location":"modules/detection/#model-variants","title":"Model Variants","text":"<pre><code>from uniface import RetinaFace\nfrom uniface.constants import RetinaFaceWeights\n\n# Lightweight (mobile/edge)\ndetector = RetinaFace(model_name=RetinaFaceWeights.MNET_025)\n\n# Balanced (default)\ndetector = RetinaFace(model_name=RetinaFaceWeights.MNET_V2)\n\n# High accuracy\ndetector = RetinaFace(model_name=RetinaFaceWeights.RESNET34)\n</code></pre> Variant Params Size Easy Medium Hard MNET_025 0.4M 1.7 MB 88.5% 87.0% 80.6% MNET_050 1.0M 2.6 MB 89.4% 88.0% 82.4% MNET_V1 3.5M 3.8 MB 90.6% 89.1% 84.1% MNET_V2 3.2M 3.5 MB 91.7% 91.0% 86.6% RESNET18 11.7M 27 MB 92.5% 91.0% 86.6% RESNET34 24.8M 56 MB 94.2% 93.1% 88.9%"},{"location":"modules/detection/#configuration","title":"Configuration","text":"<pre><code>detector = RetinaFace(\n    model_name=RetinaFaceWeights.MNET_V2,\n    confidence_threshold=0.5,  # Min confidence\n    nms_threshold=0.4,         # NMS IoU threshold\n    input_size=(640, 640),     # Input resolution\n    dynamic_size=False         # Enable dynamic input size\n)\n</code></pre>"},{"location":"modules/detection/#scrfd","title":"SCRFD","text":"<p>State-of-the-art detection with excellent accuracy-speed tradeoff.</p>"},{"location":"modules/detection/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface import SCRFD\n\ndetector = SCRFD()\nfaces = detector.detect(image)\n</code></pre>"},{"location":"modules/detection/#model-variants_1","title":"Model Variants","text":"<pre><code>from uniface import SCRFD\nfrom uniface.constants import SCRFDWeights\n\n# Real-time (lightweight)\ndetector = SCRFD(model_name=SCRFDWeights.SCRFD_500M_KPS)\n\n# High accuracy (default)\ndetector = SCRFD(model_name=SCRFDWeights.SCRFD_10G_KPS)\n</code></pre> Variant Params Size Easy Medium Hard SCRFD_500M_KPS 0.6M 2.5 MB 90.6% 88.1% 68.5% SCRFD_10G_KPS 4.2M 17 MB 95.2% 93.9% 83.1%"},{"location":"modules/detection/#configuration_1","title":"Configuration","text":"<pre><code>detector = SCRFD(\n    model_name=SCRFDWeights.SCRFD_10G_KPS,\n    confidence_threshold=0.5,\n    nms_threshold=0.4,\n    input_size=(640, 640)\n)\n</code></pre>"},{"location":"modules/detection/#yolov5-face","title":"YOLOv5-Face","text":"<p>YOLO-based detection optimized for faces.</p>"},{"location":"modules/detection/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface import YOLOv5Face\n\ndetector = YOLOv5Face()\nfaces = detector.detect(image)\n</code></pre>"},{"location":"modules/detection/#model-variants_2","title":"Model Variants","text":"<pre><code>from uniface import YOLOv5Face\nfrom uniface.constants import YOLOv5FaceWeights\n\n# Lightweight\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5N)\n\n# Balanced (default)\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5S)\n\n# High accuracy\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5M)\n</code></pre> Variant Size Easy Medium Hard YOLOV5N 11 MB 93.6% 91.5% 80.5% YOLOV5S 28 MB 94.3% 92.6% 83.2% YOLOV5M 82 MB 95.3% 93.8% 85.3% <p>Fixed Input Size</p> <p>YOLOv5-Face uses a fixed input size of 640\u00d7640.</p>"},{"location":"modules/detection/#configuration_2","title":"Configuration","text":"<pre><code>detector = YOLOv5Face(\n    model_name=YOLOv5FaceWeights.YOLOV5S,\n    confidence_threshold=0.6,\n    nms_threshold=0.5\n)\n</code></pre>"},{"location":"modules/detection/#factory-function","title":"Factory Function","text":"<p>Create detectors dynamically:</p> <pre><code>from uniface import create_detector\n\ndetector = create_detector('retinaface')\n# or\ndetector = create_detector('scrfd')\n# or\ndetector = create_detector('yolov5face')\n</code></pre>"},{"location":"modules/detection/#high-level-api","title":"High-Level API","text":"<p>One-line detection:</p> <pre><code>from uniface import detect_faces\n\nfaces = detect_faces(\n    image,\n    method='retinaface',\n    confidence_threshold=0.5\n)\n</code></pre>"},{"location":"modules/detection/#output-format","title":"Output Format","text":"<p>All detectors return <code>list[Face]</code>:</p> <pre><code>for face in faces:\n    # Bounding box [x1, y1, x2, y2]\n    bbox = face.bbox\n\n    # Detection confidence (0-1)\n    confidence = face.confidence\n\n    # 5-point landmarks (5, 2)\n    landmarks = face.landmarks\n    # [left_eye, right_eye, nose, left_mouth, right_mouth]\n</code></pre>"},{"location":"modules/detection/#visualization","title":"Visualization","text":"<pre><code>from uniface.visualization import draw_detections\n\ndraw_detections(\n    image=image,\n    bboxes=[f.bbox for f in faces],\n    scores=[f.confidence for f in faces],\n    landmarks=[f.landmarks for f in faces],\n    vis_threshold=0.6\n)\n\ncv2.imwrite(\"result.jpg\", image)\n</code></pre>"},{"location":"modules/detection/#performance-comparison","title":"Performance Comparison","text":"<p>Benchmark on your hardware:</p> <pre><code>python tools/detection.py --source image.jpg --iterations 100\n</code></pre>"},{"location":"modules/detection/#see-also","title":"See Also","text":"<ul> <li>Recognition Module - Extract embeddings from detected faces</li> <li>Landmarks Module - Get 106-point landmarks</li> <li>Image Pipeline Recipe - Complete detection workflow</li> <li>Concepts: Thresholds - Tuning detection parameters</li> </ul>"},{"location":"modules/gaze/","title":"Gaze Estimation","text":"<p>Gaze estimation predicts where a person is looking (pitch and yaw angles).</p>"},{"location":"modules/gaze/#available-models","title":"Available Models","text":"Model Backbone Size MAE* Best For ResNet18 ResNet18 43 MB 12.84\u00b0 Balanced ResNet34 ResNet34 82 MB 11.33\u00b0 Recommended ResNet50 ResNet50 91 MB 11.34\u00b0 High accuracy MobileNetV2 MobileNetV2 9.6 MB 13.07\u00b0 Mobile MobileOne-S0 MobileOne 4.8 MB 12.58\u00b0 Lightweight <p>*MAE = Mean Absolute Error on Gaze360 test set (lower is better)</p>"},{"location":"modules/gaze/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, MobileGaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor face in faces:\n    # Crop face\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        # Estimate gaze\n        result = gaze_estimator.estimate(face_crop)\n\n        # Convert to degrees\n        pitch_deg = np.degrees(result.pitch)\n        yaw_deg = np.degrees(result.yaw)\n\n        print(f\"Pitch: {pitch_deg:.1f}\u00b0, Yaw: {yaw_deg:.1f}\u00b0\")\n</code></pre>"},{"location":"modules/gaze/#model-variants","title":"Model Variants","text":"<pre><code>from uniface import MobileGaze\nfrom uniface.constants import GazeWeights\n\n# Default (ResNet34, recommended)\ngaze = MobileGaze()\n\n# Lightweight for mobile/edge\ngaze = MobileGaze(model_name=GazeWeights.MOBILEONE_S0)\n\n# Higher accuracy\ngaze = MobileGaze(model_name=GazeWeights.RESNET50)\n</code></pre>"},{"location":"modules/gaze/#output-format","title":"Output Format","text":"<pre><code>result = gaze_estimator.estimate(face_crop)\n\n# GazeResult dataclass\nresult.pitch  # Vertical angle in radians\nresult.yaw    # Horizontal angle in radians\n</code></pre>"},{"location":"modules/gaze/#angle-convention","title":"Angle Convention","text":"<pre><code>          pitch = +90\u00b0 (looking up)\n               \u2502\n               \u2502\nyaw = -90\u00b0 \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500 yaw = +90\u00b0\n(looking left) \u2502     (looking right)\n               \u2502\n          pitch = -90\u00b0 (looking down)\n</code></pre> <ul> <li>Pitch: Vertical gaze angle</li> <li>Positive = looking up</li> <li> <p>Negative = looking down</p> </li> <li> <p>Yaw: Horizontal gaze angle</p> </li> <li>Positive = looking right</li> <li>Negative = looking left</li> </ul>"},{"location":"modules/gaze/#visualization","title":"Visualization","text":"<pre><code>from uniface.visualization import draw_gaze\n\n# Detect faces\nfaces = detector.detect(image)\n\nfor face in faces:\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        result = gaze_estimator.estimate(face_crop)\n\n        # Draw gaze arrow on image\n        draw_gaze(image, face.bbox, result.pitch, result.yaw)\n\ncv2.imwrite(\"gaze_output.jpg\", image)\n</code></pre>"},{"location":"modules/gaze/#custom-visualization","title":"Custom Visualization","text":"<pre><code>import cv2\nimport numpy as np\n\ndef draw_gaze_custom(image, bbox, pitch, yaw, length=100, color=(0, 255, 0)):\n    \"\"\"Draw custom gaze arrow.\"\"\"\n    x1, y1, x2, y2 = map(int, bbox)\n\n    # Face center\n    cx = (x1 + x2) // 2\n    cy = (y1 + y2) // 2\n\n    # Calculate endpoint\n    dx = -length * np.sin(yaw) * np.cos(pitch)\n    dy = -length * np.sin(pitch)\n\n    # Draw arrow\n    end_x = int(cx + dx)\n    end_y = int(cy + dy)\n\n    cv2.arrowedLine(image, (cx, cy), (end_x, end_y), color, 2, tipLength=0.3)\n\n    return image\n</code></pre>"},{"location":"modules/gaze/#real-time-gaze-tracking","title":"Real-Time Gaze Tracking","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, MobileGaze\nfrom uniface.visualization import draw_gaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n        face_crop = frame[y1:y2, x1:x2]\n\n        if face_crop.size &gt; 0:\n            result = gaze_estimator.estimate(face_crop)\n\n            # Draw bounding box\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Draw gaze\n            draw_gaze(frame, face.bbox, result.pitch, result.yaw)\n\n            # Display angles\n            pitch_deg = np.degrees(result.pitch)\n            yaw_deg = np.degrees(result.yaw)\n            label = f\"P:{pitch_deg:.0f} Y:{yaw_deg:.0f}\"\n            cv2.putText(frame, label, (x1, y1 - 10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    cv2.imshow(\"Gaze Estimation\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/gaze/#use-cases","title":"Use Cases","text":""},{"location":"modules/gaze/#attention-detection","title":"Attention Detection","text":"<pre><code>def is_looking_at_camera(result, threshold=15):\n    \"\"\"Check if person is looking at camera.\"\"\"\n    pitch_deg = abs(np.degrees(result.pitch))\n    yaw_deg = abs(np.degrees(result.yaw))\n\n    return pitch_deg &lt; threshold and yaw_deg &lt; threshold\n\n# Usage\nresult = gaze_estimator.estimate(face_crop)\nif is_looking_at_camera(result):\n    print(\"Looking at camera\")\nelse:\n    print(\"Looking away\")\n</code></pre>"},{"location":"modules/gaze/#gaze-direction-classification","title":"Gaze Direction Classification","text":"<pre><code>def classify_gaze_direction(result, threshold=20):\n    \"\"\"Classify gaze into directions.\"\"\"\n    pitch_deg = np.degrees(result.pitch)\n    yaw_deg = np.degrees(result.yaw)\n\n    directions = []\n\n    if pitch_deg &gt; threshold:\n        directions.append(\"up\")\n    elif pitch_deg &lt; -threshold:\n        directions.append(\"down\")\n\n    if yaw_deg &gt; threshold:\n        directions.append(\"right\")\n    elif yaw_deg &lt; -threshold:\n        directions.append(\"left\")\n\n    if not directions:\n        return \"center\"\n\n    return \" \".join(directions)\n\n# Usage\nresult = gaze_estimator.estimate(face_crop)\ndirection = classify_gaze_direction(result)\nprint(f\"Looking: {direction}\")\n</code></pre>"},{"location":"modules/gaze/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_gaze_estimator\n\ngaze = create_gaze_estimator()  # Returns MobileGaze\n</code></pre>"},{"location":"modules/gaze/#next-steps","title":"Next Steps","text":"<ul> <li>Anti-Spoofing - Face liveness detection</li> <li>Privacy - Face anonymization</li> <li>Video Recipe - Real-time processing</li> </ul>"},{"location":"modules/landmarks/","title":"Landmarks","text":"<p>Facial landmark detection provides precise localization of facial features.</p>"},{"location":"modules/landmarks/#available-models","title":"Available Models","text":"Model Points Size Use Case Landmark106 106 14 MB Detailed face analysis <p>5-Point Landmarks</p> <p>Basic 5-point landmarks are included with all detection models (RetinaFace, SCRFD, YOLOv5-Face).</p>"},{"location":"modules/landmarks/#106-point-landmarks","title":"106-Point Landmarks","text":""},{"location":"modules/landmarks/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace, Landmark106\n\ndetector = RetinaFace()\nlandmarker = Landmark106()\n\n# Detect face\nfaces = detector.detect(image)\n\n# Get detailed landmarks\nif faces:\n    landmarks = landmarker.get_landmarks(image, faces[0].bbox)\n    print(f\"Landmarks shape: {landmarks.shape}\")  # (106, 2)\n</code></pre>"},{"location":"modules/landmarks/#landmark-groups","title":"Landmark Groups","text":"Range Group Points 0-32 Face Contour 33 33-50 Eyebrows 18 51-62 Nose 12 63-86 Eyes 24 87-105 Mouth 19"},{"location":"modules/landmarks/#extract-specific-features","title":"Extract Specific Features","text":"<pre><code>landmarks = landmarker.get_landmarks(image, face.bbox)\n\n# Face contour\ncontour = landmarks[0:33]\n\n# Left eyebrow\nleft_eyebrow = landmarks[33:42]\n\n# Right eyebrow\nright_eyebrow = landmarks[42:51]\n\n# Nose\nnose = landmarks[51:63]\n\n# Left eye\nleft_eye = landmarks[63:72]\n\n# Right eye\nright_eye = landmarks[76:84]\n\n# Mouth\nmouth = landmarks[87:106]\n</code></pre>"},{"location":"modules/landmarks/#5-point-landmarks-detection","title":"5-Point Landmarks (Detection)","text":"<p>All detection models provide 5-point landmarks:</p> <pre><code>from uniface import RetinaFace\n\ndetector = RetinaFace()\nfaces = detector.detect(image)\n\nif faces:\n    landmarks_5 = faces[0].landmarks\n    print(f\"Shape: {landmarks_5.shape}\")  # (5, 2)\n\n    left_eye = landmarks_5[0]\n    right_eye = landmarks_5[1]\n    nose = landmarks_5[2]\n    left_mouth = landmarks_5[3]\n    right_mouth = landmarks_5[4]\n</code></pre>"},{"location":"modules/landmarks/#visualization","title":"Visualization","text":""},{"location":"modules/landmarks/#draw-106-landmarks","title":"Draw 106 Landmarks","text":"<pre><code>import cv2\n\ndef draw_landmarks(image, landmarks, color=(0, 255, 0), radius=2):\n    \"\"\"Draw landmarks on image.\"\"\"\n    for x, y in landmarks.astype(int):\n        cv2.circle(image, (x, y), radius, color, -1)\n    return image\n\n# Usage\nlandmarks = landmarker.get_landmarks(image, face.bbox)\nimage_with_landmarks = draw_landmarks(image.copy(), landmarks)\ncv2.imwrite(\"landmarks.jpg\", image_with_landmarks)\n</code></pre>"},{"location":"modules/landmarks/#draw-with-connections","title":"Draw with Connections","text":"<pre><code>def draw_landmarks_with_connections(image, landmarks):\n    \"\"\"Draw landmarks with facial feature connections.\"\"\"\n    landmarks = landmarks.astype(int)\n\n    # Face contour (0-32)\n    for i in range(32):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (255, 255, 0), 1)\n\n    # Left eyebrow (33-41)\n    for i in range(33, 41):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 255, 0), 1)\n\n    # Right eyebrow (42-50)\n    for i in range(42, 50):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 255, 0), 1)\n\n    # Nose (51-62)\n    for i in range(51, 62):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 0, 255), 1)\n\n    # Draw points\n    for x, y in landmarks:\n        cv2.circle(image, (x, y), 2, (0, 255, 255), -1)\n\n    return image\n</code></pre>"},{"location":"modules/landmarks/#use-cases","title":"Use Cases","text":""},{"location":"modules/landmarks/#face-alignment","title":"Face Alignment","text":"<pre><code>from uniface import face_alignment\n\n# Align face using 5-point landmarks\naligned = face_alignment(image, faces[0].landmarks)\n# Returns: 112x112 aligned face\n</code></pre>"},{"location":"modules/landmarks/#eye-aspect-ratio-blink-detection","title":"Eye Aspect Ratio (Blink Detection)","text":"<pre><code>import numpy as np\n\ndef eye_aspect_ratio(eye_landmarks):\n    \"\"\"Calculate eye aspect ratio for blink detection.\"\"\"\n    # Vertical distances\n    v1 = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])\n    v2 = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])\n\n    # Horizontal distance\n    h = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])\n\n    ear = (v1 + v2) / (2.0 * h)\n    return ear\n\n# Usage with 106-point landmarks\nleft_eye = landmarks[63:72]  # Approximate eye points\near = eye_aspect_ratio(left_eye)\n\nif ear &lt; 0.2:\n    print(\"Eye closed (blink detected)\")\n</code></pre>"},{"location":"modules/landmarks/#head-pose-estimation","title":"Head Pose Estimation","text":"<pre><code>import cv2\nimport numpy as np\n\ndef estimate_head_pose(landmarks, image_shape):\n    \"\"\"Estimate head pose from facial landmarks.\"\"\"\n    # 3D model points (generic face model)\n    model_points = np.array([\n        (0.0, 0.0, 0.0),       # Nose tip\n        (0.0, -330.0, -65.0),  # Chin\n        (-225.0, 170.0, -135.0),  # Left eye corner\n        (225.0, 170.0, -135.0),   # Right eye corner\n        (-150.0, -150.0, -125.0), # Left mouth corner\n        (150.0, -150.0, -125.0)   # Right mouth corner\n    ], dtype=np.float64)\n\n    # 2D image points (from 106 landmarks)\n    image_points = np.array([\n        landmarks[51],   # Nose tip\n        landmarks[16],   # Chin\n        landmarks[63],   # Left eye corner\n        landmarks[76],   # Right eye corner\n        landmarks[87],   # Left mouth corner\n        landmarks[93]    # Right mouth corner\n    ], dtype=np.float64)\n\n    # Camera matrix\n    h, w = image_shape[:2]\n    focal_length = w\n    center = (w / 2, h / 2)\n    camera_matrix = np.array([\n        [focal_length, 0, center[0]],\n        [0, focal_length, center[1]],\n        [0, 0, 1]\n    ], dtype=np.float64)\n\n    # Solve PnP\n    dist_coeffs = np.zeros((4, 1))\n    success, rotation_vector, translation_vector = cv2.solvePnP(\n        model_points, image_points, camera_matrix, dist_coeffs\n    )\n\n    return rotation_vector, translation_vector\n</code></pre>"},{"location":"modules/landmarks/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_landmarker\n\nlandmarker = create_landmarker()  # Returns Landmark106\n</code></pre>"},{"location":"modules/landmarks/#see-also","title":"See Also","text":"<ul> <li>Detection Module - Face detection with 5-point landmarks</li> <li>Attributes Module - Age, gender, emotion</li> <li>Gaze Module - Gaze estimation</li> <li>Concepts: Coordinate Systems - Landmark formats</li> </ul>"},{"location":"modules/parsing/","title":"Parsing","text":"<p>Face parsing segments faces into semantic components (skin, eyes, nose, mouth, hair, etc.).</p>"},{"location":"modules/parsing/#available-models","title":"Available Models","text":"Model Backbone Size Classes Best For BiSeNet ResNet18 ResNet18 51 MB 19 Balanced (recommended) BiSeNet ResNet34 ResNet34 89 MB 19 Higher accuracy"},{"location":"modules/parsing/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nfrom uniface.parsing import BiSeNet\nfrom uniface.visualization import vis_parsing_maps\n\n# Initialize parser\nparser = BiSeNet()\n\n# Load face image (cropped)\nface_image = cv2.imread(\"face.jpg\")\n\n# Parse face\nmask = parser.parse(face_image)\nprint(f\"Mask shape: {mask.shape}\")  # (H, W)\n\n# Visualize\nface_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\nvis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\n# Save result\nvis_bgr = cv2.cvtColor(vis_result, cv2.COLOR_RGB2BGR)\ncv2.imwrite(\"parsed.jpg\", vis_bgr)\n</code></pre>"},{"location":"modules/parsing/#19-facial-component-classes","title":"19 Facial Component Classes","text":"ID Class ID Class 0 Background 10 Ear Ring 1 Skin 11 Nose 2 Left Eyebrow 12 Mouth 3 Right Eyebrow 13 Upper Lip 4 Left Eye 14 Lower Lip 5 Right Eye 15 Neck 6 Eye Glasses 16 Neck Lace 7 Left Ear 17 Cloth 8 Right Ear 18 Hair 9 Hat"},{"location":"modules/parsing/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.parsing import BiSeNet\nfrom uniface.constants import ParsingWeights\n\n# Default (ResNet18)\nparser = BiSeNet()\n\n# Higher accuracy (ResNet34)\nparser = BiSeNet(model_name=ParsingWeights.RESNET34)\n</code></pre> Variant Params Size Notes RESNET18 13.3M 51 MB Recommended RESNET34 24.1M 89 MB Higher accuracy"},{"location":"modules/parsing/#full-pipeline","title":"Full Pipeline","text":""},{"location":"modules/parsing/#with-face-detection","title":"With Face Detection","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.parsing import BiSeNet\nfrom uniface.visualization import vis_parsing_maps\n\ndetector = RetinaFace()\nparser = BiSeNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    # Crop face\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    # Parse\n    mask = parser.parse(face_crop)\n\n    # Visualize\n    face_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n    vis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\n    # Save\n    vis_bgr = cv2.cvtColor(vis_result, cv2.COLOR_RGB2BGR)\n    cv2.imwrite(f\"face_{i}_parsed.jpg\", vis_bgr)\n</code></pre>"},{"location":"modules/parsing/#extract-specific-components","title":"Extract Specific Components","text":""},{"location":"modules/parsing/#get-single-component-mask","title":"Get Single Component Mask","text":"<pre><code>import numpy as np\n\n# Parse face\nmask = parser.parse(face_image)\n\n# Extract specific component\nSKIN = 1\nHAIR = 18\nLEFT_EYE = 4\nRIGHT_EYE = 5\n\n# Binary mask for skin\nskin_mask = (mask == SKIN).astype(np.uint8) * 255\n\n# Binary mask for hair\nhair_mask = (mask == HAIR).astype(np.uint8) * 255\n\n# Binary mask for eyes\neyes_mask = ((mask == LEFT_EYE) | (mask == RIGHT_EYE)).astype(np.uint8) * 255\n</code></pre>"},{"location":"modules/parsing/#count-pixels-per-component","title":"Count Pixels per Component","text":"<pre><code>import numpy as np\n\nmask = parser.parse(face_image)\n\ncomponent_names = {\n    0: 'Background', 1: 'Skin', 2: 'L-Eyebrow', 3: 'R-Eyebrow',\n    4: 'L-Eye', 5: 'R-Eye', 6: 'Glasses', 7: 'L-Ear', 8: 'R-Ear',\n    9: 'Hat', 10: 'Earring', 11: 'Nose', 12: 'Mouth',\n    13: 'U-Lip', 14: 'L-Lip', 15: 'Neck', 16: 'Necklace',\n    17: 'Cloth', 18: 'Hair'\n}\n\nfor class_id in np.unique(mask):\n    pixel_count = np.sum(mask == class_id)\n    name = component_names.get(class_id, f'Class {class_id}')\n    print(f\"{name}: {pixel_count} pixels\")\n</code></pre>"},{"location":"modules/parsing/#applications","title":"Applications","text":""},{"location":"modules/parsing/#face-makeup","title":"Face Makeup","text":"<p>Apply virtual makeup using component masks:</p> <pre><code>import cv2\nimport numpy as np\n\ndef apply_lip_color(image, mask, color=(180, 50, 50)):\n    \"\"\"Apply lip color using parsing mask.\"\"\"\n    result = image.copy()\n\n    # Get lip mask (upper + lower lip)\n    lip_mask = ((mask == 13) | (mask == 14)).astype(np.uint8)\n\n    # Create color overlay\n    overlay = np.zeros_like(image)\n    overlay[:] = color\n\n    # Blend with original\n    lip_region = cv2.bitwise_and(overlay, overlay, mask=lip_mask)\n    non_lip = cv2.bitwise_and(result, result, mask=1 - lip_mask)\n\n    # Combine with alpha blending\n    alpha = 0.4\n    result = cv2.addWeighted(result, 1 - alpha * lip_mask[:,:,np.newaxis] / 255,\n                             lip_region, alpha, 0)\n\n    return result.astype(np.uint8)\n</code></pre>"},{"location":"modules/parsing/#background-replacement","title":"Background Replacement","text":"<pre><code>def replace_background(image, mask, background):\n    \"\"\"Replace background using parsing mask.\"\"\"\n    # Create foreground mask (everything except background)\n    foreground_mask = (mask != 0).astype(np.uint8)\n\n    # Resize background to match image\n    background = cv2.resize(background, (image.shape[1], image.shape[0]))\n\n    # Combine\n    result = image.copy()\n    result[foreground_mask == 0] = background[foreground_mask == 0]\n\n    return result\n</code></pre>"},{"location":"modules/parsing/#hair-segmentation","title":"Hair Segmentation","text":"<pre><code>def get_hair_mask(mask):\n    \"\"\"Extract clean hair mask.\"\"\"\n    hair_mask = (mask == 18).astype(np.uint8) * 255\n\n    # Clean up with morphological operations\n    kernel = np.ones((5, 5), np.uint8)\n    hair_mask = cv2.morphologyEx(hair_mask, cv2.MORPH_CLOSE, kernel)\n    hair_mask = cv2.morphologyEx(hair_mask, cv2.MORPH_OPEN, kernel)\n\n    return hair_mask\n</code></pre>"},{"location":"modules/parsing/#visualization-options","title":"Visualization Options","text":"<pre><code>from uniface.visualization import vis_parsing_maps\n\n# Default visualization\nvis_result = vis_parsing_maps(face_rgb, mask)\n\n# With different parameters\nvis_result = vis_parsing_maps(\n    face_rgb,\n    mask,\n    save_image=False,  # Don't save to file\n)\n</code></pre>"},{"location":"modules/parsing/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_face_parser\n\nparser = create_face_parser()  # Returns BiSeNet\n</code></pre>"},{"location":"modules/parsing/#next-steps","title":"Next Steps","text":"<ul> <li>Gaze - Gaze estimation</li> <li>Privacy - Face anonymization</li> <li>Detection - Face detection</li> </ul>"},{"location":"modules/privacy/","title":"Privacy","text":"<p>Face anonymization protects privacy by blurring or obscuring faces in images and videos.</p>"},{"location":"modules/privacy/#available-methods","title":"Available Methods","text":"Method Description Use Case pixelate Blocky pixelation News media standard gaussian Smooth blur Natural appearance blackout Solid color fill Maximum privacy elliptical Oval-shaped blur Natural face shape median Edge-preserving blur Artistic effect"},{"location":"modules/privacy/#quick-start","title":"Quick Start","text":""},{"location":"modules/privacy/#one-line-anonymization","title":"One-Line Anonymization","text":"<pre><code>from uniface.privacy import anonymize_faces\nimport cv2\n\nimage = cv2.imread(\"group_photo.jpg\")\nanonymized = anonymize_faces(image, method='pixelate')\ncv2.imwrite(\"anonymized.jpg\", anonymized)\n</code></pre>"},{"location":"modules/privacy/#blurface-class","title":"BlurFace Class","text":"<p>For more control, use the <code>BlurFace</code> class:</p> <pre><code>from uniface import RetinaFace\nfrom uniface.privacy import BlurFace\nimport cv2\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian', blur_strength=5.0)\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\nanonymized = blurrer.anonymize(image, faces)\n\ncv2.imwrite(\"anonymized.jpg\", anonymized)\n</code></pre>"},{"location":"modules/privacy/#blur-methods","title":"Blur Methods","text":""},{"location":"modules/privacy/#pixelate","title":"Pixelate","text":"<p>Blocky pixelation effect (common in news media):</p> <pre><code>blurrer = BlurFace(method='pixelate', pixel_blocks=10)\n</code></pre> Parameter Default Description <code>pixel_blocks</code> 10 Number of blocks (lower = more pixelated)"},{"location":"modules/privacy/#gaussian","title":"Gaussian","text":"<p>Smooth, natural-looking blur:</p> <pre><code>blurrer = BlurFace(method='gaussian', blur_strength=3.0)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity (higher = more blur)"},{"location":"modules/privacy/#blackout","title":"Blackout","text":"<p>Solid color fill for maximum privacy:</p> <pre><code>blurrer = BlurFace(method='blackout', color=(0, 0, 0))\n</code></pre> Parameter Default Description <code>color</code> (0, 0, 0) Fill color (BGR format)"},{"location":"modules/privacy/#elliptical","title":"Elliptical","text":"<p>Oval-shaped blur matching natural face shape:</p> <pre><code>blurrer = BlurFace(method='elliptical', blur_strength=3.0, margin=20)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity <code>margin</code> 20 Margin around face"},{"location":"modules/privacy/#median","title":"Median","text":"<p>Edge-preserving blur with artistic effect:</p> <pre><code>blurrer = BlurFace(method='median', blur_strength=3.0)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity"},{"location":"modules/privacy/#in-place-processing","title":"In-Place Processing","text":"<p>Modify image directly (faster, saves memory):</p> <pre><code>blurrer = BlurFace(method='pixelate')\n\n# In-place modification\nresult = blurrer.anonymize(image, faces, inplace=True)\n# 'image' and 'result' point to the same array\n</code></pre>"},{"location":"modules/privacy/#real-time-anonymization","title":"Real-Time Anonymization","text":""},{"location":"modules/privacy/#webcam","title":"Webcam","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='pixelate')\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n\n    cv2.imshow('Anonymized', frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/privacy/#video-file","title":"Video File","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian')\n\ncap = cv2.VideoCapture(\"input_video.mp4\")\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n    out.write(frame)\n\ncap.release()\nout.release()\n</code></pre>"},{"location":"modules/privacy/#selective-anonymization","title":"Selective Anonymization","text":""},{"location":"modules/privacy/#exclude-specific-faces","title":"Exclude Specific Faces","text":"<pre><code>def anonymize_except(image, all_faces, exclude_embeddings, recognizer, threshold=0.6):\n    \"\"\"Anonymize all faces except those matching exclude_embeddings.\"\"\"\n    faces_to_blur = []\n\n    for face in all_faces:\n        # Get embedding\n        embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n        # Check if should be excluded\n        should_exclude = False\n        for ref_emb in exclude_embeddings:\n            similarity = np.dot(embedding, ref_emb.T)[0][0]\n            if similarity &gt; threshold:\n                should_exclude = True\n                break\n\n        if not should_exclude:\n            faces_to_blur.append(face)\n\n    # Blur remaining faces\n    return blurrer.anonymize(image, faces_to_blur)\n</code></pre>"},{"location":"modules/privacy/#confidence-based","title":"Confidence-Based","text":"<pre><code>def anonymize_low_confidence(image, faces, blurrer, confidence_threshold=0.8):\n    \"\"\"Anonymize faces below confidence threshold.\"\"\"\n    faces_to_blur = [f for f in faces if f.confidence &lt; confidence_threshold]\n    return blurrer.anonymize(image, faces_to_blur)\n</code></pre>"},{"location":"modules/privacy/#comparison","title":"Comparison","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nmethods = ['pixelate', 'gaussian', 'blackout', 'elliptical', 'median']\n\nfor method in methods:\n    blurrer = BlurFace(method=method)\n    result = blurrer.anonymize(image.copy(), faces)\n    cv2.imwrite(f\"anonymized_{method}.jpg\", result)\n</code></pre>"},{"location":"modules/privacy/#command-line-tool","title":"Command-Line Tool","text":"<pre><code># Anonymize image with pixelation\npython tools/face_anonymize.py --source photo.jpg\n\n# Real-time webcam\npython tools/face_anonymize.py --source 0 --method gaussian\n\n# Custom blur strength\npython tools/face_anonymize.py --source photo.jpg --method gaussian --blur-strength 5.0\n</code></pre>"},{"location":"modules/privacy/#next-steps","title":"Next Steps","text":"<ul> <li>Anonymize Stream Recipe - Video pipeline</li> <li>Detection - Face detection options</li> <li>Batch Processing Recipe - Process multiple files</li> </ul>"},{"location":"modules/recognition/","title":"Recognition","text":"<p>Face recognition extracts embeddings for identity verification and face search.</p>"},{"location":"modules/recognition/#available-models","title":"Available Models","text":"Model Backbone Size Embedding Dim Best For ArcFace MobileNet/ResNet 8-166 MB 512 General use (recommended) MobileFace MobileNet V2/V3 1-10 MB 512 Mobile/Edge SphereFace Sphere20/36 50-92 MB 512 Research"},{"location":"modules/recognition/#arcface","title":"ArcFace","text":"<p>State-of-the-art recognition using additive angular margin loss.</p>"},{"location":"modules/recognition/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace, ArcFace\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Detect face\nfaces = detector.detect(image)\n\n# Extract embedding\nif faces:\n    embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n    print(f\"Embedding shape: {embedding.shape}\")  # (1, 512)\n</code></pre>"},{"location":"modules/recognition/#model-variants","title":"Model Variants","text":"<pre><code>from uniface import ArcFace\nfrom uniface.constants import ArcFaceWeights\n\n# Lightweight (default)\nrecognizer = ArcFace(model_name=ArcFaceWeights.MNET)\n\n# High accuracy\nrecognizer = ArcFace(model_name=ArcFaceWeights.RESNET)\n</code></pre> Variant Backbone Size Use Case MNET MobileNet 8 MB Balanced (recommended) RESNET ResNet50 166 MB Maximum accuracy"},{"location":"modules/recognition/#mobileface","title":"MobileFace","text":"<p>Lightweight recognition for resource-constrained environments.</p>"},{"location":"modules/recognition/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface import MobileFace\n\nrecognizer = MobileFace()\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n</code></pre>"},{"location":"modules/recognition/#model-variants_1","title":"Model Variants","text":"<pre><code>from uniface import MobileFace\nfrom uniface.constants import MobileFaceWeights\n\n# Ultra-lightweight\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_025)\n\n# Balanced (default)\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_V2)\n\n# Higher accuracy\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_V3_LARGE)\n</code></pre> Variant Params Size LFW Use Case MNET_025 0.36M 1 MB 98.8% Ultra-lightweight MNET_V2 2.29M 4 MB 99.6% Mobile/Edge MNET_V3_SMALL 1.25M 3 MB 99.3% Mobile optimized MNET_V3_LARGE 3.52M 10 MB 99.5% Balanced mobile"},{"location":"modules/recognition/#sphereface","title":"SphereFace","text":"<p>Recognition using angular softmax loss (A-Softmax).</p>"},{"location":"modules/recognition/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface import SphereFace\nfrom uniface.constants import SphereFaceWeights\n\nrecognizer = SphereFace(model_name=SphereFaceWeights.SPHERE20)\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n</code></pre> Variant Params Size LFW Use Case SPHERE20 24.5M 50 MB 99.7% Research SPHERE36 34.6M 92 MB 99.7% Research"},{"location":"modules/recognition/#face-comparison","title":"Face Comparison","text":""},{"location":"modules/recognition/#compute-similarity","title":"Compute Similarity","text":"<pre><code>from uniface import compute_similarity\nimport numpy as np\n\n# Extract embeddings\nemb1 = recognizer.get_normalized_embedding(image1, landmarks1)\nemb2 = recognizer.get_normalized_embedding(image2, landmarks2)\n\n# Method 1: Using utility function\nsimilarity = compute_similarity(emb1, emb2)\n\n# Method 2: Direct computation\nsimilarity = np.dot(emb1, emb2.T)[0][0]\n\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"modules/recognition/#threshold-guidelines","title":"Threshold Guidelines","text":"Threshold Decision Use Case &gt; 0.7 Very high confidence Security-critical &gt; 0.6 Same person General verification 0.4 - 0.6 Uncertain Manual review needed &lt; 0.4 Different people Rejection"},{"location":"modules/recognition/#face-alignment","title":"Face Alignment","text":"<p>Recognition models require aligned faces. UniFace handles this internally:</p> <pre><code># Alignment is done automatically\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n\n# Or manually align\nfrom uniface import face_alignment\n\naligned_face = face_alignment(image, landmarks)\n# Returns: 112x112 aligned face image\n</code></pre>"},{"location":"modules/recognition/#building-a-face-database","title":"Building a Face Database","text":"<pre><code>import numpy as np\nfrom uniface import RetinaFace, ArcFace\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Build database\ndatabase = {}\nfor person_id, image_path in person_images.items():\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n\n    if faces:\n        embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n        database[person_id] = embedding\n\n# Save for later use\nnp.savez('face_database.npz', **database)\n\n# Load database\ndata = np.load('face_database.npz')\ndatabase = {key: data[key] for key in data.files}\n</code></pre>"},{"location":"modules/recognition/#face-search","title":"Face Search","text":"<p>Find a person in a database:</p> <pre><code>def search_face(query_embedding, database, threshold=0.6):\n    \"\"\"Find best match in database.\"\"\"\n    best_match = None\n    best_similarity = -1\n\n    for person_id, db_embedding in database.items():\n        similarity = np.dot(query_embedding, db_embedding.T)[0][0]\n\n        if similarity &gt; best_similarity and similarity &gt; threshold:\n            best_similarity = similarity\n            best_match = person_id\n\n    return best_match, best_similarity\n\n# Usage\nquery_embedding = recognizer.get_normalized_embedding(query_image, landmarks)\nmatch, similarity = search_face(query_embedding, database)\n\nif match:\n    print(f\"Found: {match} (similarity: {similarity:.4f})\")\nelse:\n    print(\"No match found\")\n</code></pre>"},{"location":"modules/recognition/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_recognizer\n\nrecognizer = create_recognizer('arcface')\n</code></pre>"},{"location":"modules/recognition/#see-also","title":"See Also","text":"<ul> <li>Detection Module - Detect faces first</li> <li>Face Search Recipe - Complete search system</li> <li>Thresholds - Calibration guide</li> </ul>"},{"location":"modules/spoofing/","title":"Anti-Spoofing","text":"<p>Face anti-spoofing detects whether a face is real (live) or fake (photo, video replay, mask).</p>"},{"location":"modules/spoofing/#available-models","title":"Available Models","text":"Model Size Notes MiniFASNet V1SE 1.2 MB Squeeze-and-Excitation variant MiniFASNet V2 1.2 MB Improved version (recommended)"},{"location":"modules/spoofing/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = spoofer.predict(image, face.bbox)\n\n    label = \"Real\" if result.is_real else \"Fake\"\n    print(f\"{label}: {result.confidence:.1%}\")\n</code></pre>"},{"location":"modules/spoofing/#output-format","title":"Output Format","text":"<pre><code>result = spoofer.predict(image, face.bbox)\n\n# SpoofingResult dataclass\nresult.is_real     # True = real, False = fake\nresult.confidence  # 0.0 to 1.0\n</code></pre>"},{"location":"modules/spoofing/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.spoofing import MiniFASNet\nfrom uniface.constants import MiniFASNetWeights\n\n# Default (V2, recommended)\nspoofer = MiniFASNet()\n\n# V1SE variant\nspoofer = MiniFASNet(model_name=MiniFASNetWeights.V1SE)\n</code></pre> Variant Size Scale Factor V1SE 1.2 MB 4.0 V2 1.2 MB 2.7"},{"location":"modules/spoofing/#confidence-thresholds","title":"Confidence Thresholds","text":"<p>The default threshold is 0.5. Adjust for your use case:</p> <pre><code>result = spoofer.predict(image, face.bbox)\n\n# High security (fewer false accepts)\nHIGH_THRESHOLD = 0.7\nif result.confidence &gt; HIGH_THRESHOLD:\n    print(\"Real (high confidence)\")\nelse:\n    print(\"Suspicious\")\n\n# Balanced\nif result.is_real:  # Uses default 0.5 threshold\n    print(\"Real\")\nelse:\n    print(\"Fake\")\n</code></pre>"},{"location":"modules/spoofing/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef draw_spoofing_result(image, face, result):\n    \"\"\"Draw spoofing result on image.\"\"\"\n    x1, y1, x2, y2 = map(int, face.bbox)\n\n    # Color based on result\n    color = (0, 255, 0) if result.is_real else (0, 0, 255)\n    label = \"Real\" if result.is_real else \"Fake\"\n\n    # Draw bounding box\n    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n\n    # Draw label\n    text = f\"{label}: {result.confidence:.1%}\"\n    cv2.putText(image, text, (x1, y1 - 10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    return image\n\n# Usage\nfor face in faces:\n    result = spoofer.predict(image, face.bbox)\n    image = draw_spoofing_result(image, face, result)\n\ncv2.imwrite(\"spoofing_result.jpg\", image)\n</code></pre>"},{"location":"modules/spoofing/#real-time-liveness-detection","title":"Real-Time Liveness Detection","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    for face in faces:\n        result = spoofer.predict(frame, face.bbox)\n\n        # Draw result\n        x1, y1, x2, y2 = map(int, face.bbox)\n        color = (0, 255, 0) if result.is_real else (0, 0, 255)\n        label = f\"{'Real' if result.is_real else 'Fake'}: {result.confidence:.0%}\"\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(frame, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    cv2.imshow(\"Liveness Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/spoofing/#use-cases","title":"Use Cases","text":""},{"location":"modules/spoofing/#access-control","title":"Access Control","text":"<pre><code>def verify_liveness(image, face, spoofer, threshold=0.6):\n    \"\"\"Verify face is real for access control.\"\"\"\n    result = spoofer.predict(image, face.bbox)\n\n    if result.is_real and result.confidence &gt; threshold:\n        return True, result.confidence\n    return False, result.confidence\n\n# Usage\nis_live, confidence = verify_liveness(image, face, spoofer)\nif is_live:\n    print(f\"Access granted (confidence: {confidence:.1%})\")\nelse:\n    print(f\"Access denied - possible spoof attempt\")\n</code></pre>"},{"location":"modules/spoofing/#multi-frame-verification","title":"Multi-Frame Verification","text":"<p>For higher security, verify across multiple frames:</p> <pre><code>def verify_liveness_multiframe(frames, detector, spoofer, min_real=3):\n    \"\"\"Verify liveness across multiple frames.\"\"\"\n    real_count = 0\n\n    for frame in frames:\n        faces = detector.detect(frame)\n        if not faces:\n            continue\n\n        result = spoofer.predict(frame, faces[0].bbox)\n        if result.is_real:\n            real_count += 1\n\n    return real_count &gt;= min_real\n\n# Collect frames and verify\nframes = []\nfor _ in range(5):\n    ret, frame = cap.read()\n    if ret:\n        frames.append(frame)\n\nis_verified = verify_liveness_multiframe(frames, detector, spoofer)\n</code></pre>"},{"location":"modules/spoofing/#attack-types-detected","title":"Attack Types Detected","text":"<p>MiniFASNet can detect various spoof attacks:</p> Attack Type Detection Printed photos \u2705 Screen replay \u2705 Video replay \u2705 Paper masks \u2705 3D masks Limited <p>Limitations</p> <ul> <li>High-quality 3D masks may not be detected</li> <li>Performance varies with lighting and image quality</li> <li>Always combine with other verification methods for high-security applications</li> </ul>"},{"location":"modules/spoofing/#command-line-tool","title":"Command-Line Tool","text":"<pre><code># Image\npython tools/spoofing.py --source photo.jpg\n\n# Webcam\npython tools/spoofing.py --source 0\n</code></pre>"},{"location":"modules/spoofing/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_spoofer\n\nspoofer = create_spoofer()  # Returns MiniFASNet\n</code></pre>"},{"location":"modules/spoofing/#next-steps","title":"Next Steps","text":"<ul> <li>Privacy - Face anonymization</li> <li>Detection - Face detection</li> <li>Recognition - Face recognition</li> </ul>"},{"location":"recipes/anonymize-stream/","title":"Anonymize Stream","text":"<p>Blur faces in real-time video streams for privacy protection.</p> <p>Work in Progress</p> <p>This page contains example code patterns. Test thoroughly before using in production.</p>"},{"location":"recipes/anonymize-stream/#webcam-anonymization","title":"Webcam Anonymization","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='pixelate')\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n\n    cv2.imshow('Anonymized', frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/anonymize-stream/#video-file-anonymization","title":"Video File Anonymization","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian')\n\ncap = cv2.VideoCapture(\"input.mp4\")\nfps = cap.get(cv2.CAP_PROP_FPS)\nw, h = int(cap.get(3)), int(cap.get(4))\n\nout = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\nwhile cap.read()[0]:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    blurrer.anonymize(frame, faces, inplace=True)\n    out.write(frame)\n\ncap.release()\nout.release()\n</code></pre>"},{"location":"recipes/anonymize-stream/#one-liner-for-images","title":"One-Liner for Images","text":"<pre><code>from uniface.privacy import anonymize_faces\nimport cv2\n\nimage = cv2.imread(\"photo.jpg\")\nresult = anonymize_faces(image, method='pixelate')\ncv2.imwrite(\"anonymized.jpg\", result)\n</code></pre>"},{"location":"recipes/anonymize-stream/#available-blur-methods","title":"Available Blur Methods","text":"Method Usage Pixelate <code>BlurFace(method='pixelate', pixel_blocks=10)</code> Gaussian <code>BlurFace(method='gaussian', blur_strength=3.0)</code> Blackout <code>BlurFace(method='blackout', color=(0,0,0))</code> Elliptical <code>BlurFace(method='elliptical', margin=20)</code> Median <code>BlurFace(method='median', blur_strength=3.0)</code>"},{"location":"recipes/anonymize-stream/#see-also","title":"See Also","text":"<ul> <li>Privacy Module - Privacy protection details</li> <li>Video &amp; Webcam - Real-time processing</li> <li>Detection Module - Face detection</li> </ul>"},{"location":"recipes/batch-processing/","title":"Batch Processing","text":"<p>Process multiple images efficiently.</p> <p>Work in Progress</p> <p>This page contains example code patterns. Test thoroughly before using in production.</p>"},{"location":"recipes/batch-processing/#basic-batch-processing","title":"Basic Batch Processing","text":"<pre><code>import cv2\nfrom pathlib import Path\nfrom uniface import RetinaFace\n\ndetector = RetinaFace()\n\ndef process_directory(input_dir, output_dir):\n    \"\"\"Process all images in a directory.\"\"\"\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    for image_path in input_path.glob(\"*.jpg\"):\n        print(f\"Processing {image_path.name}...\")\n\n        image = cv2.imread(str(image_path))\n        faces = detector.detect(image)\n\n        print(f\"  Found {len(faces)} face(s)\")\n\n        # Process and save results\n        # ... your code here ...\n\n# Usage\nprocess_directory(\"input_images/\", \"output_images/\")\n</code></pre>"},{"location":"recipes/batch-processing/#with-progress-bar","title":"With Progress Bar","text":"<pre><code>from tqdm import tqdm\n\nfor image_path in tqdm(image_files, desc=\"Processing\"):\n    # ... process image ...\n    pass\n</code></pre>"},{"location":"recipes/batch-processing/#extract-embeddings","title":"Extract Embeddings","text":"<pre><code>from uniface import RetinaFace, ArcFace\nimport numpy as np\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\nembeddings = {}\nfor image_path in Path(\"faces/\").glob(\"*.jpg\"):\n    image = cv2.imread(str(image_path))\n    faces = detector.detect(image)\n\n    if faces:\n        embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n        embeddings[image_path.stem] = embedding\n\n# Save embeddings\nnp.savez(\"embeddings.npz\", **embeddings)\n</code></pre>"},{"location":"recipes/batch-processing/#see-also","title":"See Also","text":"<ul> <li>Video &amp; Webcam - Real-time processing</li> <li>Face Search - Search through embeddings</li> <li>Image Pipeline - Full analysis pipeline</li> <li>Detection Module - Detection options</li> </ul>"},{"location":"recipes/custom-models/","title":"Custom Models","text":"<p>Add your own ONNX models to UniFace.</p> <p>Work in Progress</p> <p>This page contains example code patterns for advanced users. Test thoroughly before using in production.</p>"},{"location":"recipes/custom-models/#overview","title":"Overview","text":"<p>UniFace is designed to be extensible. You can add custom ONNX models by:</p> <ol> <li>Creating a class that inherits from the appropriate base class</li> <li>Implementing required methods</li> <li>Using the ONNX Runtime utilities provided by UniFace</li> </ol>"},{"location":"recipes/custom-models/#add-custom-detection-model","title":"Add Custom Detection Model","text":"<pre><code>from uniface.detection.base import BaseDetector\nfrom uniface.onnx_utils import create_onnx_session\nfrom uniface.types import Face\nimport numpy as np\n\nclass MyDetector(BaseDetector):\n    def __init__(self, model_path: str, confidence_threshold: float = 0.5):\n        self.session = create_onnx_session(model_path)\n        self.threshold = confidence_threshold\n\n    def detect(self, image: np.ndarray) -&gt; list[Face]:\n        # 1. Preprocess image\n        input_tensor = self._preprocess(image)\n\n        # 2. Run inference\n        outputs = self.session.run(None, {'input': input_tensor})\n\n        # 3. Postprocess outputs to Face objects\n        faces = self._postprocess(outputs, image.shape)\n        return faces\n\n    def _preprocess(self, image):\n        # Your preprocessing logic\n        # e.g., resize, normalize, transpose\n        pass\n\n    def _postprocess(self, outputs, shape):\n        # Your postprocessing logic\n        # e.g., decode boxes, apply NMS, create Face objects\n        pass\n</code></pre>"},{"location":"recipes/custom-models/#add-custom-recognition-model","title":"Add Custom Recognition Model","text":"<pre><code>from uniface.recognition.base import BaseRecognizer\nfrom uniface.onnx_utils import create_onnx_session\nfrom uniface import face_alignment\nimport numpy as np\n\nclass MyRecognizer(BaseRecognizer):\n    def __init__(self, model_path: str):\n        self.session = create_onnx_session(model_path)\n\n    def get_normalized_embedding(\n        self,\n        image: np.ndarray,\n        landmarks: np.ndarray\n    ) -&gt; np.ndarray:\n        # 1. Align face\n        aligned = face_alignment(image, landmarks)\n\n        # 2. Preprocess\n        input_tensor = self._preprocess(aligned)\n\n        # 3. Run inference\n        embedding = self.session.run(None, {'input': input_tensor})[0]\n\n        # 4. Normalize\n        embedding = embedding / np.linalg.norm(embedding)\n        return embedding\n\n    def _preprocess(self, image):\n        # Your preprocessing logic\n        pass\n</code></pre>"},{"location":"recipes/custom-models/#usage","title":"Usage","text":"<pre><code>from my_module import MyDetector, MyRecognizer\n\n# Use custom models\ndetector = MyDetector(\"path/to/detection_model.onnx\")\nrecognizer = MyRecognizer(\"path/to/recognition_model.onnx\")\n\n# Use like built-in models\nfaces = detector.detect(image)\nembedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n</code></pre>"},{"location":"recipes/custom-models/#see-also","title":"See Also","text":"<ul> <li>Detection Module - Built-in detection models</li> <li>Recognition Module - Built-in recognition models</li> <li>Concepts: Overview - Architecture overview</li> </ul>"},{"location":"recipes/face-search/","title":"Face Search","text":"<p>Build a face search system for finding people in images.</p> <p>Work in Progress</p> <p>This page contains example code patterns. Test thoroughly before using in production.</p>"},{"location":"recipes/face-search/#basic-face-database","title":"Basic Face Database","text":"<pre><code>import numpy as np\nimport cv2\nfrom pathlib import Path\nfrom uniface import RetinaFace, ArcFace\n\nclass FaceDatabase:\n    def __init__(self):\n        self.detector = RetinaFace()\n        self.recognizer = ArcFace()\n        self.embeddings = {}\n\n    def add_face(self, person_id, image):\n        \"\"\"Add a face to the database.\"\"\"\n        faces = self.detector.detect(image)\n        if not faces:\n            raise ValueError(f\"No face found for {person_id}\")\n\n        face = max(faces, key=lambda f: f.confidence)\n        embedding = self.recognizer.get_normalized_embedding(image, face.landmarks)\n        self.embeddings[person_id] = embedding\n        return True\n\n    def search(self, image, threshold=0.6):\n        \"\"\"Search for faces in an image.\"\"\"\n        faces = self.detector.detect(image)\n        results = []\n\n        for face in faces:\n            embedding = self.recognizer.get_normalized_embedding(image, face.landmarks)\n\n            best_match = None\n            best_similarity = -1\n\n            for person_id, db_embedding in self.embeddings.items():\n                similarity = np.dot(embedding, db_embedding.T)[0][0]\n                if similarity &gt; best_similarity:\n                    best_similarity = similarity\n                    best_match = person_id\n\n            results.append({\n                'bbox': face.bbox,\n                'match': best_match if best_similarity &gt;= threshold else None,\n                'similarity': best_similarity\n            })\n\n        return results\n\n    def save(self, path):\n        \"\"\"Save database to file.\"\"\"\n        np.savez(path, embeddings=dict(self.embeddings))\n\n    def load(self, path):\n        \"\"\"Load database from file.\"\"\"\n        data = np.load(path, allow_pickle=True)\n        self.embeddings = data['embeddings'].item()\n\n# Usage\ndb = FaceDatabase()\n\n# Add faces\nfor image_path in Path(\"known_faces/\").glob(\"*.jpg\"):\n    person_id = image_path.stem\n    image = cv2.imread(str(image_path))\n    try:\n        db.add_face(person_id, image)\n        print(f\"Added: {person_id}\")\n    except ValueError as e:\n        print(f\"Skipped: {e}\")\n\n# Save database\ndb.save(\"face_database.npz\")\n\n# Search\nquery_image = cv2.imread(\"group_photo.jpg\")\nresults = db.search(query_image)\n\nfor r in results:\n    if r['match']:\n        print(f\"Found: {r['match']} (similarity: {r['similarity']:.3f})\")\n</code></pre>"},{"location":"recipes/face-search/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef visualize_search_results(image, results):\n    \"\"\"Draw search results on image.\"\"\"\n    for r in results:\n        x1, y1, x2, y2 = map(int, r['bbox'])\n\n        if r['match']:\n            color = (0, 255, 0)  # Green for match\n            label = f\"{r['match']} ({r['similarity']:.2f})\"\n        else:\n            color = (0, 0, 255)  # Red for unknown\n            label = f\"Unknown ({r['similarity']:.2f})\"\n\n        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(image, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    return image\n\n# Usage\nresults = db.search(image)\nannotated = visualize_search_results(image.copy(), results)\ncv2.imwrite(\"search_result.jpg\", annotated)\n</code></pre>"},{"location":"recipes/face-search/#real-time-search","title":"Real-Time Search","text":"<pre><code>import cv2\n\ndef realtime_search(db):\n    \"\"\"Real-time face search from webcam.\"\"\"\n    cap = cv2.VideoCapture(0)\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        results = db.search(frame, threshold=0.5)\n\n        for r in results:\n            x1, y1, x2, y2 = map(int, r['bbox'])\n\n            if r['match']:\n                color = (0, 255, 0)\n                label = r['match']\n            else:\n                color = (0, 0, 255)\n                label = \"Unknown\"\n\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            cv2.putText(frame, label, (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n        cv2.imshow(\"Face Search\", frame)\n\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n\n# Usage\ndb = FaceDatabase()\ndb.load(\"face_database.npz\")\nrealtime_search(db)\n</code></pre>"},{"location":"recipes/face-search/#see-also","title":"See Also","text":"<ul> <li>Recognition Module - Face recognition details</li> <li>Batch Processing - Process multiple files</li> <li>Video &amp; Webcam - Real-time processing</li> <li>Concepts: Thresholds - Tuning similarity thresholds</li> </ul>"},{"location":"recipes/image-pipeline/","title":"Image Pipeline","text":"<p>A complete pipeline for processing images with detection, recognition, and attribute analysis.</p>"},{"location":"recipes/image-pipeline/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>import cv2\nfrom uniface import RetinaFace, ArcFace, AgeGender\nfrom uniface.visualization import draw_detections\n\n# Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\nage_gender = AgeGender()\n\ndef process_image(image_path):\n    \"\"\"Process a single image through the full pipeline.\"\"\"\n    # Load image\n    image = cv2.imread(image_path)\n\n    # Step 1: Detect faces\n    faces = detector.detect(image)\n    print(f\"Found {len(faces)} face(s)\")\n\n    results = []\n\n    for i, face in enumerate(faces):\n        # Step 2: Extract embedding\n        embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n        # Step 3: Predict attributes\n        attrs = age_gender.predict(image, face.bbox)\n\n        results.append({\n            'face_id': i,\n            'bbox': face.bbox,\n            'confidence': face.confidence,\n            'embedding': embedding,\n            'gender': attrs.sex,\n            'age': attrs.age\n        })\n\n        print(f\"  Face {i+1}: {attrs.sex}, {attrs.age} years old\")\n\n    # Visualize\n    draw_detections(\n        image=image,\n        bboxes=[f.bbox for f in faces],\n        scores=[f.confidence for f in faces],\n        landmarks=[f.landmarks for f in faces]\n    )\n\n    return image, results\n\n# Usage\nresult_image, results = process_image(\"photo.jpg\")\ncv2.imwrite(\"result.jpg\", result_image)\n</code></pre>"},{"location":"recipes/image-pipeline/#using-faceanalyzer","title":"Using FaceAnalyzer","text":"<p>For convenience, use the built-in <code>FaceAnalyzer</code>:</p> <pre><code>from uniface import FaceAnalyzer\nimport cv2\n\n# Initialize with desired modules\nanalyzer = FaceAnalyzer(\n    detect=True,\n    recognize=True,\n    attributes=True\n)\n\n# Process image\nimage = cv2.imread(\"photo.jpg\")\nfaces = analyzer.analyze(image)\n\n# Access enriched Face objects\nfor face in faces:\n    print(f\"Confidence: {face.confidence:.2f}\")\n    print(f\"Embedding: {face.embedding.shape}\")\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n</code></pre>"},{"location":"recipes/image-pipeline/#full-analysis-pipeline","title":"Full Analysis Pipeline","text":"<p>Complete pipeline with all modules:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface import (\n    RetinaFace, ArcFace, AgeGender, FairFace,\n    Landmark106, MobileGaze\n)\nfrom uniface.parsing import BiSeNet\nfrom uniface.spoofing import MiniFASNet\nfrom uniface.visualization import draw_detections, draw_gaze\n\nclass FaceAnalysisPipeline:\n    def __init__(self):\n        # Initialize all models\n        self.detector = RetinaFace()\n        self.recognizer = ArcFace()\n        self.age_gender = AgeGender()\n        self.fairface = FairFace()\n        self.landmarker = Landmark106()\n        self.gaze = MobileGaze()\n        self.parser = BiSeNet()\n        self.spoofer = MiniFASNet()\n\n    def analyze(self, image):\n        \"\"\"Run full analysis pipeline.\"\"\"\n        faces = self.detector.detect(image)\n        results = []\n\n        for face in faces:\n            result = {\n                'bbox': face.bbox,\n                'confidence': face.confidence,\n                'landmarks_5': face.landmarks\n            }\n\n            # Recognition embedding\n            result['embedding'] = self.recognizer.get_normalized_embedding(\n                image, face.landmarks\n            )\n\n            # Attributes\n            ag_result = self.age_gender.predict(image, face.bbox)\n            result['age'] = ag_result.age\n            result['gender'] = ag_result.sex\n\n            # FairFace attributes\n            ff_result = self.fairface.predict(image, face.bbox)\n            result['age_group'] = ff_result.age_group\n            result['race'] = ff_result.race\n\n            # 106-point landmarks\n            result['landmarks_106'] = self.landmarker.get_landmarks(\n                image, face.bbox\n            )\n\n            # Gaze estimation\n            x1, y1, x2, y2 = map(int, face.bbox)\n            face_crop = image[y1:y2, x1:x2]\n            if face_crop.size &gt; 0:\n                gaze_result = self.gaze.estimate(face_crop)\n                result['gaze_pitch'] = gaze_result.pitch\n                result['gaze_yaw'] = gaze_result.yaw\n\n            # Face parsing\n            if face_crop.size &gt; 0:\n                result['parsing_mask'] = self.parser.parse(face_crop)\n\n            # Anti-spoofing\n            spoof_result = self.spoofer.predict(image, face.bbox)\n            result['is_real'] = spoof_result.is_real\n            result['spoof_confidence'] = spoof_result.confidence\n\n            results.append(result)\n\n        return results\n\n# Usage\npipeline = FaceAnalysisPipeline()\nresults = pipeline.analyze(cv2.imread(\"photo.jpg\"))\n\nfor i, r in enumerate(results):\n    print(f\"\\nFace {i+1}:\")\n    print(f\"  Gender: {r['gender']}, Age: {r['age']}\")\n    print(f\"  Race: {r['race']}, Age Group: {r['age_group']}\")\n    print(f\"  Gaze: pitch={np.degrees(r['gaze_pitch']):.1f}\u00b0\")\n    print(f\"  Real: {r['is_real']} ({r['spoof_confidence']:.1%})\")\n</code></pre>"},{"location":"recipes/image-pipeline/#visualization-pipeline","title":"Visualization Pipeline","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, AgeGender, MobileGaze\nfrom uniface.visualization import draw_detections, draw_gaze\n\ndef visualize_analysis(image_path, output_path):\n    \"\"\"Create annotated visualization of face analysis.\"\"\"\n    detector = RetinaFace()\n    age_gender = AgeGender()\n    gaze = MobileGaze()\n\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n\n        # Draw bounding box\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # Age and gender\n        attrs = age_gender.predict(image, face.bbox)\n        label = f\"{attrs.sex}, {attrs.age}y\"\n        cv2.putText(image, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n        # Gaze\n        face_crop = image[y1:y2, x1:x2]\n        if face_crop.size &gt; 0:\n            gaze_result = gaze.estimate(face_crop)\n            draw_gaze(image, face.bbox, gaze_result.pitch, gaze_result.yaw)\n\n        # Confidence\n        conf_label = f\"{face.confidence:.0%}\"\n        cv2.putText(image, conf_label, (x1, y2 + 20),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    cv2.imwrite(output_path, image)\n    print(f\"Saved to {output_path}\")\n\n# Usage\nvisualize_analysis(\"input.jpg\", \"output.jpg\")\n</code></pre>"},{"location":"recipes/image-pipeline/#json-output","title":"JSON Output","text":"<p>Export results to JSON:</p> <pre><code>import json\nimport numpy as np\n\ndef results_to_json(results):\n    \"\"\"Convert analysis results to JSON-serializable format.\"\"\"\n    output = []\n\n    for r in results:\n        item = {\n            'bbox': r['bbox'].tolist(),\n            'confidence': float(r['confidence']),\n            'age': int(r['age']) if r.get('age') else None,\n            'gender': r.get('gender'),\n            'race': r.get('race'),\n            'is_real': r.get('is_real'),\n            'gaze': {\n                'pitch_deg': float(np.degrees(r['gaze_pitch'])) if 'gaze_pitch' in r else None,\n                'yaw_deg': float(np.degrees(r['gaze_yaw'])) if 'gaze_yaw' in r else None\n            }\n        }\n        output.append(item)\n\n    return output\n\n# Usage\nresults = pipeline.analyze(image)\njson_data = results_to_json(results)\n\nwith open('results.json', 'w') as f:\n    json.dump(json_data, f, indent=2)\n</code></pre>"},{"location":"recipes/image-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Batch Processing - Process multiple images</li> <li>Video &amp; Webcam - Real-time processing</li> <li>Face Search - Build a search system</li> <li>Detection Module - Detection options</li> <li>Recognition Module - Recognition details</li> </ul>"},{"location":"recipes/video-webcam/","title":"Video &amp; Webcam","text":"<p>Real-time face analysis for video streams.</p> <p>Work in Progress</p> <p>This page contains example code patterns. Test thoroughly before using in production.</p>"},{"location":"recipes/video-webcam/#webcam-detection","title":"Webcam Detection","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.visualization import draw_detections\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\nprint(\"Press 'q' to quit\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    draw_detections(\n        image=frame,\n        bboxes=[f.bbox for f in faces],\n        scores=[f.confidence for f in faces],\n        landmarks=[f.landmarks for f in faces]\n    )\n\n    cv2.imshow(\"Face Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#video-file-processing","title":"Video File Processing","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\n\ndef process_video(input_path, output_path):\n    \"\"\"Process a video file.\"\"\"\n    detector = RetinaFace()\n    cap = cv2.VideoCapture(input_path)\n\n    # Get video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Setup output\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.read()[0]:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        faces = detector.detect(frame)\n        # ... process and draw ...\n\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\n# Usage\nprocess_video(\"input.mp4\", \"output.mp4\")\n</code></pre>"},{"location":"recipes/video-webcam/#performance-tips","title":"Performance Tips","text":""},{"location":"recipes/video-webcam/#skip-frames","title":"Skip Frames","text":"<pre><code>PROCESS_EVERY_N = 3  # Process every 3rd frame\nframe_count = 0\nlast_faces = []\n\nwhile True:\n    ret, frame = cap.read()\n    if frame_count % PROCESS_EVERY_N == 0:\n        last_faces = detector.detect(frame)\n    frame_count += 1\n    # Draw last_faces...\n</code></pre>"},{"location":"recipes/video-webcam/#fps-counter","title":"FPS Counter","text":"<pre><code>import time\n\nprev_time = time.time()\nwhile True:\n    curr_time = time.time()\n    fps = 1 / (curr_time - prev_time)\n    prev_time = curr_time\n\n    cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n</code></pre>"},{"location":"recipes/video-webcam/#see-also","title":"See Also","text":"<ul> <li>Anonymize Stream - Privacy protection in video</li> <li>Batch Processing - Process multiple files</li> <li>Detection Module - Detection options</li> <li>Gaze Module - Gaze tracking</li> </ul>"}]}