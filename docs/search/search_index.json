{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#uniface","title":"UniFace","text":"<p>All-in-One Open-Source Face Analysis Library</p> <p> </p> <p>Get Started View on GitHub</p>"},{"location":"#face-detection","title":"Face Detection","text":"<p>ONNX-optimized detectors (RetinaFace, SCRFD, YOLO) with 5-point landmarks.</p>"},{"location":"#face-recognition","title":"Face Recognition","text":"<p>AdaFace, ArcFace, MobileFace, and SphereFace embeddings for identity verification.</p>"},{"location":"#landmarks","title":"Landmarks","text":"<p>Accurate 106-point facial landmark localization for detailed face analysis.</p>"},{"location":"#attributes","title":"Attributes","text":"<p>Age, gender, race (FairFace), and emotion detection from faces.</p>"},{"location":"#face-parsing","title":"Face Parsing","text":"<p>BiSeNet semantic segmentation with 19 facial component classes.</p>"},{"location":"#gaze-estimation","title":"Gaze Estimation","text":"<p>Real-time gaze direction prediction with MobileGaze models.</p>"},{"location":"#tracking","title":"Tracking","text":"<p>Multi-object tracking with BYTETracker for persistent face IDs across video frames.</p>"},{"location":"#anti-spoofing","title":"Anti-Spoofing","text":"<p>Face liveness detection with MiniFASNet to prevent fraud.</p>"},{"location":"#privacy","title":"Privacy","text":"<p>Face anonymization with 5 blur methods for privacy protection.</p>"},{"location":"#installation","title":"Installation","text":"StandardGPU (CUDA)From Source <pre><code>pip install uniface\n</code></pre> <pre><code>pip install uniface[gpu]\n</code></pre> <pre><code>git clone https://github.com/yakhyo/uniface.git\ncd uniface\npip install -e .\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":""},{"location":"#quickstart","title":"Quickstart","text":"<p>Get up and running in 5 minutes with common use cases.</p> <p>Quickstart Guide \u2192</p>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Step-by-step examples for common workflows.</p> <p>View Tutorials \u2192</p>"},{"location":"#api-reference","title":"API Reference","text":"<p>Explore individual modules and their APIs.</p> <p>Browse API \u2192</p>"},{"location":"#guides","title":"Guides","text":"<p>Learn about the architecture and design principles.</p> <p>Read Guides \u2192</p>"},{"location":"#license","title":"License","text":"<p>UniFace is released under the MIT License.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for contributing to UniFace!</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<pre><code># Clone\ngit clone https://github.com/yakhyo/uniface.git\ncd uniface\n\n# Install dev dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use Ruff for formatting:</p> <pre><code>ruff format .\nruff check . --fix\n</code></pre> <p>Guidelines:</p> <ul> <li>Line length: 120</li> <li>Python 3.10+ type hints</li> <li>Google-style docstrings</li> </ul>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code>pip install pre-commit\npre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Write tests for new features</li> <li>Ensure tests pass</li> <li>Submit PR with clear description</li> </ol>"},{"location":"contributing/#adding-new-models","title":"Adding New Models","text":"<ol> <li>Create model class in appropriate submodule</li> <li>Add weight constants to <code>uniface/constants.py</code></li> <li>Export in <code>__init__.py</code> files</li> <li>Write tests in <code>tests/</code></li> <li>Add example in <code>tools/</code> or notebooks</li> </ol>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue on GitHub.</p>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers all installation options for UniFace.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.10 or higher</li> <li>Operating Systems: macOS, Linux, Windows</li> </ul>"},{"location":"installation/#quick-install","title":"Quick Install","text":"<p>The simplest way to install UniFace:</p> <pre><code>pip install uniface\n</code></pre> <p>This installs the CPU version with all core dependencies.</p>"},{"location":"installation/#platform-specific-installation","title":"Platform-Specific Installation","text":""},{"location":"installation/#macos-apple-silicon-m1m2m3m4","title":"macOS (Apple Silicon - M1/M2/M3/M4)","text":"<p>For Apple Silicon Macs, the standard installation automatically includes ARM64 optimizations:</p> <pre><code>pip install uniface\n</code></pre> <p>Native Performance</p> <p>The base <code>onnxruntime</code> package has native Apple Silicon support with ARM64 optimizations built-in since version 1.13+. No additional configuration needed.</p> <p>Verify ARM64 installation:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64\n</code></pre>"},{"location":"installation/#linuxwindows-with-nvidia-gpu","title":"Linux/Windows with NVIDIA GPU","text":"<p>For CUDA acceleration on NVIDIA GPUs:</p> <pre><code>pip install uniface[gpu]\n</code></pre> <p>Requirements:</p> <ul> <li>CUDA 11.x or 12.x</li> <li>cuDNN 8.x</li> </ul> <p>CUDA Compatibility</p> <p>See ONNX Runtime GPU requirements for detailed compatibility matrix.</p> <p>Verify GPU installation:</p> <pre><code>import onnxruntime as ort\nprint(\"Available providers:\", ort.get_available_providers())\n# Should include: 'CUDAExecutionProvider'\n</code></pre>"},{"location":"installation/#cpu-only-all-platforms","title":"CPU-Only (All Platforms)","text":"<pre><code>pip install uniface\n</code></pre> <p>Works on all platforms with automatic CPU fallback.</p>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"<p>For development or the latest features:</p> <pre><code>git clone https://github.com/yakhyo/uniface.git\ncd uniface\npip install -e .\n</code></pre> <p>With development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>UniFace has minimal dependencies:</p> Package Purpose <code>numpy</code> Array operations <code>opencv-python</code> Image processing <code>onnx</code> ONNX model format support <code>onnxruntime</code> Model inference <code>scikit-image</code> Geometric transforms <code>requests</code> Model download <code>tqdm</code> Progress bars"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation:</p> <pre><code>import uniface\nprint(f\"UniFace version: {uniface.__version__}\")\n\n# Check available ONNX providers\nimport onnxruntime as ort\nprint(f\"Available providers: {ort.get_available_providers()}\")\n\n# Quick test\nfrom uniface.detection import RetinaFace\ndetector = RetinaFace()\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you're using Python 3.10+:</p> <pre><code>python --version\n# Should show: Python 3.10.x or higher\n</code></pre>"},{"location":"installation/#model-download-issues","title":"Model Download Issues","text":"<p>Models are automatically downloaded on first use. If downloads fail:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\n# Manually download a model\nmodel_path = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Model downloaded to: {model_path}\")\n</code></pre>"},{"location":"installation/#performance-issues-on-mac","title":"Performance Issues on Mac","text":"<p>Verify you're using the ARM64 build (not x86_64 via Rosetta):</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64 (not x86_64)\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart Guide - Get started in 5 minutes</li> <li>Execution Providers - Hardware acceleration setup</li> </ul>"},{"location":"license-attribution/","title":"Licenses &amp; Attribution","text":""},{"location":"license-attribution/#uniface-license","title":"UniFace License","text":"<p>UniFace is released under the MIT License.</p>"},{"location":"license-attribution/#model-credits","title":"Model Credits","text":"Model Source License RetinaFace yakhyo/retinaface-pytorch MIT SCRFD InsightFace MIT YOLOv5-Face yakhyo/yolov5-face-onnx-inference GPL-3.0 YOLOv8-Face yakhyo/yolov8-face-onnx-inference GPL-3.0 AdaFace yakhyo/adaface-onnx MIT ArcFace InsightFace MIT MobileFace yakhyo/face-recognition MIT SphereFace yakhyo/face-recognition MIT BiSeNet yakhyo/face-parsing MIT MobileGaze yakhyo/gaze-estimation MIT MiniFASNet yakhyo/face-anti-spoofing Apache-2.0 FairFace yakhyo/fairface-onnx CC BY 4.0"},{"location":"models/","title":"Model Zoo","text":"<p>Complete guide to all available models and their performance characteristics.</p>"},{"location":"models/#face-detection-models","title":"Face Detection Models","text":""},{"location":"models/#retinaface-family","title":"RetinaFace Family","text":"<p>RetinaFace models are trained on the WIDER FACE dataset.</p> Model Name Params Size Easy Medium Hard <code>MNET_025</code> 0.4M 1.7MB 88.48% 87.02% 80.61% <code>MNET_050</code> 1.0M 2.6MB 89.42% 87.97% 82.40% <code>MNET_V1</code> 3.5M 3.8MB 90.59% 89.14% 84.13% <code>MNET_V2</code> 3.2M 3.5MB 91.70% 91.03% 86.60% <code>RESNET18</code> 11.7M 27MB 92.50% 91.02% 86.63% <code>RESNET34</code> 24.8M 56MB 94.16% 93.12% 88.90% <p>Accuracy &amp; Benchmarks</p> <p>Accuracy: WIDER FACE validation set (Easy/Medium/Hard subsets) - from RetinaFace paper</p> <p>Speed: Benchmark on your own hardware using <code>python tools/detect.py --source &lt;image&gt;</code></p>"},{"location":"models/#scrfd-family","title":"SCRFD Family","text":"<p>SCRFD (Sample and Computation Redistribution for Efficient Face Detection) models trained on WIDER FACE dataset.</p> Model Name Params Size Easy Medium Hard <code>SCRFD_500M_KPS</code> 0.6M 2.5MB 90.57% 88.12% 68.51% <code>SCRFD_10G_KPS</code> 4.2M 17MB 95.16% 93.87% 83.05% <p>Accuracy &amp; Benchmarks</p> <p>Accuracy: WIDER FACE validation set - from SCRFD paper</p> <p>Speed: Benchmark on your own hardware using <code>python tools/detect.py --source &lt;image&gt;</code></p>"},{"location":"models/#yolov5-face-family","title":"YOLOv5-Face Family","text":"<p>YOLOv5-Face models provide detection with 5-point facial landmarks, trained on WIDER FACE dataset.</p> Model Name Size Easy Medium Hard <code>YOLOV5N</code> 11MB 93.61% 91.52% 80.53% <code>YOLOV5S</code> 28MB 94.33% 92.61% 83.15% <code>YOLOV5M</code> 82MB 95.30% 93.76% 85.28% <p>Accuracy &amp; Benchmarks</p> <p>Accuracy: WIDER FACE validation set - from YOLOv5-Face paper</p> <p>Speed: Benchmark on your own hardware using <code>python tools/detect.py --source &lt;image&gt;</code></p> <p>Fixed Input Size</p> <p>All YOLOv5-Face models use a fixed input size of 640\u00d7640.</p>"},{"location":"models/#yolov8-face-family","title":"YOLOv8-Face Family","text":"<p>YOLOv8-Face models use anchor-free design with DFL (Distribution Focal Loss) for bbox regression. Provides detection with 5-point facial landmarks.</p> Model Name Size Easy Medium Hard <code>YOLOV8_LITE_S</code> 7.4MB 93.4% 91.2% 78.6% <code>YOLOV8N</code> 12MB 94.6% 92.3% 79.6% <p>Accuracy &amp; Benchmarks</p> <p>Accuracy: WIDER FACE validation set (Easy/Medium/Hard subsets)</p> <p>Speed: Benchmark on your own hardware using <code>python tools/detect.py --source &lt;image&gt; --method yolov8face</code></p> <p>Fixed Input Size</p> <p>All YOLOv8-Face models use a fixed input size of 640\u00d7640.</p>"},{"location":"models/#face-recognition-models","title":"Face Recognition Models","text":""},{"location":"models/#adaface","title":"AdaFace","text":"<p>Face recognition using adaptive margin based on image quality.</p> Model Name Backbone Dataset Size IJB-B TAR IJB-C TAR <code>IR_18</code> IR-18 WebFace4M 92 MB 93.03% 94.99% <code>IR_101</code> IR-101 WebFace12M 249 MB - 97.66% <p>Training Data &amp; Accuracy</p> <p>Dataset: WebFace4M (4M images) / WebFace12M (12M images)</p> <p>Accuracy: IJB-B and IJB-C benchmarks, TAR@FAR=0.01%</p> <p>Key Innovation</p> <p>AdaFace introduces adaptive margin that adjusts based on image quality, providing better performance on low-quality images compared to fixed-margin approaches.</p>"},{"location":"models/#arcface","title":"ArcFace","text":"<p>Face recognition using additive angular margin loss.</p> Model Name Backbone Params Size LFW CFP-FP AgeDB-30 IJB-C <code>MNET</code> MobileNet 2.0M 8MB 99.70% 98.00% 96.58% 95.02% <code>RESNET</code> ResNet50 43.6M 166MB 99.83% 99.33% 98.23% 97.25% <p>Training Data</p> <p>Dataset: Trained on WebFace600K (600K images)</p> <p>Accuracy: IJB-C accuracy reported as TAR@FAR=1e-4</p>"},{"location":"models/#mobileface","title":"MobileFace","text":"<p>Lightweight face recognition models with MobileNet backbones.</p> Model Name Backbone Params Size LFW CALFW CPLFW AgeDB-30 <code>MNET_025</code> MobileNetV1 0.25 0.36M 1MB 98.76% 92.02% 82.37% 90.02% <code>MNET_V2</code> MobileNetV2 2.29M 4MB 99.55% 94.87% 86.89% 95.16% <code>MNET_V3_SMALL</code> MobileNetV3-S 1.25M 3MB 99.30% 93.77% 85.29% 92.79% <code>MNET_V3_LARGE</code> MobileNetV3-L 3.52M 10MB 99.53% 94.56% 86.79% 95.13% <p>Training Data</p> <p>Dataset: Trained on MS1M-V2 (5.8M images, 85K identities)</p> <p>Accuracy: Evaluated on LFW, CALFW, CPLFW, and AgeDB-30 benchmarks</p>"},{"location":"models/#sphereface","title":"SphereFace","text":"<p>Face recognition using angular softmax loss.</p> Model Name Backbone Params Size LFW CALFW CPLFW AgeDB-30 <code>SPHERE20</code> Sphere20 24.5M 50MB 99.67% 95.61% 88.75% 96.58% <code>SPHERE36</code> Sphere36 34.6M 92MB 99.72% 95.64% 89.92% 96.83% <p>Training Data</p> <p>Dataset: Trained on MS1M-V2 (5.8M images, 85K identities)</p> <p>Accuracy: Evaluated on LFW, CALFW, CPLFW, and AgeDB-30 benchmarks</p> <p>Architecture</p> <p>SphereFace uses angular softmax loss, an earlier approach before ArcFace. These models provide good accuracy with moderate resource requirements.</p>"},{"location":"models/#facial-landmark-models","title":"Facial Landmark Models","text":""},{"location":"models/#106-point-landmark-detection","title":"106-Point Landmark Detection","text":"<p>Facial landmark localization model.</p> Model Name Points Params Size <code>2D106</code> 106 3.7M 14MB <p>Landmark Groups:</p> Group Points Count Face contour 0-32 33 points Eyebrows 33-50 18 points Nose 51-62 12 points Eyes 63-86 24 points Mouth 87-105 19 points"},{"location":"models/#attribute-analysis-models","title":"Attribute Analysis Models","text":""},{"location":"models/#age-gender-detection","title":"Age &amp; Gender Detection","text":"Model Name Attributes Params Size <code>AgeGender</code> Age, Gender 2.1M 8MB <p>Training Data</p> <p>Dataset: Trained on CelebA</p> <p>Accuracy Note</p> <p>Accuracy varies by demographic and image quality. Test on your specific use case.</p>"},{"location":"models/#fairface-attributes","title":"FairFace Attributes","text":"Model Name Attributes Params Size <code>FairFace</code> Race, Gender, Age Group - 44MB <p>Training Data</p> <p>Dataset: Trained on FairFace dataset with balanced demographics</p> <p>Equitable Predictions</p> <p>FairFace provides more equitable predictions across different racial and gender groups.</p> <p>Race Categories (7): White, Black, Latino Hispanic, East Asian, Southeast Asian, Indian, Middle Eastern</p> <p>Age Groups (9): 0-2, 3-9, 10-19, 20-29, 30-39, 40-49, 50-59, 60-69, 70+</p>"},{"location":"models/#emotion-detection","title":"Emotion Detection","text":"Model Name Classes Params Size <code>AFFECNET7</code> 7 0.5M 2MB <code>AFFECNET8</code> 8 0.5M 2MB <p>Classes (7): Neutral, Happy, Sad, Surprise, Fear, Disgust, Angry</p> <p>Classes (8): Above + Contempt</p> <p>Training Data</p> <p>Dataset: Trained on AffectNet</p> <p>Accuracy Note</p> <p>Emotion detection accuracy depends heavily on facial expression clarity and cultural context.</p>"},{"location":"models/#gaze-estimation-models","title":"Gaze Estimation Models","text":""},{"location":"models/#mobilegaze-family","title":"MobileGaze Family","text":"<p>Gaze direction prediction models trained on Gaze360 dataset. Returns pitch (vertical) and yaw (horizontal) angles in radians.</p> Model Name Params Size MAE* <code>RESNET18</code> 11.7M 43 MB 12.84 <code>RESNET34</code> 24.8M 81.6 MB 11.33 <code>RESNET50</code> 25.6M 91.3 MB 11.34 <code>MOBILENET_V2</code> 3.5M 9.59 MB 13.07 <code>MOBILEONE_S0</code> 2.1M 4.8 MB 12.58 <p>*MAE (Mean Absolute Error) in degrees on Gaze360 test set - lower is better</p> <p>Training Data</p> <p>Dataset: Trained on Gaze360 (indoor/outdoor scenes with diverse head poses)</p> <p>Training: 200 epochs with classification-based approach (binned angles)</p> <p>Input Requirements</p> <p>Requires face crop as input. Use face detection first to obtain bounding boxes.</p>"},{"location":"models/#face-parsing-models","title":"Face Parsing Models","text":""},{"location":"models/#bisenet-family","title":"BiSeNet Family","text":"<p>BiSeNet (Bilateral Segmentation Network) models for semantic face parsing. Segments face images into 19 facial component classes.</p> Model Name Params Size Classes <code>RESNET18</code> 13.3M 50.7 MB 19 <code>RESNET34</code> 24.1M 89.2 MB 19 <p>Training Data</p> <p>Dataset: Trained on CelebAMask-HQ</p> <p>Architecture: BiSeNet with ResNet backbone</p> <p>Input Size: 512\u00d7512 (automatically resized)</p> <p>19 Facial Component Classes:</p> # Class # Class # Class 0 Background 7 Left Ear 14 Neck 1 Skin 8 Right Ear 15 Neck Lace 2 Left Eyebrow 9 Ear Ring 16 Cloth 3 Right Eyebrow 10 Nose 17 Hair 4 Left Eye 11 Mouth 18 Hat 5 Right Eye 12 Upper Lip 6 Eye Glasses 13 Lower Lip <p>Applications:</p> <ul> <li>Face makeup and beauty applications</li> <li>Virtual try-on systems</li> <li>Face editing and manipulation</li> <li>Facial feature extraction</li> <li>Portrait segmentation</li> </ul> <p>Input Requirements</p> <p>Input should be a cropped face image. For full pipeline, use face detection first to obtain face crops.</p>"},{"location":"models/#xseg","title":"XSeg","text":"<p>XSeg from DeepFaceLab outputs masks for face regions. Requires 5-point landmarks for face alignment.</p> Model Name Size Output <code>DEFAULT</code> 67 MB Mask [0, 1] <p>Model Details</p> <p>Origin: DeepFaceLab</p> <p>Input: NHWC format, normalized to [0, 1]</p> <p>Alignment: Requires 5-point landmarks (not bbox crops)</p> <p>Applications:</p> <ul> <li>Face region extraction</li> <li>Face swapping pipelines</li> <li>Occlusion handling</li> </ul> <p>Input Requirements</p> <p>Requires 5-point facial landmarks. Use a face detector like RetinaFace to obtain landmarks first.</p>"},{"location":"models/#anti-spoofing-models","title":"Anti-Spoofing Models","text":""},{"location":"models/#minifasnet-family","title":"MiniFASNet Family","text":"<p>Face anti-spoofing models for liveness detection. Detect if a face is real (live) or fake (photo, video replay, mask).</p> Model Name Size Scale <code>V1SE</code> 1.2 MB 4.0 <code>V2</code> 1.2 MB 2.7 <p>Output Format</p> <p>Output: Returns <code>SpoofingResult(is_real, confidence)</code> where is_real: True=Real, False=Fake</p> <p>Input Requirements</p> <p>Requires face bounding box from a detector.</p>"},{"location":"models/#model-management","title":"Model Management","text":"<p>Models are automatically downloaded and cached on first use.</p> <ul> <li>Cache location: <code>~/.uniface/models/</code> (configurable via <code>set_cache_dir()</code> or <code>UNIFACE_CACHE_DIR</code> env var)</li> <li>Inspect cache path: <code>get_cache_dir()</code> returns the resolved active path</li> <li>Verification: Models are verified with SHA-256 checksums</li> <li>Concurrent download: <code>download_models([...])</code> fetches multiple models in parallel</li> <li>Manual download: Use <code>python tools/download_model.py</code> to pre-download models</li> </ul> <p>See Model Cache &amp; Offline Use for full details.</p>"},{"location":"models/#references","title":"References","text":""},{"location":"models/#model-training-architectures","title":"Model Training &amp; Architectures","text":"<ul> <li>RetinaFace Training: yakhyo/retinaface-pytorch - PyTorch implementation and training code</li> <li>YOLOv5-Face Original: deepcam-cn/yolov5-face - Original PyTorch implementation</li> <li>YOLOv5-Face ONNX: yakhyo/yolov5-face-onnx-inference - ONNX inference implementation</li> <li>YOLOv8-Face Original: derronqi/yolov8-face - Original PyTorch implementation</li> <li>YOLOv8-Face ONNX: yakhyo/yolov8-face-onnx-inference - ONNX inference implementation</li> <li>AdaFace Original: mk-minchul/AdaFace - Original PyTorch implementation</li> <li>AdaFace ONNX: yakhyo/adaface-onnx - ONNX export and inference</li> <li>Face Recognition Training: yakhyo/face-recognition - ArcFace, MobileFace, SphereFace training code</li> <li>Gaze Estimation Training: yakhyo/gaze-estimation - MobileGaze training code and pretrained weights</li> <li>Face Parsing Training: yakhyo/face-parsing - BiSeNet training code and pretrained weights</li> <li>Face Segmentation: yakhyo/face-segmentation - XSeg ONNX Inference</li> <li>Face Anti-Spoofing: yakhyo/face-anti-spoofing - MiniFASNet ONNX inference (weights from minivision-ai/Silent-Face-Anti-Spoofing)</li> <li>FairFace: yakhyo/fairface-onnx - FairFace ONNX inference for race, gender, age prediction</li> <li>InsightFace: deepinsight/insightface - Model architectures and pretrained weights</li> </ul>"},{"location":"models/#papers","title":"Papers","text":"<ul> <li>RetinaFace: Single-Shot Multi-Level Face Localisation in the Wild</li> <li>SCRFD: Sample and Computation Redistribution for Efficient Face Detection</li> <li>YOLOv5-Face: YOLO5Face: Why Reinventing a Face Detector</li> <li>AdaFace: AdaFace: Quality Adaptive Margin for Face Recognition</li> <li>ArcFace: Additive Angular Margin Loss for Deep Face Recognition</li> <li>SphereFace: Deep Hypersphere Embedding for Face Recognition</li> <li>BiSeNet: Bilateral Segmentation Network for Real-time Semantic Segmentation</li> </ul>"},{"location":"notebooks/","title":"Interactive Notebooks","text":"<p>Run UniFace examples directly in your browser with Google Colab, or download and run locally with Jupyter.</p>"},{"location":"notebooks/#available-notebooks","title":"Available Notebooks","text":"Notebook Colab Description Face Detection Detect faces and 5-point landmarks Face Alignment Align faces for recognition Face Verification Compare faces for identity Face Search Find a person in group photos Face Analyzer All-in-one face analysis Face Parsing Semantic face segmentation Face Anonymization Privacy-preserving blur Gaze Estimation Gaze direction estimation Face Segmentation Face segmentation with XSeg"},{"location":"notebooks/#running-locally","title":"Running Locally","text":"<p>Download and run notebooks on your machine:</p> <pre><code># Clone the repository\ngit clone https://github.com/yakhyo/uniface.git\ncd uniface\n\n# Install dependencies\npip install uniface jupyter\n\n# Launch Jupyter\njupyter notebook examples/\n</code></pre>"},{"location":"notebooks/#running-on-google-colab","title":"Running on Google Colab","text":"<p>Click any \"Open in Colab\" badge above. The notebooks automatically:</p> <ol> <li>Install UniFace via pip</li> <li>Clone the repository to access test images</li> <li>Set up the correct working directory</li> </ol> <p>GPU Acceleration</p> <p>In Colab, go to Runtime \u2192 Change runtime type \u2192 GPU for faster inference.</p>"},{"location":"notebooks/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Code snippets for common use cases</li> <li>Tutorials - Step-by-step workflow guides</li> <li>API Reference - Detailed module documentation</li> </ul>"},{"location":"quickstart/","title":"Quickstart","text":"<p>Get up and running with UniFace in 5 minutes. This guide covers the most common use cases.</p>"},{"location":"quickstart/#face-detection","title":"Face Detection","text":"<p>Detect faces in an image:</p> <pre><code>import cv2\nfrom uniface.detection import RetinaFace\n\n# Load image\nimage = cv2.imread(\"photo.jpg\")\n\n# Initialize detector (models auto-download on first use)\ndetector = RetinaFace()\n\n# Detect faces\nfaces = detector.detect(image)\n\n# Print results\nfor i, face in enumerate(faces):\n    print(f\"Face {i+1}:\")\n    print(f\"  Confidence: {face.confidence:.2f}\")\n    print(f\"  BBox: {face.bbox}\")\n    print(f\"  Landmarks: {len(face.landmarks)} points\")\n</code></pre> <p>Output:</p> <pre><code>Face 1:\n  Confidence: 0.99\n  BBox: [120.5, 85.3, 245.8, 210.6]\n  Landmarks: 5 points\n</code></pre>"},{"location":"quickstart/#visualize-detections","title":"Visualize Detections","text":"<p>Draw bounding boxes and landmarks:</p> <pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.draw import draw_detections\n\n# Detect faces\ndetector = RetinaFace()\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\n# Extract visualization data\nbboxes = [f.bbox for f in faces]\nscores = [f.confidence for f in faces]\nlandmarks = [f.landmarks for f in faces]\n\n# Draw on image\ndraw_detections(\n    image=image,\n    bboxes=bboxes,\n    scores=scores,\n    landmarks=landmarks,\n    vis_threshold=0.6,\n)\n\n# Save result\ncv2.imwrite(\"output.jpg\", image)\n</code></pre>"},{"location":"quickstart/#face-recognition","title":"Face Recognition","text":"<p>Compare two faces:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\n\n# Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Load two images\nimage1 = cv2.imread(\"person1.jpg\")\nimage2 = cv2.imread(\"person2.jpg\")\n\n# Detect faces\nfaces1 = detector.detect(image1)\nfaces2 = detector.detect(image2)\n\nif faces1 and faces2:\n    # Extract embeddings\n    emb1 = recognizer.get_normalized_embedding(image1, faces1[0].landmarks)\n    emb2 = recognizer.get_normalized_embedding(image2, faces2[0].landmarks)\n\n    # Compute similarity (cosine similarity)\n    similarity = np.dot(emb1, emb2.T)[0][0]\n\n    # Interpret result\n    if similarity &gt; 0.6:\n        print(f\"Same person (similarity: {similarity:.3f})\")\n    else:\n        print(f\"Different people (similarity: {similarity:.3f})\")\n</code></pre> <p>Similarity Thresholds</p> <ul> <li><code>&gt; 0.6</code>: Same person (high confidence)</li> <li><code>0.4 - 0.6</code>: Uncertain (manual review)</li> <li><code>&lt; 0.4</code>: Different people</li> </ul>"},{"location":"quickstart/#age-gender-detection","title":"Age &amp; Gender Detection","text":"<pre><code>import cv2\nfrom uniface.attribute import AgeGender\nfrom uniface.detection import RetinaFace\n\n# Initialize models\ndetector = RetinaFace()\nage_gender = AgeGender()\n\n# Load image\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\n# Predict attributes\nfor i, face in enumerate(faces):\n    result = age_gender.predict(image, face.bbox)\n    print(f\"Face {i+1}: {result.sex}, {result.age} years old\")\n</code></pre> <p>Output:</p> <pre><code>Face 1: Male, 32 years old\nFace 2: Female, 28 years old\n</code></pre>"},{"location":"quickstart/#fairface-attributes","title":"FairFace Attributes","text":"<p>Detect race, gender, and age group:</p> <pre><code>import cv2\nfrom uniface.attribute import FairFace\nfrom uniface.detection import RetinaFace\n\ndetector = RetinaFace()\nfairface = FairFace()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    result = fairface.predict(image, face.bbox)\n    print(f\"Face {i+1}: {result.sex}, {result.age_group}, {result.race}\")\n</code></pre> <p>Output:</p> <pre><code>Face 1: Male, 30-39, East Asian\nFace 2: Female, 20-29, White\n</code></pre>"},{"location":"quickstart/#facial-landmarks-106-points","title":"Facial Landmarks (106 Points)","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.landmark import Landmark106\n\ndetector = RetinaFace()\nlandmarker = Landmark106()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nif faces:\n    landmarks = landmarker.get_landmarks(image, faces[0].bbox)\n    print(f\"Detected {len(landmarks)} landmarks\")\n\n    # Draw landmarks\n    for x, y in landmarks.astype(int):\n        cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n\n    cv2.imwrite(\"landmarks.jpg\", image)\n</code></pre>"},{"location":"quickstart/#gaze-estimation","title":"Gaze Estimation","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface.detection import RetinaFace\nfrom uniface.gaze import MobileGaze\nfrom uniface.draw import draw_gaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    x1, y1, x2, y2 = map(int, face.bbox[:4])\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        result = gaze_estimator.estimate(face_crop)\n        print(f\"Face {i+1}: pitch={np.degrees(result.pitch):.1f}\u00b0, yaw={np.degrees(result.yaw):.1f}\u00b0\")\n\n        # Draw gaze direction\n        draw_gaze(image, face.bbox, result.pitch, result.yaw)\n\ncv2.imwrite(\"gaze_output.jpg\", image)\n</code></pre>"},{"location":"quickstart/#face-parsing","title":"Face Parsing","text":"<p>Segment face into semantic components:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface.parsing import BiSeNet\nfrom uniface.draw import vis_parsing_maps\n\nparser = BiSeNet()\n\n# Load face image (already cropped)\nface_image = cv2.imread(\"face.jpg\")\n\n# Parse face into 19 components\nmask = parser.parse(face_image)\n\n# Visualize with overlay\nface_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\nvis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\nprint(f\"Detected {len(np.unique(mask))} facial components\")\n</code></pre>"},{"location":"quickstart/#face-anonymization","title":"Face Anonymization","text":"<p>Blur faces for privacy protection:</p> <pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='pixelate')\n\nimage = cv2.imread(\"group_photo.jpg\")\nfaces = detector.detect(image)\nanonymized = blurrer.anonymize(image, faces)\ncv2.imwrite(\"anonymized.jpg\", anonymized)\n</code></pre> <p>Custom blur settings:</p> <pre><code>blurrer = BlurFace(method='gaussian', blur_strength=5.0)\nanonymized = blurrer.anonymize(image, faces)\n</code></pre> <p>Available methods:</p> Method Description <code>pixelate</code> Blocky effect (news media standard) <code>gaussian</code> Smooth, natural blur <code>blackout</code> Solid color boxes (maximum privacy) <code>elliptical</code> Soft oval blur (natural face shape) <code>median</code> Edge-preserving blur"},{"location":"quickstart/#face-anti-spoofing","title":"Face Anti-Spoofing","text":"<p>Detect real vs. fake faces:</p> <pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    result = spoofer.predict(image, face.bbox)\n    label = 'Real' if result.is_real else 'Fake'\n    print(f\"Face {i+1}: {label} ({result.confidence:.1%})\")\n</code></pre>"},{"location":"quickstart/#webcam-demo","title":"Webcam Demo","text":"<p>Real-time face detection:</p> <pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.draw import draw_detections\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\nprint(\"Press 'q' to quit\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    bboxes = [f.bbox for f in faces]\n    scores = [f.confidence for f in faces]\n    landmarks = [f.landmarks for f in faces]\n    draw_detections(image=frame, bboxes=bboxes, scores=scores, landmarks=landmarks)\n\n    cv2.imshow(\"UniFace - Press 'q' to quit\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"quickstart/#face-tracking","title":"Face Tracking","text":"<p>Track faces across video frames with persistent IDs:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface.common import xyxy_to_cxcywh\nfrom uniface.detection import SCRFD\nfrom uniface.tracking import BYTETracker\nfrom uniface.draw import draw_tracks\n\ndetector = SCRFD()\ntracker = BYTETracker(track_thresh=0.5, track_buffer=30)\n\ncap = cv2.VideoCapture(\"video.mp4\")\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    dets = np.array([[*f.bbox, f.confidence] for f in faces])\n    dets = dets if len(dets) &gt; 0 else np.empty((0, 5))\n\n    tracks = tracker.update(dets)\n\n    # Assign track IDs to faces\n    if len(tracks) &gt; 0 and len(faces) &gt; 0:\n        face_bboxes = np.array([f.bbox for f in faces], dtype=np.float32)\n        track_ids = tracks[:, 4].astype(int)\n\n        face_centers = xyxy_to_cxcywh(face_bboxes)[:, :2]\n        track_centers = xyxy_to_cxcywh(tracks[:, :4])[:, :2]\n\n        for ti in range(len(tracks)):\n            dists = (track_centers[ti, 0] - face_centers[:, 0]) ** 2 + (track_centers[ti, 1] - face_centers[:, 1]) ** 2\n            faces[int(np.argmin(dists))].track_id = track_ids[ti]\n\n    tracked_faces = [f for f in faces if f.track_id is not None]\n    draw_tracks(image=frame, faces=tracked_faces)\n    cv2.imshow(\"Tracking\", frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>For more details, see the Tracking module.</p>"},{"location":"quickstart/#model-selection","title":"Model Selection","text":"<p>For detailed model comparisons and benchmarks, see the Model Zoo.</p> <p>Available models by task:</p> Task Available Models Detection <code>RetinaFace</code>, <code>SCRFD</code>, <code>YOLOv5Face</code>, <code>YOLOv8Face</code> Recognition <code>ArcFace</code>, <code>AdaFace</code>, <code>MobileFace</code>, <code>SphereFace</code> Tracking <code>BYTETracker</code> Gaze <code>MobileGaze</code> (ResNet18/34/50, MobileNetV2, MobileOneS0) Parsing <code>BiSeNet</code> (ResNet18/34) Attributes <code>AgeGender</code>, <code>FairFace</code>, <code>Emotion</code> Anti-Spoofing <code>MiniFASNet</code> (V1SE, V2)"},{"location":"quickstart/#common-issues","title":"Common Issues","text":""},{"location":"quickstart/#models-not-downloading","title":"Models Not Downloading","text":"<pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\n# Manually download a model\nmodel_path = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Model downloaded to: {model_path}\")\n</code></pre>"},{"location":"quickstart/#check-hardware-acceleration","title":"Check Hardware Acceleration","text":"<pre><code>import onnxruntime as ort\nprint(\"Available providers:\", ort.get_available_providers())\n\n# macOS M-series should show: ['CoreMLExecutionProvider', ...]\n# NVIDIA GPU should show: ['CUDAExecutionProvider', ...]\n</code></pre>"},{"location":"quickstart/#slow-performance-on-mac","title":"Slow Performance on Mac","text":"<p>Verify you're using the ARM64 build of Python:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64 (not x86_64)\n</code></pre>"},{"location":"quickstart/#import-errors","title":"Import Errors","text":"<pre><code>from uniface.detection import RetinaFace, SCRFD\nfrom uniface.recognition import ArcFace, AdaFace\nfrom uniface.attribute import AgeGender, FairFace\nfrom uniface.landmark import Landmark106\nfrom uniface.gaze import MobileGaze\nfrom uniface.parsing import BiSeNet, XSeg\nfrom uniface.privacy import BlurFace\nfrom uniface.spoofing import MiniFASNet\nfrom uniface.tracking import BYTETracker\nfrom uniface.analyzer import FaceAnalyzer\nfrom uniface.draw import draw_detections, draw_tracks\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Model Zoo - All models, benchmarks, and selection guide</li> <li>API Reference - Explore individual modules and their APIs</li> <li>Tutorials - Step-by-step examples for common workflows</li> <li>Guides - Learn about the architecture and design principles</li> </ul>"},{"location":"concepts/coordinate-systems/","title":"Coordinate Systems","text":"<p>This page explains the coordinate formats used in UniFace.</p>"},{"location":"concepts/coordinate-systems/#image-coordinates","title":"Image Coordinates","text":"<p>All coordinates use pixel-based, top-left origin:</p> <pre><code>(0, 0) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba x (width)\n   \u2502\n   \u2502    Image\n   \u2502\n   \u25bc\n   y (height)\n</code></pre>"},{"location":"concepts/coordinate-systems/#bounding-box-format","title":"Bounding Box Format","text":"<p>Bounding boxes use <code>[x1, y1, x2, y2]</code> format (top-left and bottom-right corners):</p> <pre><code>(x1, y1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                     \u2502\n    \u2502      Face           \u2502\n    \u2502                     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 (x2, y2)\n</code></pre>"},{"location":"concepts/coordinate-systems/#accessing-coordinates","title":"Accessing Coordinates","text":"<pre><code>face = faces[0]\n\n# Direct access\nx1, y1, x2, y2 = face.bbox\n\n# As properties\nbbox_xyxy = face.bbox_xyxy  # [x1, y1, x2, y2]\nbbox_xywh = face.bbox_xywh  # [x1, y1, width, height]\n</code></pre>"},{"location":"concepts/coordinate-systems/#conversion","title":"Conversion","text":"<pre><code>import numpy as np\n\n# xyxy \u2192 xywh\ndef xyxy_to_xywh(bbox):\n    x1, y1, x2, y2 = bbox\n    return np.array([x1, y1, x2 - x1, y2 - y1])\n\n# xywh \u2192 xyxy\ndef xywh_to_xyxy(bbox):\n    x, y, w, h = bbox\n    return np.array([x, y, x + w, y + h])\n</code></pre>"},{"location":"concepts/coordinate-systems/#landmarks","title":"Landmarks","text":""},{"location":"concepts/coordinate-systems/#5-point-landmarks-detection","title":"5-Point Landmarks (Detection)","text":"<p>Returned by all detection models:</p> <pre><code>landmarks = face.landmarks  # Shape: (5, 2)\n</code></pre> Index Point 0 Left Eye 1 Right Eye 2 Nose Tip 3 Left Mouth Corner 4 Right Mouth Corner <pre><code>      0 \u25cf           \u25cf 1\n\n            \u25cf 2\n\n        3 \u25cf     \u25cf 4\n</code></pre>"},{"location":"concepts/coordinate-systems/#106-point-landmarks","title":"106-Point Landmarks","text":"<p>Returned by <code>Landmark106</code>:</p> <pre><code>from uniface.landmark import Landmark106\n\nlandmarker = Landmark106()\nlandmarks = landmarker.get_landmarks(image, face.bbox)\n# Shape: (106, 2)\n</code></pre> <p>Landmark Groups:</p> Range Group Points 0-32 Face Contour 33 33-50 Eyebrows 18 51-62 Nose 12 63-86 Eyes 24 87-105 Mouth 19"},{"location":"concepts/coordinate-systems/#face-crop","title":"Face Crop","text":"<p>To crop a face from an image:</p> <pre><code>def crop_face(image, bbox, margin=0):\n    \"\"\"Crop face with optional margin.\"\"\"\n    h, w = image.shape[:2]\n    x1, y1, x2, y2 = map(int, bbox)\n\n    # Add margin\n    if margin &gt; 0:\n        bw, bh = x2 - x1, y2 - y1\n        x1 = max(0, x1 - int(bw * margin))\n        y1 = max(0, y1 - int(bh * margin))\n        x2 = min(w, x2 + int(bw * margin))\n        y2 = min(h, y2 + int(bh * margin))\n\n    return image[y1:y2, x1:x2]\n\n# Usage\nface_crop = crop_face(image, face.bbox, margin=0.1)\n</code></pre>"},{"location":"concepts/coordinate-systems/#gaze-angles","title":"Gaze Angles","text":"<p>Gaze estimation returns pitch and yaw in radians:</p> <pre><code>result = gaze_estimator.estimate(face_crop)\n\n# Angles in radians\npitch = result.pitch  # Vertical: + = up, - = down\nyaw = result.yaw      # Horizontal: + = right, - = left\n\n# Convert to degrees\nimport numpy as np\npitch_deg = np.degrees(pitch)\nyaw_deg = np.degrees(yaw)\n</code></pre> <p>Angle Reference:</p> <pre><code>          pitch = +90\u00b0 (up)\n               \u2502\n               \u2502\nyaw = -90\u00b0 \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500 yaw = +90\u00b0\n(left)         \u2502      (right)\n               \u2502\n          pitch = -90\u00b0 (down)\n</code></pre>"},{"location":"concepts/coordinate-systems/#face-alignment","title":"Face Alignment","text":"<p>Face alignment uses 5-point landmarks to normalize face orientation:</p> <pre><code>from uniface.face_utils import face_alignment\n\n# Align face to standard template\naligned_face = face_alignment(image, face.landmarks)\n# Output: 112x112 aligned face image\n</code></pre> <p>The alignment transforms faces to a canonical pose for better recognition accuracy.</p>"},{"location":"concepts/coordinate-systems/#next-steps","title":"Next Steps","text":"<ul> <li>Inputs &amp; Outputs - Data types reference</li> <li>Recognition Module - Face recognition details</li> </ul>"},{"location":"concepts/execution-providers/","title":"Execution Providers","text":"<p>UniFace uses ONNX Runtime for model inference, which supports multiple hardware acceleration backends.</p>"},{"location":"concepts/execution-providers/#automatic-provider-selection","title":"Automatic Provider Selection","text":"<p>UniFace automatically selects the optimal execution provider based on available hardware:</p> <pre><code>from uniface.detection import RetinaFace\n\n# Automatically uses best available provider\ndetector = RetinaFace()\n</code></pre> <p>Priority order:</p> <ol> <li>CoreMLExecutionProvider - Apple Silicon</li> <li>CUDAExecutionProvider - NVIDIA GPU</li> <li>CPUExecutionProvider - Fallback</li> </ol>"},{"location":"concepts/execution-providers/#explicit-provider-selection","title":"Explicit Provider Selection","text":"<p>You can specify which execution provider to use by passing the <code>providers</code> parameter:</p> <pre><code>from uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\n\n# Force CPU execution (even if GPU is available)\ndetector = RetinaFace(providers=['CPUExecutionProvider'])\nrecognizer = ArcFace(providers=['CPUExecutionProvider'])\n\n# Use CUDA with CPU fallback\ndetector = RetinaFace(providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n</code></pre> <p>All model classes accept the <code>providers</code> parameter:</p> <ul> <li>Detection: <code>RetinaFace</code>, <code>SCRFD</code>, <code>YOLOv5Face</code>, <code>YOLOv8Face</code></li> <li>Recognition: <code>ArcFace</code>, <code>AdaFace</code>, <code>MobileFace</code>, <code>SphereFace</code></li> <li>Landmarks: <code>Landmark106</code></li> <li>Gaze: <code>MobileGaze</code></li> <li>Parsing: <code>BiSeNet</code></li> <li>Attributes: <code>AgeGender</code>, <code>FairFace</code></li> <li>Anti-Spoofing: <code>MiniFASNet</code></li> </ul>"},{"location":"concepts/execution-providers/#check-available-providers","title":"Check Available Providers","text":"<pre><code>import onnxruntime as ort\n\nproviders = ort.get_available_providers()\nprint(\"Available providers:\", providers)\n</code></pre> <p>Example outputs:</p> macOS (Apple Silicon)Linux (NVIDIA GPU)Windows (CPU) <pre><code>['CoreMLExecutionProvider', 'CPUExecutionProvider']\n</code></pre> <pre><code>['CUDAExecutionProvider', 'CPUExecutionProvider']\n</code></pre> <pre><code>['CPUExecutionProvider']\n</code></pre>"},{"location":"concepts/execution-providers/#platform-specific-setup","title":"Platform-Specific Setup","text":""},{"location":"concepts/execution-providers/#apple-silicon-m1m2m3m4","title":"Apple Silicon (M1/M2/M3/M4)","text":"<p>No additional setup required. ARM64 optimizations are built into <code>onnxruntime</code>:</p> <pre><code>pip install uniface\n</code></pre> <p>Verify ARM64:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64\n</code></pre> <p>Performance</p> <p>Apple Silicon Macs use CoreML acceleration automatically, providing excellent performance for face analysis tasks.</p>"},{"location":"concepts/execution-providers/#nvidia-gpu-cuda","title":"NVIDIA GPU (CUDA)","text":"<p>Install with GPU support:</p> <pre><code>pip install uniface[gpu]\n</code></pre> <p>Requirements:</p> <ul> <li>CUDA 11.x or 12.x</li> <li>cuDNN 8.x</li> <li>Compatible NVIDIA driver</li> </ul> <p>Verify CUDA:</p> <pre><code>import onnxruntime as ort\n\nif 'CUDAExecutionProvider' in ort.get_available_providers():\n    print(\"CUDA is available!\")\nelse:\n    print(\"CUDA not available, using CPU\")\n</code></pre>"},{"location":"concepts/execution-providers/#cpu-fallback","title":"CPU Fallback","text":"<p>CPU execution is always available:</p> <pre><code>pip install uniface\n</code></pre> <p>Works on all platforms without additional configuration.</p>"},{"location":"concepts/execution-providers/#internal-api","title":"Internal API","text":"<p>For advanced use cases, you can access the provider utilities:</p> <pre><code>from uniface.onnx_utils import get_available_providers, create_onnx_session\n\n# Check available providers\nproviders = get_available_providers()\nprint(f\"Available: {providers}\")\n\n# Models use create_onnx_session() internally\n# which auto-selects the best provider\n</code></pre>"},{"location":"concepts/execution-providers/#performance-tips","title":"Performance Tips","text":""},{"location":"concepts/execution-providers/#1-use-gpu-when-available","title":"1. Use GPU When Available","text":"<p>For batch processing or real-time applications, GPU acceleration provides significant speedups:</p> <pre><code>pip install uniface[gpu]\n</code></pre>"},{"location":"concepts/execution-providers/#2-optimize-input-size","title":"2. Optimize Input Size","text":"<p>Smaller input sizes are faster but may reduce accuracy:</p> <pre><code>from uniface.detection import RetinaFace\n\n# Faster, lower accuracy\ndetector = RetinaFace(input_size=(320, 320))\n\n# Balanced (default)\ndetector = RetinaFace(input_size=(640, 640))\n</code></pre>"},{"location":"concepts/execution-providers/#3-batch-processing","title":"3. Batch Processing","text":"<p>Process multiple images to maximize GPU utilization:</p> <pre><code># Process images in batch (GPU-efficient)\nfor image_path in image_paths:\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n    # ...\n</code></pre>"},{"location":"concepts/execution-providers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/execution-providers/#cuda-not-detected","title":"CUDA Not Detected","text":"<ol> <li> <p>Verify CUDA installation:    <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Check CUDA version compatibility with ONNX Runtime</p> </li> <li> <p>Reinstall with GPU support:    <pre><code>pip uninstall onnxruntime onnxruntime-gpu\npip install uniface[gpu]\n</code></pre></p> </li> </ol>"},{"location":"concepts/execution-providers/#slow-performance-on-mac","title":"Slow Performance on Mac","text":"<p>Verify you're using ARM64 Python (not Rosetta):</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64 (not x86_64)\n</code></pre>"},{"location":"concepts/execution-providers/#next-steps","title":"Next Steps","text":"<ul> <li>Model Cache &amp; Offline - Model management</li> <li>Thresholds &amp; Calibration - Tuning parameters</li> </ul>"},{"location":"concepts/inputs-outputs/","title":"Inputs &amp; Outputs","text":"<p>This page describes the data types used throughout UniFace.</p>"},{"location":"concepts/inputs-outputs/#input-images","title":"Input: Images","text":"<p>All models accept NumPy arrays in BGR format (OpenCV default):</p> <pre><code>import cv2\n\n# Load image (BGR format)\nimage = cv2.imread(\"photo.jpg\")\nprint(f\"Shape: {image.shape}\")  # (H, W, 3)\nprint(f\"Dtype: {image.dtype}\")  # uint8\n</code></pre> <p>Color Format</p> <p>UniFace expects BGR format (OpenCV default). If using PIL or other libraries, convert first:</p> <pre><code>from PIL import Image\nimport numpy as np\n\npil_image = Image.open(\"photo.jpg\")\nbgr_image = np.array(pil_image)[:, :, ::-1]  # RGB \u2192 BGR\n</code></pre>"},{"location":"concepts/inputs-outputs/#output-face-dataclass","title":"Output: Face Dataclass","text":"<p>Detection returns a list of <code>Face</code> objects:</p> <pre><code>from dataclasses import dataclass\nimport numpy as np\n\n@dataclass\nclass Face:\n    # Required (from detection)\n    bbox: np.ndarray        # [x1, y1, x2, y2]\n    confidence: float       # 0.0 to 1.0\n    landmarks: np.ndarray   # (5, 2) or (106, 2)\n\n    # Optional (enriched by analyzers)\n    embedding: np.ndarray | None = None\n    gender: int | None = None           # 0=Female, 1=Male\n    age: int | None = None              # Years\n    age_group: str | None = None        # \"20-29\", etc.\n    race: str | None = None             # \"East Asian\", etc.\n    emotion: str | None = None          # \"Happy\", etc.\n    emotion_confidence: float | None = None\n    track_id: int | None = None         # Persistent ID from tracker\n</code></pre>"},{"location":"concepts/inputs-outputs/#properties","title":"Properties","text":"<pre><code>face = faces[0]\n\n# Bounding box formats\nface.bbox_xyxy  # [x1, y1, x2, y2] - same as bbox\nface.bbox_xywh  # [x1, y1, width, height]\n\n# Gender as string\nface.sex  # \"Female\" or \"Male\" (None if not predicted)\n</code></pre>"},{"location":"concepts/inputs-outputs/#methods","title":"Methods","text":"<pre><code># Compute similarity with another face\nsimilarity = face1.compute_similarity(face2)\n\n# Convert to dictionary\nface_dict = face.to_dict()\n</code></pre>"},{"location":"concepts/inputs-outputs/#result-types","title":"Result Types","text":""},{"location":"concepts/inputs-outputs/#gazeresult","title":"GazeResult","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass GazeResult:\n    pitch: float  # Vertical angle (radians), + = up\n    yaw: float    # Horizontal angle (radians), + = right\n</code></pre> <p>Usage:</p> <pre><code>import numpy as np\n\nresult = gaze_estimator.estimate(face_crop)\nprint(f\"Pitch: {np.degrees(result.pitch):.1f}\u00b0\")\nprint(f\"Yaw: {np.degrees(result.yaw):.1f}\u00b0\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#spoofingresult","title":"SpoofingResult","text":"<pre><code>@dataclass(frozen=True)\nclass SpoofingResult:\n    is_real: bool      # True = real, False = fake\n    confidence: float  # 0.0 to 1.0\n</code></pre> <p>Usage:</p> <pre><code>result = spoofer.predict(image, face.bbox)\nlabel = \"Real\" if result.is_real else \"Fake\"\nprint(f\"{label}: {result.confidence:.1%}\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#attributeresult","title":"AttributeResult","text":"<pre><code>@dataclass(frozen=True)\nclass AttributeResult:\n    gender: int              # 0=Female, 1=Male\n    age: int | None          # Years (AgeGender model)\n    age_group: str | None    # \"20-29\" (FairFace model)\n    race: str | None         # Race label (FairFace model)\n\n    @property\n    def sex(self) -&gt; str:\n        return \"Female\" if self.gender == 0 else \"Male\"\n</code></pre> <p>Usage:</p> <pre><code># AgeGender model\nresult = age_gender.predict(image, face.bbox)\nprint(f\"{result.sex}, {result.age} years old\")\n\n# FairFace model\nresult = fairface.predict(image, face.bbox)\nprint(f\"{result.sex}, {result.age_group}, {result.race}\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#emotionresult","title":"EmotionResult","text":"<pre><code>@dataclass(frozen=True)\nclass EmotionResult:\n    emotion: str       # \"Happy\", \"Sad\", etc.\n    confidence: float  # 0.0 to 1.0\n</code></pre>"},{"location":"concepts/inputs-outputs/#embeddings","title":"Embeddings","text":"<p>Face recognition models return normalized 512-dimensional embeddings:</p> <pre><code>embedding = recognizer.get_normalized_embedding(image, landmarks)\nprint(f\"Shape: {embedding.shape}\")  # (1, 512)\nprint(f\"Norm: {np.linalg.norm(embedding):.4f}\")  # ~1.0\n</code></pre>"},{"location":"concepts/inputs-outputs/#similarity-computation","title":"Similarity Computation","text":"<pre><code>from uniface.face_utils import compute_similarity\n\nsimilarity = compute_similarity(embedding1, embedding2)\n# Returns: float between -1 and 1 (cosine similarity)\n</code></pre>"},{"location":"concepts/inputs-outputs/#parsing-masks","title":"Parsing Masks","text":"<p>Face parsing returns a segmentation mask:</p> <pre><code>mask = parser.parse(face_image)\nprint(f\"Shape: {mask.shape}\")  # (H, W)\nprint(f\"Classes: {np.unique(mask)}\")  # [0, 1, 2, ...]\n</code></pre> <p>19 Classes:</p> ID Class ID Class 0 Background 10 Nose 1 Skin 11 Mouth 2 Left Eyebrow 12 Upper Lip 3 Right Eyebrow 13 Lower Lip 4 Left Eye 14 Neck 5 Right Eye 15 Necklace 6 Eyeglasses 16 Cloth 7 Left Ear 17 Hair 8 Right Ear 18 Hat 9 Earring"},{"location":"concepts/inputs-outputs/#next-steps","title":"Next Steps","text":"<ul> <li>Coordinate Systems - Bbox and landmark formats</li> <li>Thresholds &amp; Calibration - Tuning confidence thresholds</li> </ul>"},{"location":"concepts/model-cache-offline/","title":"Model Cache &amp; Offline Use","text":"<p>UniFace automatically downloads and caches models. This page explains how model management works.</p>"},{"location":"concepts/model-cache-offline/#automatic-download","title":"Automatic Download","text":"<p>Models are downloaded on first use:</p> <pre><code>from uniface.detection import RetinaFace\n\n# First run: downloads model to cache\ndetector = RetinaFace()  # ~3.5 MB download\n\n# Subsequent runs: loads from cache\ndetector = RetinaFace()  # Instant\n</code></pre>"},{"location":"concepts/model-cache-offline/#cache-location","title":"Cache Location","text":"<p>Default cache directory:</p> <pre><code>~/.uniface/models/\n</code></pre> <p>Example structure:</p> <pre><code>~/.uniface/models/\n\u251c\u2500\u2500 retinaface_mnet_v2.onnx\n\u251c\u2500\u2500 arcface_mnet.onnx\n\u251c\u2500\u2500 2d_106.onnx\n\u251c\u2500\u2500 gaze_resnet34.onnx\n\u251c\u2500\u2500 parsing_resnet18.onnx\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"concepts/model-cache-offline/#custom-cache-directory","title":"Custom Cache Directory","text":"<p>Use the programmatic API to change the cache location at runtime:</p> <pre><code>from uniface.model_store import get_cache_dir, set_cache_dir\n\n# Set a custom cache directory\nset_cache_dir('/data/models')\n\n# Verify the current path\nprint(get_cache_dir())  # /data/models\n\n# All subsequent model loads use the new directory\nfrom uniface.detection import RetinaFace\ndetector = RetinaFace()  # Downloads to /data/models/\n</code></pre> <p>Or set the <code>UNIFACE_CACHE_DIR</code> environment variable (see Environment Variables below).</p>"},{"location":"concepts/model-cache-offline/#pre-download-models","title":"Pre-Download Models","text":"<p>Download models before deployment using the concurrent downloader:</p> <pre><code>from uniface.model_store import download_models\nfrom uniface.constants import (\n    RetinaFaceWeights,\n    ArcFaceWeights,\n    AgeGenderWeights,\n)\n\n# Download multiple models concurrently (up to 4 threads by default)\npaths = download_models([\n    RetinaFaceWeights.MNET_V2,\n    ArcFaceWeights.MNET,\n    AgeGenderWeights.DEFAULT,\n])\n\nfor model, path in paths.items():\n    print(f\"{model.value} -&gt; {path}\")\n</code></pre> <p>Or download one at a time:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\npath = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Downloaded: {path}\")\n</code></pre> <p>Or use the CLI tool:</p> <pre><code>python tools/download_model.py\n</code></pre>"},{"location":"concepts/model-cache-offline/#offline-use","title":"Offline Use","text":"<p>For air-gapped or offline environments:</p>"},{"location":"concepts/model-cache-offline/#1-pre-download-models","title":"1. Pre-download models","text":"<p>On a connected machine:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\npath = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Copy from: {path}\")\n</code></pre>"},{"location":"concepts/model-cache-offline/#2-copy-to-target-machine","title":"2. Copy to target machine","text":"<pre><code># Copy the entire cache directory\nscp -r ~/.uniface/models/ user@offline-machine:~/.uniface/models/\n</code></pre>"},{"location":"concepts/model-cache-offline/#3-point-to-the-cache-if-non-default-location","title":"3. Point to the cache (if non-default location)","text":"<pre><code>from uniface.model_store import set_cache_dir\n\n# Only needed if the models are not at ~/.uniface/models/\nset_cache_dir('/path/to/copied/models')\n</code></pre>"},{"location":"concepts/model-cache-offline/#4-use-normally","title":"4. Use normally","text":"<pre><code># Models load from local cache\nfrom uniface.detection import RetinaFace\ndetector = RetinaFace()  # No network required\n</code></pre>"},{"location":"concepts/model-cache-offline/#model-verification","title":"Model Verification","text":"<p>Models are verified with SHA-256 checksums:</p> <pre><code>from uniface.constants import MODEL_SHA256, RetinaFaceWeights\n\n# Check expected checksum\nexpected = MODEL_SHA256[RetinaFaceWeights.MNET_V2]\nprint(f\"Expected SHA256: {expected}\")\n</code></pre> <p>If a model fails verification, it's re-downloaded automatically.</p>"},{"location":"concepts/model-cache-offline/#available-models","title":"Available Models","text":""},{"location":"concepts/model-cache-offline/#detection-models","title":"Detection Models","text":"Model Size Download RetinaFace MNET_025 1.7 MB \u2705 RetinaFace MNET_V2 3.5 MB \u2705 RetinaFace RESNET34 56 MB \u2705 SCRFD 500M 2.5 MB \u2705 SCRFD 10G 17 MB \u2705 YOLOv5n-Face 11 MB \u2705 YOLOv5s-Face 28 MB \u2705 YOLOv5m-Face 82 MB \u2705 YOLOv8-Lite-S 7.4 MB \u2705 YOLOv8n-Face 12 MB \u2705"},{"location":"concepts/model-cache-offline/#recognition-models","title":"Recognition Models","text":"Model Size Download ArcFace MNET 8 MB \u2705 ArcFace RESNET 166 MB \u2705 MobileFace MNET_V2 4 MB \u2705 SphereFace SPHERE20 50 MB \u2705"},{"location":"concepts/model-cache-offline/#other-models","title":"Other Models","text":"Model Size Download Landmark106 14 MB \u2705 AgeGender 8 MB \u2705 FairFace 44 MB \u2705 Gaze ResNet34 82 MB \u2705 BiSeNet ResNet18 51 MB \u2705 MiniFASNet V2 1.2 MB \u2705"},{"location":"concepts/model-cache-offline/#clear-cache","title":"Clear Cache","text":"<p>Find and remove cached models:</p> <pre><code>from uniface.model_store import get_cache_dir\nprint(get_cache_dir())  # shows the active cache path\n</code></pre> <pre><code># Remove all cached models\nrm -rf ~/.uniface/models/\n\n# Remove specific model\nrm ~/.uniface/models/retinaface_mv2.onnx\n</code></pre> <p>Models will be re-downloaded on next use.</p>"},{"location":"concepts/model-cache-offline/#environment-variables","title":"Environment Variables","text":"<p>There are three equivalent ways to configure the cache directory:</p> <p>1. Programmatic API (recommended)</p> <pre><code>from uniface.model_store import get_cache_dir, set_cache_dir\n\nset_cache_dir('/path/to/custom/cache')\nprint(get_cache_dir())  # /path/to/custom/cache\n</code></pre> <p>2. Direct environment variable (Python)</p> <pre><code>import os\nos.environ['UNIFACE_CACHE_DIR'] = '/path/to/custom/cache'\n\nfrom uniface.detection import RetinaFace\ndetector = RetinaFace()  # Uses custom cache\n</code></pre> <p>3. Shell environment variable</p> <pre><code>export UNIFACE_CACHE_DIR=/path/to/custom/cache\n</code></pre> <p>All three methods set the same <code>UNIFACE_CACHE_DIR</code> environment variable under the hood. <code>get_cache_dir()</code> always returns the resolved path.</p>"},{"location":"concepts/model-cache-offline/#next-steps","title":"Next Steps","text":"<ul> <li>Thresholds &amp; Calibration - Tune model parameters</li> <li>Detection Module - Detection model details</li> </ul>"},{"location":"concepts/overview/","title":"Overview","text":"<p>UniFace is designed as a modular, production-ready face analysis library. This page explains the architecture and design principles.</p>"},{"location":"concepts/overview/#architecture","title":"Architecture","text":"<p>UniFace follows a modular architecture where each face analysis task is handled by a dedicated module:</p> <pre><code>graph TB\n    subgraph Input\n        IMG[Image/Frame]\n    end\n\n    subgraph Detection\n        DET[RetinaFace / SCRFD / YOLOv5Face / YOLOv8Face]\n    end\n\n    subgraph Analysis\n        REC[Recognition]\n        LMK[Landmarks]\n        ATTR[Attributes]\n        GAZE[Gaze]\n        PARSE[Parsing]\n        SPOOF[Anti-Spoofing]\n        PRIV[Privacy]\n    end\n\n    subgraph Tracking\n        TRK[BYTETracker]\n    end\n\n    subgraph Output\n        FACE[Face Objects]\n    end\n\n    IMG --&gt; DET\n    DET --&gt; REC\n    DET --&gt; LMK\n    DET --&gt; ATTR\n    DET --&gt; GAZE\n    DET --&gt; PARSE\n    DET --&gt; SPOOF\n    DET --&gt; PRIV\n    DET --&gt; TRK\n    REC --&gt; FACE\n    LMK --&gt; FACE\n    ATTR --&gt; FACE\n    TRK --&gt; FACE</code></pre>"},{"location":"concepts/overview/#design-principles","title":"Design Principles","text":""},{"location":"concepts/overview/#1-onnx-first","title":"1. ONNX-First","text":"<p>All models use ONNX Runtime for inference:</p> <ul> <li>Cross-platform: Same models work on macOS, Linux, Windows</li> <li>Hardware acceleration: Automatic selection of optimal provider</li> <li>Production-ready: No Python-only dependencies for inference</li> </ul>"},{"location":"concepts/overview/#2-minimal-dependencies","title":"2. Minimal Dependencies","text":"<p>Core dependencies are kept minimal:</p> <pre><code>numpy         # Array operations\nopencv-python # Image processing\nonnxruntime   # Model inference\nrequests      # Model download\ntqdm          # Progress bars\n</code></pre>"},{"location":"concepts/overview/#3-simple-api","title":"3. Simple API","text":"<p>Factory functions and direct instantiation:</p> <pre><code>from uniface.detection import RetinaFace\n\ndetector = RetinaFace()\n\n# Or via factory function\nfrom uniface.detection import create_detector\n\ndetector = create_detector('retinaface')\n</code></pre>"},{"location":"concepts/overview/#4-type-safety","title":"4. Type Safety","text":"<p>Full type hints throughout:</p> <pre><code>def detect(self, image: np.ndarray) -&gt; list[Face]:\n    ...\n</code></pre>"},{"location":"concepts/overview/#module-structure","title":"Module Structure","text":"<pre><code>uniface/\n\u251c\u2500\u2500 detection/      # Face detection (RetinaFace, SCRFD, YOLOv5Face, YOLOv8Face)\n\u251c\u2500\u2500 recognition/    # Face recognition (AdaFace, ArcFace, MobileFace, SphereFace)\n\u251c\u2500\u2500 tracking/       # Multi-object tracking (BYTETracker)\n\u251c\u2500\u2500 landmark/       # 106-point landmarks\n\u251c\u2500\u2500 attribute/      # Age, gender, emotion, race\n\u251c\u2500\u2500 parsing/        # Face semantic segmentation\n\u251c\u2500\u2500 gaze/           # Gaze estimation\n\u251c\u2500\u2500 spoofing/       # Anti-spoofing\n\u251c\u2500\u2500 privacy/        # Face anonymization\n\u251c\u2500\u2500 types.py        # Dataclasses (Face, GazeResult, etc.)\n\u251c\u2500\u2500 constants.py    # Model weights and URLs\n\u251c\u2500\u2500 model_store.py  # Model download and caching\n\u251c\u2500\u2500 onnx_utils.py   # ONNX Runtime utilities\n\u2514\u2500\u2500 draw.py         # Drawing utilities\n</code></pre>"},{"location":"concepts/overview/#workflow","title":"Workflow","text":"<p>A typical face analysis workflow:</p> <pre><code>import cv2\nfrom uniface.attribute import AgeGender\nfrom uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\n\n# 1. Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\nage_gender = AgeGender()\n\n# 2. Load image\nimage = cv2.imread(\"photo.jpg\")\n\n# 3. Detect faces\nfaces = detector.detect(image)\n\n# 4. Analyze each face\nfor face in faces:\n    # Recognition embedding\n    embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n    # Attributes\n    attrs = age_gender.predict(image, face.bbox)\n\n    print(f\"Face: {attrs.sex}, {attrs.age} years\")\n</code></pre>"},{"location":"concepts/overview/#faceanalyzer","title":"FaceAnalyzer","text":"<p>For convenience, <code>FaceAnalyzer</code> combines multiple modules:</p> <pre><code>from uniface.analyzer import FaceAnalyzer\nfrom uniface.attribute import AgeGender, FairFace\nfrom uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\nage_gender = AgeGender()\nfairface = FairFace()\n\nanalyzer = FaceAnalyzer(\n    detector,\n    recognizer=recognizer,\n    age_gender=age_gender,\n    fairface=fairface,\n)\n\nfaces = analyzer.analyze(image)\nfor face in faces:\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n    print(f\"Embedding: {face.embedding.shape}\")\n</code></pre>"},{"location":"concepts/overview/#model-lifecycle","title":"Model Lifecycle","text":"<ol> <li>First use: Model is downloaded from GitHub releases</li> <li>Cached: Stored in <code>~/.uniface/models/</code> (configurable via <code>set_cache_dir()</code> or <code>UNIFACE_CACHE_DIR</code>)</li> <li>Verified: SHA-256 checksum validation</li> <li>Loaded: ONNX Runtime session created</li> <li>Inference: Hardware-accelerated execution</li> </ol> <pre><code># Models auto-download on first use\ndetector = RetinaFace()  # Downloads if not cached\n\n# Optionally configure cache location\nfrom uniface.model_store import get_cache_dir, set_cache_dir\nset_cache_dir('/data/models')\nprint(get_cache_dir())  # /data/models\n\n# Or manually pre-download\nfrom uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\npath = verify_model_weights(RetinaFaceWeights.MNET_V2)\n</code></pre>"},{"location":"concepts/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Inputs &amp; Outputs - Understand data types</li> <li>Execution Providers - Hardware acceleration</li> <li>Detection Module - Start with face detection</li> <li>Image Pipeline Recipe - Complete workflow</li> </ul>"},{"location":"concepts/thresholds-calibration/","title":"Thresholds &amp; Calibration","text":"<p>This page explains how to tune detection and recognition thresholds for your use case.</p>"},{"location":"concepts/thresholds-calibration/#detection-thresholds","title":"Detection Thresholds","text":""},{"location":"concepts/thresholds-calibration/#confidence-threshold","title":"Confidence Threshold","text":"<p>Controls minimum confidence for face detection:</p> <pre><code>from uniface.detection import RetinaFace\n\n# Default (balanced)\ndetector = RetinaFace(confidence_threshold=0.5)\n\n# High precision (fewer false positives)\ndetector = RetinaFace(confidence_threshold=0.8)\n\n# High recall (catch more faces)\ndetector = RetinaFace(confidence_threshold=0.3)\n</code></pre> <p>Guidelines:</p> Threshold Use Case 0.3 - 0.4 Maximum recall (research, analysis) 0.5 - 0.6 Balanced (default, general use) 0.7 - 0.9 High precision (production, security)"},{"location":"concepts/thresholds-calibration/#nms-threshold","title":"NMS Threshold","text":"<p>Non-Maximum Suppression removes overlapping detections:</p> <pre><code># Default\ndetector = RetinaFace(nms_threshold=0.4)\n\n# Stricter (fewer overlapping boxes)\ndetector = RetinaFace(nms_threshold=0.3)\n\n# Looser (for crowded scenes)\ndetector = RetinaFace(nms_threshold=0.5)\n</code></pre>"},{"location":"concepts/thresholds-calibration/#input-size","title":"Input Size","text":"<p>Affects detection accuracy and speed:</p> <pre><code># Faster, lower accuracy\ndetector = RetinaFace(input_size=(320, 320))\n\n# Balanced (default)\ndetector = RetinaFace(input_size=(640, 640))\n\n# Higher accuracy, slower\ndetector = RetinaFace(input_size=(1280, 1280))\n</code></pre> <p>Dynamic Size</p> <p>For RetinaFace, enable dynamic input for variable image sizes: <pre><code>detector = RetinaFace(dynamic_size=True)\n</code></pre></p>"},{"location":"concepts/thresholds-calibration/#recognition-thresholds","title":"Recognition Thresholds","text":""},{"location":"concepts/thresholds-calibration/#similarity-threshold","title":"Similarity Threshold","text":"<p>For identity verification (same person check):</p> <pre><code>import numpy as np\nfrom uniface.face_utils import compute_similarity\n\nsimilarity = compute_similarity(embedding1, embedding2)\n\n# Threshold interpretation\nif similarity &gt; 0.6:\n    print(\"Same person (high confidence)\")\nelif similarity &gt; 0.4:\n    print(\"Uncertain (manual review)\")\nelse:\n    print(\"Different people\")\n</code></pre> <p>Recommended thresholds:</p> Threshold Decision False Accept Rate 0.4 Low security Higher FAR 0.5 Balanced Moderate FAR 0.6 High security Lower FAR 0.7 Very strict Very low FAR"},{"location":"concepts/thresholds-calibration/#calibration-for-your-dataset","title":"Calibration for Your Dataset","text":"<p>Test on your data to find optimal thresholds:</p> <pre><code>import numpy as np\n\ndef calibrate_threshold(same_pairs, diff_pairs, recognizer, detector):\n    \"\"\"Find optimal threshold for your dataset.\"\"\"\n    same_scores = []\n    diff_scores = []\n\n    # Compute similarities for same-person pairs\n    for img1_path, img2_path in same_pairs:\n        img1 = cv2.imread(img1_path)\n        img2 = cv2.imread(img2_path)\n\n        faces1 = detector.detect(img1)\n        faces2 = detector.detect(img2)\n\n        if faces1 and faces2:\n            emb1 = recognizer.get_normalized_embedding(img1, faces1[0].landmarks)\n            emb2 = recognizer.get_normalized_embedding(img2, faces2[0].landmarks)\n            same_scores.append(np.dot(emb1, emb2.T)[0][0])\n\n    # Compute similarities for different-person pairs\n    for img1_path, img2_path in diff_pairs:\n        # ... similar process\n        diff_scores.append(similarity)\n\n    # Find optimal threshold\n    thresholds = np.arange(0.3, 0.8, 0.05)\n    best_threshold = 0.5\n    best_accuracy = 0\n\n    for thresh in thresholds:\n        tp = sum(1 for s in same_scores if s &gt;= thresh)\n        tn = sum(1 for s in diff_scores if s &lt; thresh)\n        accuracy = (tp + tn) / (len(same_scores) + len(diff_scores))\n\n        if accuracy &gt; best_accuracy:\n            best_accuracy = accuracy\n            best_threshold = thresh\n\n    return best_threshold, best_accuracy\n</code></pre>"},{"location":"concepts/thresholds-calibration/#anti-spoofing-thresholds","title":"Anti-Spoofing Thresholds","text":"<p>The MiniFASNet model returns a confidence score:</p> <pre><code>from uniface.spoofing import MiniFASNet\n\nspoofer = MiniFASNet()\nresult = spoofer.predict(image, face.bbox)\n\n# Default threshold (0.5)\nif result.is_real:  # confidence &gt; 0.5\n    print(\"Real face\")\n\n# Custom threshold for high security\nSPOOF_THRESHOLD = 0.7\nif result.confidence &gt; SPOOF_THRESHOLD:\n    print(\"Real face (high confidence)\")\nelse:\n    print(\"Potentially fake\")\n</code></pre>"},{"location":"concepts/thresholds-calibration/#attribute-model-confidence","title":"Attribute Model Confidence","text":""},{"location":"concepts/thresholds-calibration/#emotion","title":"Emotion","text":"<pre><code>result = emotion_predictor.predict(image, landmarks)\n\n# Filter low-confidence predictions\nif result.confidence &gt; 0.6:\n    print(f\"Emotion: {result.emotion}\")\nelse:\n    print(\"Uncertain emotion\")\n</code></pre>"},{"location":"concepts/thresholds-calibration/#visualization-threshold","title":"Visualization Threshold","text":"<p>For drawing detections, filter by confidence:</p> <pre><code>from uniface.draw import draw_detections\n\n# Only draw high-confidence detections\nbboxes = [f.bbox for f in faces if f.confidence &gt; 0.7]\nscores = [f.confidence for f in faces if f.confidence &gt; 0.7]\nlandmarks = [f.landmarks for f in faces if f.confidence &gt; 0.7]\n\ndraw_detections(\n    image=image,\n    bboxes=bboxes,\n    scores=scores,\n    landmarks=landmarks,\n    vis_threshold=0.6  # Additional visualization filter\n)\n</code></pre>"},{"location":"concepts/thresholds-calibration/#summary","title":"Summary","text":"Parameter Default Range Lower = Higher = <code>confidence_threshold</code> 0.5 0.1-0.9 More detections Fewer false positives <code>nms_threshold</code> 0.4 0.1-0.7 Fewer overlaps More overlapping boxes Similarity threshold 0.6 0.3-0.8 More matches (FAR\u2191) Fewer matches (FRR\u2191) Spoof confidence 0.5 0.3-0.9 More \"real\" Stricter liveness"},{"location":"concepts/thresholds-calibration/#next-steps","title":"Next Steps","text":"<ul> <li>Detection Module - Detection model options</li> <li>Recognition Module - Recognition model options</li> </ul>"},{"location":"modules/attributes/","title":"Attributes","text":"<p>Facial attribute analysis for age, gender, race, and emotion detection.</p>"},{"location":"modules/attributes/#available-models","title":"Available Models","text":"Model Attributes Size Notes AgeGender Age, Gender 8 MB Exact age prediction FairFace Gender, Age Group, Race 44 MB Balanced demographics Emotion 7-8 emotions 2 MB Requires PyTorch"},{"location":"modules/attributes/#agegender","title":"AgeGender","text":"<p>Predicts exact age and binary gender.</p>"},{"location":"modules/attributes/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface.attribute import AgeGender\nfrom uniface.detection import RetinaFace\n\ndetector = RetinaFace()\nage_gender = AgeGender()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = age_gender.predict(image, face.bbox)\n    print(f\"Gender: {result.sex}\")  # \"Female\" or \"Male\"\n    print(f\"Age: {result.age} years\")\n</code></pre>"},{"location":"modules/attributes/#output","title":"Output","text":"<pre><code># AttributeResult fields\nresult.gender     # 0=Female, 1=Male\nresult.sex        # \"Female\" or \"Male\" (property)\nresult.age        # int, age in years\nresult.age_group  # None (not provided by this model)\nresult.race       # None (not provided by this model)\n</code></pre>"},{"location":"modules/attributes/#fairface","title":"FairFace","text":"<p>Predicts gender, age group, and race with balanced demographics.</p>"},{"location":"modules/attributes/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface.attribute import FairFace\nfrom uniface.detection import RetinaFace\n\ndetector = RetinaFace()\nfairface = FairFace()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = fairface.predict(image, face.bbox)\n    print(f\"Gender: {result.sex}\")\n    print(f\"Age Group: {result.age_group}\")\n    print(f\"Race: {result.race}\")\n</code></pre>"},{"location":"modules/attributes/#output_1","title":"Output","text":"<pre><code># AttributeResult fields\nresult.gender     # 0=Female, 1=Male\nresult.sex        # \"Female\" or \"Male\"\nresult.age        # None (not provided by this model)\nresult.age_group  # \"20-29\", \"30-39\", etc.\nresult.race       # Race/ethnicity label\n</code></pre>"},{"location":"modules/attributes/#race-categories","title":"Race Categories","text":"Label White Black Latino Hispanic East Asian Southeast Asian Indian Middle Eastern"},{"location":"modules/attributes/#age-groups","title":"Age Groups","text":"Group 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 70+"},{"location":"modules/attributes/#emotion","title":"Emotion","text":"<p>Predicts facial emotions. Requires PyTorch.</p> <p>Optional Dependency</p> <p>Emotion detection requires PyTorch. Install with: <pre><code>pip install torch\n</code></pre></p>"},{"location":"modules/attributes/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface.detection import RetinaFace\nfrom uniface.attribute import Emotion\nfrom uniface.constants import DDAMFNWeights\n\ndetector = RetinaFace()\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET7)\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = emotion.predict(image, face.landmarks)\n    print(f\"Emotion: {result.emotion}\")\n    print(f\"Confidence: {result.confidence:.2%}\")\n</code></pre>"},{"location":"modules/attributes/#emotion-classes","title":"Emotion Classes","text":"7-Class (AFFECNET7)8-Class (AFFECNET8) Label Neutral Happy Sad Surprise Fear Disgust Angry Label Neutral Happy Sad Surprise Fear Disgust Angry Contempt"},{"location":"modules/attributes/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.attribute import Emotion\nfrom uniface.constants import DDAMFNWeights\n\n# 7-class emotion\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET7)\n\n# 8-class emotion\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET8)\n</code></pre>"},{"location":"modules/attributes/#combining-models","title":"Combining Models","text":""},{"location":"modules/attributes/#full-attribute-analysis","title":"Full Attribute Analysis","text":"<pre><code>from uniface.attribute import AgeGender, FairFace\nfrom uniface.detection import RetinaFace\n\ndetector = RetinaFace()\nage_gender = AgeGender()\nfairface = FairFace()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    # Get exact age from AgeGender\n    ag_result = age_gender.predict(image, face.bbox)\n\n    # Get race from FairFace\n    ff_result = fairface.predict(image, face.bbox)\n\n    print(f\"Gender: {ag_result.sex}\")\n    print(f\"Exact Age: {ag_result.age}\")\n    print(f\"Age Group: {ff_result.age_group}\")\n    print(f\"Race: {ff_result.race}\")\n</code></pre>"},{"location":"modules/attributes/#using-faceanalyzer","title":"Using FaceAnalyzer","text":"<pre><code>from uniface.analyzer import FaceAnalyzer\nfrom uniface.attribute import AgeGender\nfrom uniface.detection import RetinaFace\n\nanalyzer = FaceAnalyzer(\n    RetinaFace(),\n    age_gender=AgeGender(),\n)\n\nfaces = analyzer.analyze(image)\n\nfor face in faces:\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n</code></pre>"},{"location":"modules/attributes/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef draw_attributes(image, face, result):\n    \"\"\"Draw attributes on image.\"\"\"\n    x1, y1, x2, y2 = map(int, face.bbox)\n\n    # Draw bounding box\n    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    # Build label\n    label = f\"{result.sex}\"\n    if result.age:\n        label += f\", {result.age}y\"\n    if result.age_group:\n        label += f\", {result.age_group}\"\n    if result.race:\n        label += f\", {result.race}\"\n\n    # Draw label\n    cv2.putText(\n        image, label, (x1, y1 - 10),\n        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n    )\n\n    return image\n\n# Usage\nfor face in faces:\n    result = age_gender.predict(image, face.bbox)\n    image = draw_attributes(image, face, result)\n\ncv2.imwrite(\"attributes.jpg\", image)\n</code></pre>"},{"location":"modules/attributes/#accuracy-notes","title":"Accuracy Notes","text":"<p>Model Limitations</p> <ul> <li>AgeGender: Trained on CelebA; accuracy varies by demographic</li> <li>FairFace: Trained for balanced demographics; better cross-racial accuracy</li> <li>Emotion: Accuracy depends on facial expression clarity</li> </ul> <p>Always test on your specific use case and consider cultural context.</p>"},{"location":"modules/attributes/#next-steps","title":"Next Steps","text":"<ul> <li>Parsing - Face semantic segmentation</li> <li>Gaze - Gaze estimation</li> <li>Image Pipeline Recipe - Complete workflow</li> </ul>"},{"location":"modules/detection/","title":"Detection","text":"<p>Face detection is the first step in any face analysis pipeline. UniFace provides four detection models.</p>"},{"location":"modules/detection/#available-models","title":"Available Models","text":"Model Backbone Size Easy Medium Hard Landmarks RetinaFace MobileNet V2 3.5 MB 91.7% 91.0% 86.6% SCRFD SCRFD-10G 17 MB 95.2% 93.9% 83.1% YOLOv5-Face YOLOv5s 28 MB 94.3% 92.6% 83.2% YOLOv8-Face YOLOv8n 12 MB 94.6% 92.3% 79.6% <p>Dataset</p> <p>All models trained on WIDERFACE dataset.</p>"},{"location":"modules/detection/#retinaface","title":"RetinaFace","text":"<p>Single-shot face detector with multi-scale feature pyramid.</p>"},{"location":"modules/detection/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface.detection import RetinaFace\n\ndetector = RetinaFace()\nfaces = detector.detect(image)\n\nfor face in faces:\n    print(f\"Confidence: {face.confidence:.2f}\")\n    print(f\"BBox: {face.bbox}\")\n    print(f\"Landmarks: {face.landmarks.shape}\")  # (5, 2)\n</code></pre>"},{"location":"modules/detection/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.detection import RetinaFace\nfrom uniface.constants import RetinaFaceWeights\n\n# Lightweight (mobile/edge)\ndetector = RetinaFace(model_name=RetinaFaceWeights.MNET_025)\n\n# Balanced (default)\ndetector = RetinaFace(model_name=RetinaFaceWeights.MNET_V2)\n\n# High accuracy\ndetector = RetinaFace(model_name=RetinaFaceWeights.RESNET34)\n</code></pre> Variant Params Size Easy Medium Hard MNET_025 0.4M 1.7 MB 88.5% 87.0% 80.6% MNET_050 1.0M 2.6 MB 89.4% 88.0% 82.4% MNET_V1 3.5M 3.8 MB 90.6% 89.1% 84.1% MNET_V2 3.2M 3.5 MB 91.7% 91.0% 86.6% RESNET18 11.7M 27 MB 92.5% 91.0% 86.6% RESNET34 24.8M 56 MB 94.2% 93.1% 88.9%"},{"location":"modules/detection/#configuration","title":"Configuration","text":"<pre><code>detector = RetinaFace(\n    model_name=RetinaFaceWeights.MNET_V2,\n    confidence_threshold=0.5,  # Min confidence\n    nms_threshold=0.4,         # NMS IoU threshold\n    input_size=(640, 640),     # Input resolution\n    dynamic_size=False,        # Enable dynamic input size\n    providers=None,            # Auto-detect, or ['CPUExecutionProvider']\n)\n</code></pre>"},{"location":"modules/detection/#scrfd","title":"SCRFD","text":"<p>State-of-the-art detection with excellent accuracy-speed tradeoff.</p>"},{"location":"modules/detection/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface.detection import SCRFD\n\ndetector = SCRFD()\nfaces = detector.detect(image)\n</code></pre>"},{"location":"modules/detection/#model-variants_1","title":"Model Variants","text":"<pre><code>from uniface.detection import SCRFD\nfrom uniface.constants import SCRFDWeights\n\n# Real-time (lightweight)\ndetector = SCRFD(model_name=SCRFDWeights.SCRFD_500M_KPS)\n\n# High accuracy (default)\ndetector = SCRFD(model_name=SCRFDWeights.SCRFD_10G_KPS)\n</code></pre> Variant Params Size Easy Medium Hard SCRFD_500M_KPS 0.6M 2.5 MB 90.6% 88.1% 68.5% SCRFD_10G_KPS 4.2M 17 MB 95.2% 93.9% 83.1%"},{"location":"modules/detection/#configuration_1","title":"Configuration","text":"<pre><code>detector = SCRFD(\n    model_name=SCRFDWeights.SCRFD_10G_KPS,\n    confidence_threshold=0.5,\n    nms_threshold=0.4,\n    input_size=(640, 640),\n    providers=None,  # Auto-detect, or ['CPUExecutionProvider']\n)\n</code></pre>"},{"location":"modules/detection/#yolov5-face","title":"YOLOv5-Face","text":"<p>YOLO-based detection optimized for faces.</p>"},{"location":"modules/detection/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface.detection import YOLOv5Face\n\ndetector = YOLOv5Face()\nfaces = detector.detect(image)\n</code></pre>"},{"location":"modules/detection/#model-variants_2","title":"Model Variants","text":"<pre><code>from uniface.detection import YOLOv5Face\nfrom uniface.constants import YOLOv5FaceWeights\n\n# Lightweight\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5N)\n\n# Balanced (default)\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5S)\n\n# High accuracy\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5M)\n</code></pre> Variant Size Easy Medium Hard YOLOV5N 11 MB 93.6% 91.5% 80.5% YOLOV5S 28 MB 94.3% 92.6% 83.2% YOLOV5M 82 MB 95.3% 93.8% 85.3% <p>Fixed Input Size</p> <p>YOLOv5-Face uses a fixed input size of 640\u00d7640.</p>"},{"location":"modules/detection/#configuration_2","title":"Configuration","text":"<pre><code>detector = YOLOv5Face(\n    model_name=YOLOv5FaceWeights.YOLOV5S,\n    confidence_threshold=0.6,\n    nms_threshold=0.5,\n    nms_mode='numpy',  # or 'torchvision' for faster NMS\n    providers=None,    # Auto-detect, or ['CPUExecutionProvider']\n)\n</code></pre>"},{"location":"modules/detection/#yolov8-face","title":"YOLOv8-Face","text":"<p>Anchor-free detection with DFL (Distribution Focal Loss) for accurate bbox regression.</p>"},{"location":"modules/detection/#basic-usage_3","title":"Basic Usage","text":"<pre><code>from uniface.detection import YOLOv8Face\n\ndetector = YOLOv8Face()\nfaces = detector.detect(image)\n</code></pre>"},{"location":"modules/detection/#model-variants_3","title":"Model Variants","text":"<pre><code>from uniface.detection import YOLOv8Face\nfrom uniface.constants import YOLOv8FaceWeights\n\n# Lightweight\ndetector = YOLOv8Face(model_name=YOLOv8FaceWeights.YOLOV8_LITE_S)\n\n# Recommended (default)\ndetector = YOLOv8Face(model_name=YOLOv8FaceWeights.YOLOV8N)\n</code></pre> Variant Size Easy Medium Hard YOLOV8_LITE_S 7.4 MB 93.4% 91.2% 78.6% YOLOV8N 12 MB 94.6% 92.3% 79.6% <p>Fixed Input Size</p> <p>YOLOv8-Face uses a fixed input size of 640\u00d7640.</p>"},{"location":"modules/detection/#configuration_3","title":"Configuration","text":"<pre><code>detector = YOLOv8Face(\n    model_name=YOLOv8FaceWeights.YOLOV8N,\n    confidence_threshold=0.5,\n    nms_threshold=0.45,\n    nms_mode='numpy',  # or 'torchvision' for faster NMS\n    providers=None,    # Auto-detect, or ['CPUExecutionProvider']\n)\n</code></pre>"},{"location":"modules/detection/#factory-function","title":"Factory Function","text":"<p>Create detectors dynamically:</p> <pre><code>from uniface.detection import create_detector\n\ndetector = create_detector('retinaface')\n# or\ndetector = create_detector('scrfd')\n# or\ndetector = create_detector('yolov5face')\n# or\ndetector = create_detector('yolov8face')\n</code></pre>"},{"location":"modules/detection/#output-format","title":"Output Format","text":"<p>All detectors return <code>list[Face]</code>:</p> <pre><code>for face in faces:\n    # Bounding box [x1, y1, x2, y2]\n    bbox = face.bbox\n\n    # Detection confidence (0-1)\n    confidence = face.confidence\n\n    # 5-point landmarks (5, 2)\n    landmarks = face.landmarks\n    # [left_eye, right_eye, nose, left_mouth, right_mouth]\n</code></pre>"},{"location":"modules/detection/#visualization","title":"Visualization","text":"<pre><code>from uniface.draw import draw_detections\n\ndraw_detections(\n    image=image,\n    bboxes=[f.bbox for f in faces],\n    scores=[f.confidence for f in faces],\n    landmarks=[f.landmarks for f in faces],\n    vis_threshold=0.6\n)\n\ncv2.imwrite(\"result.jpg\", image)\n</code></pre>"},{"location":"modules/detection/#performance-comparison","title":"Performance Comparison","text":"<p>Benchmark on your hardware:</p> <pre><code>python tools/detect.py --source image.jpg\n</code></pre>"},{"location":"modules/detection/#see-also","title":"See Also","text":"<ul> <li>Recognition Module - Extract embeddings from detected faces</li> <li>Landmarks Module - Get 106-point landmarks</li> <li>Image Pipeline Recipe - Complete detection workflow</li> <li>Concepts: Thresholds - Tuning detection parameters</li> </ul>"},{"location":"modules/gaze/","title":"Gaze Estimation","text":"<p>Gaze estimation predicts where a person is looking (pitch and yaw angles).</p>"},{"location":"modules/gaze/#available-models","title":"Available Models","text":"Model Backbone Size MAE* ResNet18 ResNet18 43 MB 12.84\u00b0 ResNet34 ResNet34 82 MB 11.33\u00b0 ResNet50 ResNet50 91 MB 11.34\u00b0 MobileNetV2 MobileNetV2 9.6 MB 13.07\u00b0 MobileOne-S0 MobileOne 4.8 MB 12.58\u00b0 <p>*MAE = Mean Absolute Error on Gaze360 test set (lower is better)</p>"},{"location":"modules/gaze/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface.detection import RetinaFace\nfrom uniface.gaze import MobileGaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor face in faces:\n    # Crop face\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        # Estimate gaze\n        result = gaze_estimator.estimate(face_crop)\n\n        # Convert to degrees\n        pitch_deg = np.degrees(result.pitch)\n        yaw_deg = np.degrees(result.yaw)\n\n        print(f\"Pitch: {pitch_deg:.1f}\u00b0, Yaw: {yaw_deg:.1f}\u00b0\")\n</code></pre>"},{"location":"modules/gaze/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.gaze import MobileGaze\nfrom uniface.constants import GazeWeights\n\n# Default (ResNet34, recommended)\ngaze = MobileGaze()\n\n# Lightweight for mobile/edge\ngaze = MobileGaze(model_name=GazeWeights.MOBILEONE_S0)\n\n# Higher accuracy\ngaze = MobileGaze(model_name=GazeWeights.RESNET50)\n</code></pre>"},{"location":"modules/gaze/#output-format","title":"Output Format","text":"<pre><code>result = gaze_estimator.estimate(face_crop)\n\n# GazeResult dataclass\nresult.pitch  # Vertical angle in radians\nresult.yaw    # Horizontal angle in radians\n</code></pre>"},{"location":"modules/gaze/#angle-convention","title":"Angle Convention","text":"<pre><code>          pitch = +90\u00b0 (looking up)\n               \u2502\n               \u2502\nyaw = -90\u00b0 \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500 yaw = +90\u00b0\n(looking left) \u2502     (looking right)\n               \u2502\n          pitch = -90\u00b0 (looking down)\n</code></pre> <ul> <li>Pitch: Vertical gaze angle</li> <li>Positive = looking up</li> <li> <p>Negative = looking down</p> </li> <li> <p>Yaw: Horizontal gaze angle</p> </li> <li>Positive = looking right</li> <li>Negative = looking left</li> </ul>"},{"location":"modules/gaze/#visualization","title":"Visualization","text":"<pre><code>from uniface.draw import draw_gaze\n\n# Detect faces\nfaces = detector.detect(image)\n\nfor face in faces:\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        result = gaze_estimator.estimate(face_crop)\n\n        # Draw gaze arrow on image\n        draw_gaze(image, face.bbox, result.pitch, result.yaw)\n\ncv2.imwrite(\"gaze_output.jpg\", image)\n</code></pre>"},{"location":"modules/gaze/#custom-visualization","title":"Custom Visualization","text":"<pre><code>import cv2\nimport numpy as np\n\ndef draw_gaze_custom(image, bbox, pitch, yaw, length=100, color=(0, 255, 0)):\n    \"\"\"Draw custom gaze arrow.\"\"\"\n    x1, y1, x2, y2 = map(int, bbox)\n\n    # Face center\n    cx = (x1 + x2) // 2\n    cy = (y1 + y2) // 2\n\n    # Calculate endpoint\n    dx = -length * np.sin(yaw) * np.cos(pitch)\n    dy = -length * np.sin(pitch)\n\n    # Draw arrow\n    end_x = int(cx + dx)\n    end_y = int(cy + dy)\n\n    cv2.arrowedLine(image, (cx, cy), (end_x, end_y), color, 2, tipLength=0.3)\n\n    return image\n</code></pre>"},{"location":"modules/gaze/#real-time-gaze-tracking","title":"Real-Time Gaze Tracking","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface.detection import RetinaFace\nfrom uniface.gaze import MobileGaze\nfrom uniface.draw import draw_gaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n        face_crop = frame[y1:y2, x1:x2]\n\n        if face_crop.size &gt; 0:\n            result = gaze_estimator.estimate(face_crop)\n\n            # Draw bounding box\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Draw gaze\n            draw_gaze(frame, face.bbox, result.pitch, result.yaw)\n\n            # Display angles\n            pitch_deg = np.degrees(result.pitch)\n            yaw_deg = np.degrees(result.yaw)\n            label = f\"P:{pitch_deg:.0f} Y:{yaw_deg:.0f}\"\n            cv2.putText(frame, label, (x1, y1 - 10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    cv2.imshow(\"Gaze Estimation\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/gaze/#use-cases","title":"Use Cases","text":""},{"location":"modules/gaze/#attention-detection","title":"Attention Detection","text":"<pre><code>def is_looking_at_camera(result, threshold=15):\n    \"\"\"Check if person is looking at camera.\"\"\"\n    pitch_deg = abs(np.degrees(result.pitch))\n    yaw_deg = abs(np.degrees(result.yaw))\n\n    return pitch_deg &lt; threshold and yaw_deg &lt; threshold\n\n# Usage\nresult = gaze_estimator.estimate(face_crop)\nif is_looking_at_camera(result):\n    print(\"Looking at camera\")\nelse:\n    print(\"Looking away\")\n</code></pre>"},{"location":"modules/gaze/#gaze-direction-classification","title":"Gaze Direction Classification","text":"<pre><code>def classify_gaze_direction(result, threshold=20):\n    \"\"\"Classify gaze into directions.\"\"\"\n    pitch_deg = np.degrees(result.pitch)\n    yaw_deg = np.degrees(result.yaw)\n\n    directions = []\n\n    if pitch_deg &gt; threshold:\n        directions.append(\"up\")\n    elif pitch_deg &lt; -threshold:\n        directions.append(\"down\")\n\n    if yaw_deg &gt; threshold:\n        directions.append(\"right\")\n    elif yaw_deg &lt; -threshold:\n        directions.append(\"left\")\n\n    if not directions:\n        return \"center\"\n\n    return \" \".join(directions)\n\n# Usage\nresult = gaze_estimator.estimate(face_crop)\ndirection = classify_gaze_direction(result)\nprint(f\"Looking: {direction}\")\n</code></pre>"},{"location":"modules/gaze/#factory-function","title":"Factory Function","text":"<pre><code>from uniface.gaze import create_gaze_estimator\n\ngaze = create_gaze_estimator()  # Returns MobileGaze\n</code></pre>"},{"location":"modules/gaze/#next-steps","title":"Next Steps","text":"<ul> <li>Anti-Spoofing - Face liveness detection</li> <li>Privacy - Face anonymization</li> <li>Video Recipe - Real-time processing</li> </ul>"},{"location":"modules/landmarks/","title":"Landmarks","text":"<p>Facial landmark detection provides precise localization of facial features.</p>"},{"location":"modules/landmarks/#available-models","title":"Available Models","text":"Model Points Size Landmark106 106 14 MB <p>5-Point Landmarks</p> <p>Basic 5-point landmarks are included with all detection models (RetinaFace, SCRFD, YOLOv5-Face, YOLOv8-Face).</p>"},{"location":"modules/landmarks/#106-point-landmarks","title":"106-Point Landmarks","text":""},{"location":"modules/landmarks/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface.detection import RetinaFace\nfrom uniface.landmark import Landmark106\n\ndetector = RetinaFace()\nlandmarker = Landmark106()\n\n# Detect face\nfaces = detector.detect(image)\n\n# Get detailed landmarks\nif faces:\n    landmarks = landmarker.get_landmarks(image, faces[0].bbox)\n    print(f\"Landmarks shape: {landmarks.shape}\")  # (106, 2)\n</code></pre>"},{"location":"modules/landmarks/#landmark-groups","title":"Landmark Groups","text":"Range Group Points 0-32 Face Contour 33 33-50 Eyebrows 18 51-62 Nose 12 63-86 Eyes 24 87-105 Mouth 19"},{"location":"modules/landmarks/#extract-specific-features","title":"Extract Specific Features","text":"<pre><code>landmarks = landmarker.get_landmarks(image, face.bbox)\n\n# Face contour\ncontour = landmarks[0:33]\n\n# Left eyebrow\nleft_eyebrow = landmarks[33:42]\n\n# Right eyebrow\nright_eyebrow = landmarks[42:51]\n\n# Nose\nnose = landmarks[51:63]\n\n# Left eye\nleft_eye = landmarks[63:72]\n\n# Right eye\nright_eye = landmarks[76:84]\n\n# Mouth\nmouth = landmarks[87:106]\n</code></pre>"},{"location":"modules/landmarks/#5-point-landmarks-detection","title":"5-Point Landmarks (Detection)","text":"<p>All detection models provide 5-point landmarks:</p> <pre><code>from uniface.detection import RetinaFace\n\ndetector = RetinaFace()\nfaces = detector.detect(image)\n\nif faces:\n    landmarks_5 = faces[0].landmarks\n    print(f\"Shape: {landmarks_5.shape}\")  # (5, 2)\n\n    left_eye = landmarks_5[0]\n    right_eye = landmarks_5[1]\n    nose = landmarks_5[2]\n    left_mouth = landmarks_5[3]\n    right_mouth = landmarks_5[4]\n</code></pre>"},{"location":"modules/landmarks/#visualization","title":"Visualization","text":""},{"location":"modules/landmarks/#draw-106-landmarks","title":"Draw 106 Landmarks","text":"<pre><code>import cv2\n\ndef draw_landmarks(image, landmarks, color=(0, 255, 0), radius=2):\n    \"\"\"Draw landmarks on image.\"\"\"\n    for x, y in landmarks.astype(int):\n        cv2.circle(image, (x, y), radius, color, -1)\n    return image\n\n# Usage\nlandmarks = landmarker.get_landmarks(image, face.bbox)\nimage_with_landmarks = draw_landmarks(image.copy(), landmarks)\ncv2.imwrite(\"landmarks.jpg\", image_with_landmarks)\n</code></pre>"},{"location":"modules/landmarks/#draw-with-connections","title":"Draw with Connections","text":"<pre><code>def draw_landmarks_with_connections(image, landmarks):\n    \"\"\"Draw landmarks with facial feature connections.\"\"\"\n    landmarks = landmarks.astype(int)\n\n    # Face contour (0-32)\n    for i in range(32):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (255, 255, 0), 1)\n\n    # Left eyebrow (33-41)\n    for i in range(33, 41):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 255, 0), 1)\n\n    # Right eyebrow (42-50)\n    for i in range(42, 50):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 255, 0), 1)\n\n    # Nose (51-62)\n    for i in range(51, 62):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 0, 255), 1)\n\n    # Draw points\n    for x, y in landmarks:\n        cv2.circle(image, (x, y), 2, (0, 255, 255), -1)\n\n    return image\n</code></pre>"},{"location":"modules/landmarks/#use-cases","title":"Use Cases","text":""},{"location":"modules/landmarks/#face-alignment","title":"Face Alignment","text":"<pre><code>from uniface.face_utils import face_alignment\n\n# Align face using 5-point landmarks\naligned = face_alignment(image, faces[0].landmarks)\n# Returns: 112x112 aligned face\n</code></pre>"},{"location":"modules/landmarks/#eye-aspect-ratio-blink-detection","title":"Eye Aspect Ratio (Blink Detection)","text":"<pre><code>import numpy as np\n\ndef eye_aspect_ratio(eye_landmarks):\n    \"\"\"Calculate eye aspect ratio for blink detection.\"\"\"\n    # Vertical distances\n    v1 = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])\n    v2 = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])\n\n    # Horizontal distance\n    h = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])\n\n    ear = (v1 + v2) / (2.0 * h)\n    return ear\n\n# Usage with 106-point landmarks\nleft_eye = landmarks[63:72]  # Approximate eye points\near = eye_aspect_ratio(left_eye)\n\nif ear &lt; 0.2:\n    print(\"Eye closed (blink detected)\")\n</code></pre>"},{"location":"modules/landmarks/#head-pose-estimation","title":"Head Pose Estimation","text":"<pre><code>import cv2\nimport numpy as np\n\ndef estimate_head_pose(landmarks, image_shape):\n    \"\"\"Estimate head pose from facial landmarks.\"\"\"\n    # 3D model points (generic face model)\n    model_points = np.array([\n        (0.0, 0.0, 0.0),       # Nose tip\n        (0.0, -330.0, -65.0),  # Chin\n        (-225.0, 170.0, -135.0),  # Left eye corner\n        (225.0, 170.0, -135.0),   # Right eye corner\n        (-150.0, -150.0, -125.0), # Left mouth corner\n        (150.0, -150.0, -125.0)   # Right mouth corner\n    ], dtype=np.float64)\n\n    # 2D image points (from 106 landmarks)\n    image_points = np.array([\n        landmarks[51],   # Nose tip\n        landmarks[16],   # Chin\n        landmarks[63],   # Left eye corner\n        landmarks[76],   # Right eye corner\n        landmarks[87],   # Left mouth corner\n        landmarks[93]    # Right mouth corner\n    ], dtype=np.float64)\n\n    # Camera matrix\n    h, w = image_shape[:2]\n    focal_length = w\n    center = (w / 2, h / 2)\n    camera_matrix = np.array([\n        [focal_length, 0, center[0]],\n        [0, focal_length, center[1]],\n        [0, 0, 1]\n    ], dtype=np.float64)\n\n    # Solve PnP\n    dist_coeffs = np.zeros((4, 1))\n    success, rotation_vector, translation_vector = cv2.solvePnP(\n        model_points, image_points, camera_matrix, dist_coeffs\n    )\n\n    return rotation_vector, translation_vector\n</code></pre>"},{"location":"modules/landmarks/#factory-function","title":"Factory Function","text":"<pre><code>from uniface.landmark import create_landmarker\n\nlandmarker = create_landmarker()  # Returns Landmark106\n</code></pre>"},{"location":"modules/landmarks/#see-also","title":"See Also","text":"<ul> <li>Detection Module - Face detection with 5-point landmarks</li> <li>Attributes Module - Age, gender, emotion</li> <li>Gaze Module - Gaze estimation</li> <li>Concepts: Coordinate Systems - Landmark formats</li> </ul>"},{"location":"modules/parsing/","title":"Parsing","text":"<p>Face parsing segments faces into semantic components or face regions.</p>"},{"location":"modules/parsing/#available-models","title":"Available Models","text":"Model Backbone Size Output BiSeNet ResNet18 ResNet18 51 MB 19 classes BiSeNet ResNet34 ResNet34 89 MB 19 classes XSeg - 67 MB Mask"},{"location":"modules/parsing/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nfrom uniface.parsing import BiSeNet\nfrom uniface.draw import vis_parsing_maps\n\n# Initialize parser\nparser = BiSeNet()\n\n# Load face image (cropped)\nface_image = cv2.imread(\"face.jpg\")\n\n# Parse face\nmask = parser.parse(face_image)\nprint(f\"Mask shape: {mask.shape}\")  # (H, W)\n\n# Visualize\nface_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\nvis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\n# Save result\nvis_bgr = cv2.cvtColor(vis_result, cv2.COLOR_RGB2BGR)\ncv2.imwrite(\"parsed.jpg\", vis_bgr)\n</code></pre>"},{"location":"modules/parsing/#19-facial-component-classes","title":"19 Facial Component Classes","text":"ID Class ID Class 0 Background 10 Nose 1 Skin 11 Mouth 2 Left Eyebrow 12 Upper Lip 3 Right Eyebrow 13 Lower Lip 4 Left Eye 14 Neck 5 Right Eye 15 Necklace 6 Eyeglasses 16 Cloth 7 Left Ear 17 Hair 8 Right Ear 18 Hat 9 Earring"},{"location":"modules/parsing/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.parsing import BiSeNet\nfrom uniface.constants import ParsingWeights\n\n# Default (ResNet18)\nparser = BiSeNet()\n\n# Higher accuracy (ResNet34)\nparser = BiSeNet(model_name=ParsingWeights.RESNET34)\n</code></pre> Variant Params Size RESNET18 13.3M 51 MB RESNET34 24.1M 89 MB"},{"location":"modules/parsing/#full-pipeline","title":"Full Pipeline","text":""},{"location":"modules/parsing/#with-face-detection","title":"With Face Detection","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.parsing import BiSeNet\nfrom uniface.draw import vis_parsing_maps\n\ndetector = RetinaFace()\nparser = BiSeNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    # Crop face\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    # Parse\n    mask = parser.parse(face_crop)\n\n    # Visualize\n    face_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n    vis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\n    # Save\n    vis_bgr = cv2.cvtColor(vis_result, cv2.COLOR_RGB2BGR)\n    cv2.imwrite(f\"face_{i}_parsed.jpg\", vis_bgr)\n</code></pre>"},{"location":"modules/parsing/#extract-specific-components","title":"Extract Specific Components","text":""},{"location":"modules/parsing/#get-single-component-mask","title":"Get Single Component Mask","text":"<pre><code>import numpy as np\n\n# Parse face\nmask = parser.parse(face_image)\n\n# Extract specific component\nSKIN = 1\nHAIR = 17\nLEFT_EYE = 4\nRIGHT_EYE = 5\n\n# Binary mask for skin\nskin_mask = (mask == SKIN).astype(np.uint8) * 255\n\n# Binary mask for hair\nhair_mask = (mask == HAIR).astype(np.uint8) * 255\n\n# Binary mask for eyes\neyes_mask = ((mask == LEFT_EYE) | (mask == RIGHT_EYE)).astype(np.uint8) * 255\n</code></pre>"},{"location":"modules/parsing/#count-pixels-per-component","title":"Count Pixels per Component","text":"<pre><code>import numpy as np\n\nmask = parser.parse(face_image)\n\ncomponent_names = {\n    0: 'Background', 1: 'Skin', 2: 'L-Eyebrow', 3: 'R-Eyebrow',\n    4: 'L-Eye', 5: 'R-Eye', 6: 'Eyeglasses', 7: 'L-Ear', 8: 'R-Ear',\n    9: 'Earring', 10: 'Nose', 11: 'Mouth',\n    12: 'U-Lip', 13: 'L-Lip', 14: 'Neck', 15: 'Necklace',\n    16: 'Cloth', 17: 'Hair', 18: 'Hat'\n}\n\nfor class_id in np.unique(mask):\n    pixel_count = np.sum(mask == class_id)\n    name = component_names.get(class_id, f'Class {class_id}')\n    print(f\"{name}: {pixel_count} pixels\")\n</code></pre>"},{"location":"modules/parsing/#applications","title":"Applications","text":""},{"location":"modules/parsing/#face-makeup","title":"Face Makeup","text":"<p>Apply virtual makeup using component masks:</p> <pre><code>import cv2\nimport numpy as np\n\ndef apply_lip_color(image, mask, color=(180, 50, 50)):\n    \"\"\"Apply lip color using parsing mask.\"\"\"\n    result = image.copy()\n\n    # Get lip mask (upper lip=12, lower lip=13)\n    lip_mask = ((mask == 12) | (mask == 13)).astype(np.uint8)\n\n    # Create color overlay\n    overlay = np.zeros_like(image)\n    overlay[:] = color\n\n    # Alpha blend lip region\n    alpha = 0.4\n    mask_3ch = lip_mask[:, :, np.newaxis]\n    result = np.where(mask_3ch, (image * (1 - alpha) + overlay * alpha).astype(np.uint8), result)\n\n    return result\n</code></pre>"},{"location":"modules/parsing/#background-replacement","title":"Background Replacement","text":"<pre><code>def replace_background(image, mask, background):\n    \"\"\"Replace background using parsing mask.\"\"\"\n    # Create foreground mask (everything except background)\n    foreground_mask = (mask != 0).astype(np.uint8)\n\n    # Resize background to match image\n    background = cv2.resize(background, (image.shape[1], image.shape[0]))\n\n    # Combine\n    result = image.copy()\n    result[foreground_mask == 0] = background[foreground_mask == 0]\n\n    return result\n</code></pre>"},{"location":"modules/parsing/#hair-segmentation","title":"Hair Segmentation","text":"<pre><code>def get_hair_mask(mask):\n    \"\"\"Extract clean hair mask.\"\"\"\n    hair_mask = (mask == 17).astype(np.uint8) * 255\n\n    # Clean up with morphological operations\n    kernel = np.ones((5, 5), np.uint8)\n    hair_mask = cv2.morphologyEx(hair_mask, cv2.MORPH_CLOSE, kernel)\n    hair_mask = cv2.morphologyEx(hair_mask, cv2.MORPH_OPEN, kernel)\n\n    return hair_mask\n</code></pre>"},{"location":"modules/parsing/#visualization-options","title":"Visualization Options","text":"<pre><code>from uniface.draw import vis_parsing_maps\n\n# Default visualization\nvis_result = vis_parsing_maps(face_rgb, mask)\n\n# With different parameters\nvis_result = vis_parsing_maps(\n    face_rgb,\n    mask,\n    save_image=False,  # Don't save to file\n)\n</code></pre>"},{"location":"modules/parsing/#xseg","title":"XSeg","text":"<p>XSeg outputs a mask for face regions. Unlike BiSeNet which works on bbox crops, XSeg requires 5-point landmarks for face alignment.</p>"},{"location":"modules/parsing/#basic-usage_1","title":"Basic Usage","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.parsing import XSeg\n\ndetector = RetinaFace()\nparser = XSeg()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor face in faces:\n    if face.landmarks is not None:\n        mask = parser.parse(image, landmarks=face.landmarks)\n        print(f\"Mask shape: {mask.shape}\")  # (H, W), values in [0, 1]\n</code></pre>"},{"location":"modules/parsing/#parameters","title":"Parameters","text":"<pre><code>from uniface.parsing import XSeg\n\n# Default settings\nparser = XSeg()\n\n# Custom settings\nparser = XSeg(\n    align_size=256,   # Face alignment size\n    blur_sigma=5,     # Gaussian blur for smoothing (0 = raw)\n)\n</code></pre> Parameter Default Description <code>align_size</code> 256 Face alignment output size <code>blur_sigma</code> 0 Mask smoothing (0 = no blur)"},{"location":"modules/parsing/#methods","title":"Methods","text":"<pre><code># Full pipeline: align -&gt; segment -&gt; warp back to original space\nmask = parser.parse(image, landmarks=landmarks)\n\n# For pre-aligned face crops\nmask = parser.parse_aligned(face_crop)\n\n# Get mask + crop + inverse matrix for custom warping\nmask, face_crop, inverse_matrix = parser.parse_with_inverse(image, landmarks)\n</code></pre>"},{"location":"modules/parsing/#bisenet-vs-xseg","title":"BiSeNet vs XSeg","text":"Feature BiSeNet XSeg Output 19 class labels Mask [0, 1] Input Bbox crop Requires landmarks Use case Facial components Face region extraction"},{"location":"modules/parsing/#factory-function","title":"Factory Function","text":"<pre><code>from uniface.parsing import create_face_parser\nfrom uniface.constants import ParsingWeights, XSegWeights\n\n# BiSeNet (default)\nparser = create_face_parser()\n\n# XSeg\nparser = create_face_parser(XSegWeights.DEFAULT)\n</code></pre>"},{"location":"modules/parsing/#next-steps","title":"Next Steps","text":"<ul> <li>Gaze - Gaze estimation</li> <li>Privacy - Face anonymization</li> <li>Detection - Face detection</li> </ul>"},{"location":"modules/privacy/","title":"Privacy","text":"<p>Face anonymization protects privacy by blurring or obscuring faces in images and videos.</p>"},{"location":"modules/privacy/#available-methods","title":"Available Methods","text":"Method Description pixelate Blocky pixelation gaussian Smooth blur blackout Solid color fill elliptical Oval-shaped blur median Edge-preserving blur"},{"location":"modules/privacy/#quick-start","title":"Quick Start","text":"<pre><code>from uniface.detection import RetinaFace\nfrom uniface.privacy import BlurFace\nimport cv2\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian', blur_strength=5.0)\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\nanonymized = blurrer.anonymize(image, faces)\n\ncv2.imwrite(\"anonymized.jpg\", anonymized)\n</code></pre>"},{"location":"modules/privacy/#blur-methods","title":"Blur Methods","text":""},{"location":"modules/privacy/#pixelate","title":"Pixelate","text":"<p>Blocky pixelation effect (common in news media):</p> <pre><code>blurrer = BlurFace(method='pixelate', pixel_blocks=15)\n</code></pre> Parameter Default Description <code>pixel_blocks</code> 15 Number of blocks (lower = more pixelated)"},{"location":"modules/privacy/#gaussian","title":"Gaussian","text":"<p>Smooth, natural-looking blur:</p> <pre><code>blurrer = BlurFace(method='gaussian', blur_strength=3.0)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity (higher = more blur)"},{"location":"modules/privacy/#blackout","title":"Blackout","text":"<p>Solid color fill for maximum privacy:</p> <pre><code>blurrer = BlurFace(method='blackout', color=(0, 0, 0))\n</code></pre> Parameter Default Description <code>color</code> (0, 0, 0) Fill color (BGR format)"},{"location":"modules/privacy/#elliptical","title":"Elliptical","text":"<p>Oval-shaped blur matching natural face shape:</p> <pre><code>blurrer = BlurFace(method='elliptical', blur_strength=3.0, margin=20)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity <code>margin</code> 20 Margin around face"},{"location":"modules/privacy/#median","title":"Median","text":"<p>Edge-preserving blur with artistic effect:</p> <pre><code>blurrer = BlurFace(method='median', blur_strength=3.0)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity"},{"location":"modules/privacy/#in-place-processing","title":"In-Place Processing","text":"<p>Modify image directly (faster, saves memory):</p> <pre><code>blurrer = BlurFace(method='pixelate')\n\n# In-place modification\nresult = blurrer.anonymize(image, faces, inplace=True)\n# 'image' and 'result' point to the same array\n</code></pre>"},{"location":"modules/privacy/#real-time-anonymization","title":"Real-Time Anonymization","text":""},{"location":"modules/privacy/#webcam","title":"Webcam","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='pixelate')\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n\n    cv2.imshow('Anonymized', frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/privacy/#video-file","title":"Video File","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian')\n\ncap = cv2.VideoCapture(\"input_video.mp4\")\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n    out.write(frame)\n\ncap.release()\nout.release()\n</code></pre>"},{"location":"modules/privacy/#selective-anonymization","title":"Selective Anonymization","text":""},{"location":"modules/privacy/#exclude-specific-faces","title":"Exclude Specific Faces","text":"<pre><code>def anonymize_except(image, all_faces, exclude_embeddings, recognizer, threshold=0.6):\n    \"\"\"Anonymize all faces except those matching exclude_embeddings.\"\"\"\n    faces_to_blur = []\n\n    for face in all_faces:\n        # Get embedding\n        embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n        # Check if should be excluded\n        should_exclude = False\n        for ref_emb in exclude_embeddings:\n            similarity = np.dot(embedding, ref_emb.T)[0][0]\n            if similarity &gt; threshold:\n                should_exclude = True\n                break\n\n        if not should_exclude:\n            faces_to_blur.append(face)\n\n    # Blur remaining faces\n    return blurrer.anonymize(image, faces_to_blur)\n</code></pre>"},{"location":"modules/privacy/#confidence-based","title":"Confidence-Based","text":"<pre><code>def anonymize_low_confidence(image, faces, blurrer, confidence_threshold=0.8):\n    \"\"\"Anonymize faces below confidence threshold.\"\"\"\n    faces_to_blur = [f for f in faces if f.confidence &lt; confidence_threshold]\n    return blurrer.anonymize(image, faces_to_blur)\n</code></pre>"},{"location":"modules/privacy/#comparison","title":"Comparison","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nmethods = ['pixelate', 'gaussian', 'blackout', 'elliptical', 'median']\n\nfor method in methods:\n    blurrer = BlurFace(method=method)\n    result = blurrer.anonymize(image.copy(), faces)\n    cv2.imwrite(f\"anonymized_{method}.jpg\", result)\n</code></pre>"},{"location":"modules/privacy/#command-line-tool","title":"Command-Line Tool","text":"<pre><code># Anonymize image with pixelation\npython tools/anonymize.py --source photo.jpg\n\n# Real-time webcam\npython tools/anonymize.py --source 0 --method gaussian\n\n# Custom blur strength\npython tools/anonymize.py --source photo.jpg --method gaussian --blur-strength 5.0\n</code></pre>"},{"location":"modules/privacy/#next-steps","title":"Next Steps","text":"<ul> <li>Anonymize Stream Recipe - Video pipeline</li> <li>Detection - Face detection options</li> <li>Batch Processing Recipe - Process multiple files</li> </ul>"},{"location":"modules/recognition/","title":"Recognition","text":"<p>Face recognition extracts embeddings for identity verification and face search.</p>"},{"location":"modules/recognition/#available-models","title":"Available Models","text":"Model Backbone Size Embedding Dim AdaFace IR-18/IR-101 92-249 MB 512 ArcFace MobileNet/ResNet 8-166 MB 512 MobileFace MobileNet V2/V3 1-10 MB 512 SphereFace Sphere20/36 50-92 MB 512"},{"location":"modules/recognition/#adaface","title":"AdaFace","text":"<p>Face recognition using adaptive margin based on image quality.</p>"},{"location":"modules/recognition/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface.detection import RetinaFace\nfrom uniface.recognition import AdaFace\n\ndetector = RetinaFace()\nrecognizer = AdaFace()\n\n# Detect face\nfaces = detector.detect(image)\n\n# Extract embedding\nif faces:\n    embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n    print(f\"Embedding shape: {embedding.shape}\")  # (1, 512)\n</code></pre>"},{"location":"modules/recognition/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.recognition import AdaFace\nfrom uniface.constants import AdaFaceWeights\n\n# Lightweight (default)\nrecognizer = AdaFace(model_name=AdaFaceWeights.IR_18)\n\n# High accuracy\nrecognizer = AdaFace(model_name=AdaFaceWeights.IR_101)\n\n# Force CPU execution\nrecognizer = AdaFace(providers=['CPUExecutionProvider'])\n</code></pre> Variant Dataset Size IJB-B IJB-C IR_18 WebFace4M 92 MB 93.03% 94.99% IR_101 WebFace12M 249 MB - 97.66% <p>Benchmark Metrics</p> <p>IJB-B and IJB-C accuracy reported as TAR@FAR=0.01%</p>"},{"location":"modules/recognition/#arcface","title":"ArcFace","text":"<p>Face recognition using additive angular margin loss.</p>"},{"location":"modules/recognition/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Detect face\nfaces = detector.detect(image)\n\n# Extract embedding\nif faces:\n    embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n    print(f\"Embedding shape: {embedding.shape}\")  # (1, 512)\n</code></pre>"},{"location":"modules/recognition/#model-variants_1","title":"Model Variants","text":"<pre><code>from uniface.recognition import ArcFace\nfrom uniface.constants import ArcFaceWeights\n\n# Lightweight (default)\nrecognizer = ArcFace(model_name=ArcFaceWeights.MNET)\n\n# High accuracy\nrecognizer = ArcFace(model_name=ArcFaceWeights.RESNET)\n\n# Force CPU execution\nrecognizer = ArcFace(providers=['CPUExecutionProvider'])\n</code></pre> Variant Backbone Size LFW CFP-FP AgeDB-30 IJB-C MNET MobileNet 8 MB 99.70% 98.00% 96.58% 95.02% RESNET ResNet50 166 MB 99.83% 99.33% 98.23% 97.25% <p>Training Data &amp; Metrics</p> <p>Dataset: Trained on WebFace600K (600K images)</p> <p>Accuracy: IJB-C reported as TAR@FAR=1e-4</p>"},{"location":"modules/recognition/#mobileface","title":"MobileFace","text":"<p>Lightweight face recognition models with MobileNet backbones.</p>"},{"location":"modules/recognition/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface.recognition import MobileFace\n\nrecognizer = MobileFace()\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n</code></pre>"},{"location":"modules/recognition/#model-variants_2","title":"Model Variants","text":"<pre><code>from uniface.recognition import MobileFace\nfrom uniface.constants import MobileFaceWeights\n\n# Ultra-lightweight\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_025)\n\n# Balanced (default)\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_V2)\n\n# Higher accuracy\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_V3_LARGE)\n</code></pre> Variant Params Size LFW CALFW CPLFW AgeDB-30 MNET_025 0.36M 1 MB 98.76% 92.02% 82.37% 90.02% MNET_V2 2.29M 4 MB 99.55% 94.87% 86.89% 95.16% MNET_V3_SMALL 1.25M 3 MB 99.30% 93.77% 85.29% 92.79% MNET_V3_LARGE 3.52M 10 MB 99.53% 94.56% 86.79% 95.13%"},{"location":"modules/recognition/#sphereface","title":"SphereFace","text":"<p>Face recognition using angular softmax loss (A-Softmax).</p>"},{"location":"modules/recognition/#basic-usage_3","title":"Basic Usage","text":"<pre><code>from uniface.recognition import SphereFace\nfrom uniface.constants import SphereFaceWeights\n\nrecognizer = SphereFace(model_name=SphereFaceWeights.SPHERE20)\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n</code></pre> Variant Params Size LFW CALFW CPLFW AgeDB-30 SPHERE20 24.5M 50 MB 99.67% 95.61% 88.75% 96.58% SPHERE36 34.6M 92 MB 99.72% 95.64% 89.92% 96.83%"},{"location":"modules/recognition/#face-comparison","title":"Face Comparison","text":""},{"location":"modules/recognition/#compute-similarity","title":"Compute Similarity","text":"<pre><code>from uniface.face_utils import compute_similarity\nimport numpy as np\n\n# Extract embeddings\nemb1 = recognizer.get_normalized_embedding(image1, landmarks1)\nemb2 = recognizer.get_normalized_embedding(image2, landmarks2)\n\n# Method 1: Using utility function\nsimilarity = compute_similarity(emb1, emb2)\n\n# Method 2: Direct computation\nsimilarity = np.dot(emb1, emb2.T)[0][0]\n\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"modules/recognition/#threshold-guidelines","title":"Threshold Guidelines","text":"Threshold Decision Use Case &gt; 0.7 Very high confidence Security-critical &gt; 0.6 Same person General verification 0.4 - 0.6 Uncertain Manual review needed &lt; 0.4 Different people Rejection"},{"location":"modules/recognition/#face-alignment","title":"Face Alignment","text":"<p>Recognition models require aligned faces. UniFace handles this internally:</p> <pre><code># Alignment is done automatically\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n\n# Or manually align\nfrom uniface.face_utils import face_alignment\n\naligned_face = face_alignment(image, landmarks)\n# Returns: 112x112 aligned face image\n</code></pre>"},{"location":"modules/recognition/#building-a-face-database","title":"Building a Face Database","text":"<pre><code>import numpy as np\nfrom uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Build database\ndatabase = {}\nfor person_id, image_path in person_images.items():\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n\n    if faces:\n        embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n        database[person_id] = embedding\n\n# Save for later use\nnp.savez('face_database.npz', **database)\n\n# Load database\ndata = np.load('face_database.npz')\ndatabase = {key: data[key] for key in data.files}\n</code></pre>"},{"location":"modules/recognition/#face-search","title":"Face Search","text":"<p>Find a person in a database:</p> <pre><code>def search_face(query_embedding, database, threshold=0.6):\n    \"\"\"Find best match in database.\"\"\"\n    best_match = None\n    best_similarity = -1\n\n    for person_id, db_embedding in database.items():\n        similarity = np.dot(query_embedding, db_embedding.T)[0][0]\n\n        if similarity &gt; best_similarity and similarity &gt; threshold:\n            best_similarity = similarity\n            best_match = person_id\n\n    return best_match, best_similarity\n\n# Usage\nquery_embedding = recognizer.get_normalized_embedding(query_image, landmarks)\nmatch, similarity = search_face(query_embedding, database)\n\nif match:\n    print(f\"Found: {match} (similarity: {similarity:.4f})\")\nelse:\n    print(\"No match found\")\n</code></pre>"},{"location":"modules/recognition/#factory-function","title":"Factory Function","text":"<pre><code>from uniface.recognition import create_recognizer\n\n# Available methods: 'arcface', 'adaface', 'mobileface', 'sphereface'\nrecognizer = create_recognizer('arcface')\nrecognizer = create_recognizer('adaface')\n</code></pre>"},{"location":"modules/recognition/#see-also","title":"See Also","text":"<ul> <li>Detection Module - Detect faces first</li> <li>Face Search Recipe - Complete search system</li> <li>Thresholds - Calibration guide</li> </ul>"},{"location":"modules/spoofing/","title":"Anti-Spoofing","text":"<p>Face anti-spoofing detects whether a face is real (live) or fake (photo, video replay, mask).</p>"},{"location":"modules/spoofing/#available-models","title":"Available Models","text":"Model Size MiniFASNet V1SE 1.2 MB MiniFASNet V2 1.2 MB"},{"location":"modules/spoofing/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = spoofer.predict(image, face.bbox)\n\n    label = \"Real\" if result.is_real else \"Fake\"\n    print(f\"{label}: {result.confidence:.1%}\")\n</code></pre>"},{"location":"modules/spoofing/#output-format","title":"Output Format","text":"<pre><code>result = spoofer.predict(image, face.bbox)\n\n# SpoofingResult dataclass\nresult.is_real     # True = real, False = fake\nresult.confidence  # 0.0 to 1.0\n</code></pre>"},{"location":"modules/spoofing/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.spoofing import MiniFASNet\nfrom uniface.constants import MiniFASNetWeights\n\n# Default (V2, recommended)\nspoofer = MiniFASNet()\n\n# V1SE variant\nspoofer = MiniFASNet(model_name=MiniFASNetWeights.V1SE)\n</code></pre> Variant Size Scale Factor V1SE 1.2 MB 4.0 V2 1.2 MB 2.7"},{"location":"modules/spoofing/#confidence-thresholds","title":"Confidence Thresholds","text":"<p><code>result.is_real</code> is based on the model's top predicted class (argmax). If you want stricter behavior, apply your own confidence threshold:</p> <pre><code>result = spoofer.predict(image, face.bbox)\n\n# High security (fewer false accepts)\nHIGH_THRESHOLD = 0.7\nif result.is_real and result.confidence &gt; HIGH_THRESHOLD:\n    print(\"Real (high confidence)\")\nelse:\n    print(\"Suspicious\")\n\n# Balanced (argmax decision)\nif result.is_real:\n    print(\"Real\")\nelse:\n    print(\"Fake\")\n</code></pre>"},{"location":"modules/spoofing/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef draw_spoofing_result(image, face, result):\n    \"\"\"Draw spoofing result on image.\"\"\"\n    x1, y1, x2, y2 = map(int, face.bbox)\n\n    # Color based on result\n    color = (0, 255, 0) if result.is_real else (0, 0, 255)\n    label = \"Real\" if result.is_real else \"Fake\"\n\n    # Draw bounding box\n    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n\n    # Draw label\n    text = f\"{label}: {result.confidence:.1%}\"\n    cv2.putText(image, text, (x1, y1 - 10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    return image\n\n# Usage\nfor face in faces:\n    result = spoofer.predict(image, face.bbox)\n    image = draw_spoofing_result(image, face, result)\n\ncv2.imwrite(\"spoofing_result.jpg\", image)\n</code></pre>"},{"location":"modules/spoofing/#real-time-liveness-detection","title":"Real-Time Liveness Detection","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    for face in faces:\n        result = spoofer.predict(frame, face.bbox)\n\n        # Draw result\n        x1, y1, x2, y2 = map(int, face.bbox)\n        color = (0, 255, 0) if result.is_real else (0, 0, 255)\n        label = f\"{'Real' if result.is_real else 'Fake'}: {result.confidence:.0%}\"\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(frame, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    cv2.imshow(\"Liveness Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/spoofing/#use-cases","title":"Use Cases","text":""},{"location":"modules/spoofing/#access-control","title":"Access Control","text":"<pre><code>def verify_liveness(image, face, spoofer, threshold=0.6):\n    \"\"\"Verify face is real for access control.\"\"\"\n    result = spoofer.predict(image, face.bbox)\n\n    if result.is_real and result.confidence &gt; threshold:\n        return True, result.confidence\n    return False, result.confidence\n\n# Usage\nis_live, confidence = verify_liveness(image, face, spoofer)\nif is_live:\n    print(f\"Access granted (confidence: {confidence:.1%})\")\nelse:\n    print(f\"Access denied - possible spoof attempt\")\n</code></pre>"},{"location":"modules/spoofing/#multi-frame-verification","title":"Multi-Frame Verification","text":"<p>For higher security, verify across multiple frames:</p> <pre><code>def verify_liveness_multiframe(frames, detector, spoofer, min_real=3):\n    \"\"\"Verify liveness across multiple frames.\"\"\"\n    real_count = 0\n\n    for frame in frames:\n        faces = detector.detect(frame)\n        if not faces:\n            continue\n\n        result = spoofer.predict(frame, faces[0].bbox)\n        if result.is_real:\n            real_count += 1\n\n    return real_count &gt;= min_real\n\n# Collect frames and verify\nframes = []\nfor _ in range(5):\n    ret, frame = cap.read()\n    if ret:\n        frames.append(frame)\n\nis_verified = verify_liveness_multiframe(frames, detector, spoofer)\n</code></pre>"},{"location":"modules/spoofing/#attack-types-detected","title":"Attack Types Detected","text":"<p>MiniFASNet can detect various spoof attacks:</p> Attack Type Detection Printed photos \u2705 Screen replay \u2705 Video replay \u2705 Paper masks \u2705 3D masks Limited <p>Limitations</p> <ul> <li>High-quality 3D masks may not be detected</li> <li>Performance varies with lighting and image quality</li> <li>Always combine with other verification methods for high-security applications</li> </ul>"},{"location":"modules/spoofing/#command-line-tool","title":"Command-Line Tool","text":"<pre><code># Image\npython tools/spoofing.py --source photo.jpg\n\n# Webcam\npython tools/spoofing.py --source 0\n</code></pre>"},{"location":"modules/spoofing/#factory-function","title":"Factory Function","text":"<pre><code>from uniface.spoofing import create_spoofer\n\nspoofer = create_spoofer()  # Returns MiniFASNet\n</code></pre>"},{"location":"modules/spoofing/#next-steps","title":"Next Steps","text":"<ul> <li>Privacy - Face anonymization</li> <li>Detection - Face detection</li> <li>Recognition - Face recognition</li> </ul>"},{"location":"modules/tracking/","title":"Tracking","text":"<p>Multi-object tracking using BYTETracker with Kalman filtering and IoU-based association. The tracker assigns persistent IDs to detected objects across video frames using a two-stage association strategy \u2014 first matching high-confidence detections, then low-confidence ones.</p>"},{"location":"modules/tracking/#how-it-works","title":"How It Works","text":"<p>BYTETracker takes detection bounding boxes as input and returns tracked bounding boxes with persistent IDs. It does not depend on any specific detector \u2014 any source of <code>[x1, y1, x2, y2, score]</code> arrays will work.</p> <p>Each frame, the tracker:</p> <ol> <li>Splits detections into high-confidence and low-confidence groups</li> <li>Matches high-confidence detections to existing tracks using IoU</li> <li>Matches remaining tracks to low-confidence detections (second chance)</li> <li>Starts new tracks for unmatched high-confidence detections</li> <li>Removes tracks that have been lost for too long</li> </ol> <p>The Kalman filter predicts where each track will be in the next frame, which helps maintain associations even when detections are noisy.</p>"},{"location":"modules/tracking/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface.common import xyxy_to_cxcywh\nfrom uniface.detection import SCRFD\nfrom uniface.tracking import BYTETracker\nfrom uniface.draw import draw_tracks\n\ndetector = SCRFD()\ntracker = BYTETracker(track_thresh=0.5, track_buffer=30)\n\ncap = cv2.VideoCapture(\"video.mp4\")\n\nwhile cap.isOpened():\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # 1. Detect faces\n    faces = detector.detect(frame)\n\n    # 2. Build detections array: [x1, y1, x2, y2, score]\n    dets = np.array([[*f.bbox, f.confidence] for f in faces])\n    dets = dets if len(dets) &gt; 0 else np.empty((0, 5))\n\n    # 3. Update tracker\n    tracks = tracker.update(dets)\n\n    # 4. Map track IDs back to face objects\n    if len(tracks) &gt; 0 and len(faces) &gt; 0:\n        face_bboxes = np.array([f.bbox for f in faces], dtype=np.float32)\n        track_ids = tracks[:, 4].astype(int)\n\n        face_centers = xyxy_to_cxcywh(face_bboxes)[:, :2]\n        track_centers = xyxy_to_cxcywh(tracks[:, :4])[:, :2]\n\n        for ti in range(len(tracks)):\n            dists = (track_centers[ti, 0] - face_centers[:, 0]) ** 2 + (track_centers[ti, 1] - face_centers[:, 1]) ** 2\n            faces[int(np.argmin(dists))].track_id = track_ids[ti]\n\n    # 5. Draw\n    tracked_faces = [f for f in faces if f.track_id is not None]\n    draw_tracks(image=frame, faces=tracked_faces)\n    cv2.imshow(\"Tracking\", frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>Each track ID gets a deterministic color via golden-ratio hue stepping, so the same person keeps the same color across the entire video.</p>"},{"location":"modules/tracking/#webcam-tracking","title":"Webcam Tracking","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface.common import xyxy_to_cxcywh\nfrom uniface.detection import SCRFD\nfrom uniface.tracking import BYTETracker\nfrom uniface.draw import draw_tracks\n\ndetector = SCRFD()\ntracker = BYTETracker(track_thresh=0.5, track_buffer=30)\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    dets = np.array([[*f.bbox, f.confidence] for f in faces])\n    dets = dets if len(dets) &gt; 0 else np.empty((0, 5))\n\n    tracks = tracker.update(dets)\n\n    if len(tracks) &gt; 0 and len(faces) &gt; 0:\n        face_bboxes = np.array([f.bbox for f in faces], dtype=np.float32)\n        track_ids = tracks[:, 4].astype(int)\n\n        face_centers = xyxy_to_cxcywh(face_bboxes)[:, :2]\n        track_centers = xyxy_to_cxcywh(tracks[:, :4])[:, :2]\n\n        for ti in range(len(tracks)):\n            dists = (track_centers[ti, 0] - face_centers[:, 0]) ** 2 + (track_centers[ti, 1] - face_centers[:, 1]) ** 2\n            faces[int(np.argmin(dists))].track_id = track_ids[ti]\n\n    draw_tracks(image=frame, faces=[f for f in faces if f.track_id is not None])\n    cv2.imshow(\"Face Tracking - Press 'q' to quit\", frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/tracking/#parameters","title":"Parameters","text":"<pre><code>from uniface.tracking import BYTETracker\n\ntracker = BYTETracker(\n    track_thresh=0.5,\n    track_buffer=30,\n    match_thresh=0.8,\n    low_thresh=0.1,\n)\n</code></pre> Parameter Default Description <code>track_thresh</code> 0.5 Detections above this score go through first-pass association <code>track_buffer</code> 30 How many frames to keep a lost track before removing it <code>match_thresh</code> 0.8 IoU threshold for matching tracks to detections <code>low_thresh</code> 0.1 Detections below this score are discarded entirely"},{"location":"modules/tracking/#input-output","title":"Input / Output","text":"<p>Input \u2014 <code>(N, 5)</code> numpy array with <code>[x1, y1, x2, y2, confidence]</code> per detection:</p> <pre><code>detections = np.array([\n    [100, 50, 200, 160, 0.95],\n    [300, 80, 380, 200, 0.87],\n])\n</code></pre> <p>Output \u2014 <code>(M, 5)</code> numpy array with <code>[x1, y1, x2, y2, track_id]</code> per active track:</p> <pre><code>tracks = tracker.update(detections)\n# array([[101.2, 51.3, 199.8, 159.8, 1.],\n#        [300.5, 80.2, 379.7, 200.1, 2.]])\n</code></pre> <p>The output bounding boxes come from the Kalman filter prediction, so they may differ slightly from the input. Track IDs are integers that persist across frames for the same object.</p>"},{"location":"modules/tracking/#resetting-the-tracker","title":"Resetting the Tracker","text":"<p>When switching to a different video or scene, reset the tracker to clear all internal state:</p> <pre><code>tracker.reset()\n</code></pre> <p>This clears all active, lost, and removed tracks, resets the frame counter, and resets the ID counter back to zero.</p>"},{"location":"modules/tracking/#visualization","title":"Visualization","text":"<p><code>draw_tracks</code> draws bounding boxes color-coded by track ID:</p> <pre><code>from uniface.draw import draw_tracks\n\ndraw_tracks(\n    image=frame,\n    faces=tracked_faces,\n    draw_landmarks=True,\n    draw_id=True,\n    corner_bbox=True,\n)\n</code></pre>"},{"location":"modules/tracking/#small-face-performance","title":"Small Face Performance","text":"<p>Tracking performance with small faces</p> <p>The tracker relies on IoU (Intersection over Union) to match detections across frames. When faces occupy a small portion of the image \u2014 for example in surveillance footage or wide-angle cameras \u2014 even slight movement between frames can cause a large drop in IoU. This makes it harder for the tracker to maintain consistent IDs, and you may see IDs switching or resetting more often than expected.</p> <p>This is not specific to BYTETracker; it applies to any IoU-based tracker. A few things that can help:</p> <ul> <li>Lower <code>match_thresh</code> (e.g. <code>0.5</code> or <code>0.6</code>) so the tracker accepts lower   overlap as a valid match.</li> <li>Increase <code>track_buffer</code> (e.g. <code>60</code> or higher) to hold onto lost tracks   longer before discarding them.</li> <li>Use a higher-resolution input if possible, so face bounding boxes are   larger in pixel terms.</li> </ul> <pre><code>tracker = BYTETracker(\n    track_thresh=0.4,\n    track_buffer=60,\n    match_thresh=0.6,\n)\n</code></pre>"},{"location":"modules/tracking/#cli-tool","title":"CLI Tool","text":"<pre><code># Track faces in a video\npython tools/track.py --source video.mp4\n\n# Webcam\npython tools/track.py --source 0\n\n# Save output\npython tools/track.py --source video.mp4 --output tracked.mp4\n\n# Use RetinaFace instead of SCRFD\npython tools/track.py --source video.mp4 --detector retinaface\n\n# Keep lost tracks longer\npython tools/track.py --source video.mp4 --track-buffer 60\n</code></pre>"},{"location":"modules/tracking/#references","title":"References","text":"<ul> <li>yakhyo/bytetrack-tracker \u2014 standalone BYTETracker implementation used in UniFace</li> <li>ByteTrack paper \u2014 Zhang et al., \"ByteTrack: Multi-Object Tracking by Associating Every Detection Box\"</li> </ul>"},{"location":"modules/tracking/#see-also","title":"See Also","text":"<ul> <li>Detection \u2014 face detection models</li> <li>Video &amp; Webcam \u2014 video processing patterns</li> <li>Inputs &amp; Outputs \u2014 data types and formats</li> </ul>"},{"location":"recipes/anonymize-stream/","title":"Anonymize Stream","text":"<p>Blur faces in real-time video streams for privacy protection.</p> <p>Work in Progress</p> <p>This page contains example code patterns. Test thoroughly before using in production.</p>"},{"location":"recipes/anonymize-stream/#webcam-anonymization","title":"Webcam Anonymization","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='pixelate')\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n\n    cv2.imshow('Anonymized', frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/anonymize-stream/#video-file-anonymization","title":"Video File Anonymization","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian')\n\ncap = cv2.VideoCapture(\"input.mp4\")\nfps = cap.get(cv2.CAP_PROP_FPS)\nw, h = int(cap.get(3)), int(cap.get(4))\n\nout = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\nwhile cap.read()[0]:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    blurrer.anonymize(frame, faces, inplace=True)\n    out.write(frame)\n\ncap.release()\nout.release()\n</code></pre>"},{"location":"recipes/anonymize-stream/#single-image","title":"Single Image","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='pixelate')\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\nresult = blurrer.anonymize(image, faces)\ncv2.imwrite(\"anonymized.jpg\", result)\n</code></pre>"},{"location":"recipes/anonymize-stream/#available-blur-methods","title":"Available Blur Methods","text":"Method Usage Pixelate <code>BlurFace(method='pixelate', pixel_blocks=15)</code> Gaussian <code>BlurFace(method='gaussian', blur_strength=3.0)</code> Blackout <code>BlurFace(method='blackout', color=(0,0,0))</code> Elliptical <code>BlurFace(method='elliptical', margin=20)</code> Median <code>BlurFace(method='median', blur_strength=3.0)</code>"},{"location":"recipes/anonymize-stream/#see-also","title":"See Also","text":"<ul> <li>Privacy Module - Privacy protection details</li> <li>Video &amp; Webcam - Real-time processing</li> <li>Detection Module - Face detection</li> </ul>"},{"location":"recipes/batch-processing/","title":"Batch Processing","text":"<p>Process multiple images efficiently.</p> <p>Work in Progress</p> <p>This page contains example code patterns. Test thoroughly before using in production.</p>"},{"location":"recipes/batch-processing/#basic-batch-processing","title":"Basic Batch Processing","text":"<pre><code>import cv2\nfrom pathlib import Path\nfrom uniface.detection import RetinaFace\n\ndetector = RetinaFace()\n\ndef process_directory(input_dir, output_dir):\n    \"\"\"Process all images in a directory.\"\"\"\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    for image_path in input_path.glob(\"*.jpg\"):\n        print(f\"Processing {image_path.name}...\")\n\n        image = cv2.imread(str(image_path))\n        faces = detector.detect(image)\n\n        print(f\"  Found {len(faces)} face(s)\")\n\n        # Process and save results\n        # ... your code here ...\n\n# Usage\nprocess_directory(\"input_images/\", \"output_images/\")\n</code></pre>"},{"location":"recipes/batch-processing/#with-progress-bar","title":"With Progress Bar","text":"<pre><code>from tqdm import tqdm\n\nfor image_path in tqdm(image_files, desc=\"Processing\"):\n    # ... process image ...\n    pass\n</code></pre>"},{"location":"recipes/batch-processing/#extract-embeddings","title":"Extract Embeddings","text":"<pre><code>from uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\nimport numpy as np\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\nembeddings = {}\nfor image_path in Path(\"faces/\").glob(\"*.jpg\"):\n    image = cv2.imread(str(image_path))\n    faces = detector.detect(image)\n\n    if faces:\n        embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n        embeddings[image_path.stem] = embedding\n\n# Save embeddings\nnp.savez(\"embeddings.npz\", **embeddings)\n</code></pre>"},{"location":"recipes/batch-processing/#see-also","title":"See Also","text":"<ul> <li>Video &amp; Webcam - Real-time processing</li> <li>Face Search - Search through embeddings</li> <li>Image Pipeline - Full analysis pipeline</li> <li>Detection Module - Detection options</li> </ul>"},{"location":"recipes/custom-models/","title":"Custom Models","text":"<p>Add your own ONNX models to UniFace.</p> <p>Work in Progress</p> <p>This page contains example code patterns for advanced users. Test thoroughly before using in production.</p>"},{"location":"recipes/custom-models/#overview","title":"Overview","text":"<p>UniFace is designed to be extensible. You can add custom ONNX models by:</p> <ol> <li>Creating a class that inherits from the appropriate base class</li> <li>Implementing required methods</li> <li>Using the ONNX Runtime utilities provided by UniFace</li> </ol>"},{"location":"recipes/custom-models/#add-custom-detection-model","title":"Add Custom Detection Model","text":"<pre><code>from uniface.detection.base import BaseDetector\nfrom uniface.onnx_utils import create_onnx_session\nfrom uniface.types import Face\nimport numpy as np\n\nclass MyDetector(BaseDetector):\n    def __init__(self, model_path: str, confidence_threshold: float = 0.5):\n        super().__init__(confidence_threshold=confidence_threshold)\n        self.session = create_onnx_session(model_path)\n        self.threshold = confidence_threshold\n\n    def preprocess(self, image: np.ndarray) -&gt; np.ndarray:\n        # Your preprocessing logic\n        # e.g., resize, normalize, transpose\n        raise NotImplementedError\n\n    def postprocess(self, outputs, shape) -&gt; list[Face]:\n        # Your postprocessing logic\n        # e.g., decode boxes, apply NMS, create Face objects\n        raise NotImplementedError\n\n    def detect(self, image: np.ndarray) -&gt; list[Face]:\n        # 1. Preprocess image\n        input_tensor = self.preprocess(image)\n\n        # 2. Run inference\n        outputs = self.session.run(None, {'input': input_tensor})\n\n        # 3. Postprocess outputs to Face objects\n        return self.postprocess(outputs, image.shape)\n</code></pre>"},{"location":"recipes/custom-models/#add-custom-recognition-model","title":"Add Custom Recognition Model","text":"<pre><code>from uniface.recognition.base import BaseRecognizer, PreprocessConfig\n\nclass MyRecognizer(BaseRecognizer):\n    def __init__(self, model_path: str, providers=None):\n        preprocessing = PreprocessConfig(input_mean=127.5, input_std=127.5, input_size=(112, 112))\n        super().__init__(model_path, preprocessing, providers=providers)\n\n    # Optional: override preprocess() if your model expects custom normalization.\n</code></pre>"},{"location":"recipes/custom-models/#usage","title":"Usage","text":"<pre><code>from my_module import MyDetector, MyRecognizer\n\n# Use custom models\ndetector = MyDetector(\"path/to/detection_model.onnx\")\nrecognizer = MyRecognizer(\"path/to/recognition_model.onnx\")\n\n# Use like built-in models\nfaces = detector.detect(image)\nembedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n</code></pre>"},{"location":"recipes/custom-models/#see-also","title":"See Also","text":"<ul> <li>Detection Module - Built-in detection models</li> <li>Recognition Module - Built-in recognition models</li> <li>Concepts: Overview - Architecture overview</li> </ul>"},{"location":"recipes/face-search/","title":"Face Search","text":"<p>Build a face search system for finding people in images.</p> <p>Work in Progress</p> <p>This page contains example code patterns. Test thoroughly before using in production.</p>"},{"location":"recipes/face-search/#basic-face-database","title":"Basic Face Database","text":"<pre><code>import numpy as np\nimport cv2\nfrom pathlib import Path\nfrom uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\n\nclass FaceDatabase:\n    def __init__(self):\n        self.detector = RetinaFace()\n        self.recognizer = ArcFace()\n        self.embeddings = {}\n\n    def add_face(self, person_id, image):\n        \"\"\"Add a face to the database.\"\"\"\n        faces = self.detector.detect(image)\n        if not faces:\n            raise ValueError(f\"No face found for {person_id}\")\n\n        face = max(faces, key=lambda f: f.confidence)\n        embedding = self.recognizer.get_normalized_embedding(image, face.landmarks)\n        self.embeddings[person_id] = embedding\n        return True\n\n    def search(self, image, threshold=0.6):\n        \"\"\"Search for faces in an image.\"\"\"\n        faces = self.detector.detect(image)\n        results = []\n\n        for face in faces:\n            embedding = self.recognizer.get_normalized_embedding(image, face.landmarks)\n\n            best_match = None\n            best_similarity = -1\n\n            for person_id, db_embedding in self.embeddings.items():\n                similarity = np.dot(embedding, db_embedding.T)[0][0]\n                if similarity &gt; best_similarity:\n                    best_similarity = similarity\n                    best_match = person_id\n\n            results.append({\n                'bbox': face.bbox,\n                'match': best_match if best_similarity &gt;= threshold else None,\n                'similarity': best_similarity\n            })\n\n        return results\n\n    def save(self, path):\n        \"\"\"Save database to file.\"\"\"\n        np.savez(path, embeddings=dict(self.embeddings))\n\n    def load(self, path):\n        \"\"\"Load database from file.\"\"\"\n        data = np.load(path, allow_pickle=True)\n        self.embeddings = data['embeddings'].item()\n\n# Usage\ndb = FaceDatabase()\n\n# Add faces\nfor image_path in Path(\"known_faces/\").glob(\"*.jpg\"):\n    person_id = image_path.stem\n    image = cv2.imread(str(image_path))\n    try:\n        db.add_face(person_id, image)\n        print(f\"Added: {person_id}\")\n    except ValueError as e:\n        print(f\"Skipped: {e}\")\n\n# Save database\ndb.save(\"face_database.npz\")\n\n# Search\nquery_image = cv2.imread(\"group_photo.jpg\")\nresults = db.search(query_image)\n\nfor r in results:\n    if r['match']:\n        print(f\"Found: {r['match']} (similarity: {r['similarity']:.3f})\")\n</code></pre>"},{"location":"recipes/face-search/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef visualize_search_results(image, results):\n    \"\"\"Draw search results on image.\"\"\"\n    for r in results:\n        x1, y1, x2, y2 = map(int, r['bbox'])\n\n        if r['match']:\n            color = (0, 255, 0)  # Green for match\n            label = f\"{r['match']} ({r['similarity']:.2f})\"\n        else:\n            color = (0, 0, 255)  # Red for unknown\n            label = f\"Unknown ({r['similarity']:.2f})\"\n\n        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(image, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    return image\n\n# Usage\nresults = db.search(image)\nannotated = visualize_search_results(image.copy(), results)\ncv2.imwrite(\"search_result.jpg\", annotated)\n</code></pre>"},{"location":"recipes/face-search/#real-time-search","title":"Real-Time Search","text":"<pre><code>import cv2\n\ndef realtime_search(db):\n    \"\"\"Real-time face search from webcam.\"\"\"\n    cap = cv2.VideoCapture(0)\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        results = db.search(frame, threshold=0.5)\n\n        for r in results:\n            x1, y1, x2, y2 = map(int, r['bbox'])\n\n            if r['match']:\n                color = (0, 255, 0)\n                label = r['match']\n            else:\n                color = (0, 0, 255)\n                label = \"Unknown\"\n\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            cv2.putText(frame, label, (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n        cv2.imshow(\"Face Search\", frame)\n\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n\n# Usage\ndb = FaceDatabase()\ndb.load(\"face_database.npz\")\nrealtime_search(db)\n</code></pre>"},{"location":"recipes/face-search/#see-also","title":"See Also","text":"<ul> <li>Recognition Module - Face recognition details</li> <li>Batch Processing - Process multiple files</li> <li>Video &amp; Webcam - Real-time processing</li> <li>Concepts: Thresholds - Tuning similarity thresholds</li> </ul>"},{"location":"recipes/image-pipeline/","title":"Image Pipeline","text":"<p>A complete pipeline for processing images with detection, recognition, and attribute analysis.</p>"},{"location":"recipes/image-pipeline/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>import cv2\nfrom uniface.attribute import AgeGender\nfrom uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\nfrom uniface.draw import draw_detections\n\n# Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\nage_gender = AgeGender()\n\ndef process_image(image_path):\n    \"\"\"Process a single image through the full pipeline.\"\"\"\n    # Load image\n    image = cv2.imread(image_path)\n\n    # Step 1: Detect faces\n    faces = detector.detect(image)\n    print(f\"Found {len(faces)} face(s)\")\n\n    results = []\n\n    for i, face in enumerate(faces):\n        # Step 2: Extract embedding\n        embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n        # Step 3: Predict attributes\n        attrs = age_gender.predict(image, face.bbox)\n\n        results.append({\n            'face_id': i,\n            'bbox': face.bbox,\n            'confidence': face.confidence,\n            'embedding': embedding,\n            'gender': attrs.sex,\n            'age': attrs.age\n        })\n\n        print(f\"  Face {i+1}: {attrs.sex}, {attrs.age} years old\")\n\n    # Visualize\n    draw_detections(\n        image=image,\n        bboxes=[f.bbox for f in faces],\n        scores=[f.confidence for f in faces],\n        landmarks=[f.landmarks for f in faces]\n    )\n\n    return image, results\n\n# Usage\nresult_image, results = process_image(\"photo.jpg\")\ncv2.imwrite(\"result.jpg\", result_image)\n</code></pre>"},{"location":"recipes/image-pipeline/#using-faceanalyzer","title":"Using FaceAnalyzer","text":"<p>For convenience, use the built-in <code>FaceAnalyzer</code>:</p> <pre><code>from uniface.analyzer import FaceAnalyzer\nfrom uniface.attribute import AgeGender\nfrom uniface.detection import RetinaFace\nfrom uniface.recognition import ArcFace\nimport cv2\n\n# Initialize with desired modules\ndetector = RetinaFace()\nrecognizer = ArcFace()\nage_gender = AgeGender()\n\nanalyzer = FaceAnalyzer(\n    detector,\n    recognizer=recognizer,\n    age_gender=age_gender,\n)\n\n# Process image\nimage = cv2.imread(\"photo.jpg\")\nfaces = analyzer.analyze(image)\n\n# Access enriched Face objects\nfor face in faces:\n    print(f\"Confidence: {face.confidence:.2f}\")\n    print(f\"Embedding: {face.embedding.shape}\")\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n</code></pre>"},{"location":"recipes/image-pipeline/#full-analysis-pipeline","title":"Full Analysis Pipeline","text":"<p>Complete pipeline with all modules:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface.attribute import AgeGender, FairFace\nfrom uniface.detection import RetinaFace\nfrom uniface.gaze import MobileGaze\nfrom uniface.landmark import Landmark106\nfrom uniface.recognition import ArcFace\nfrom uniface.parsing import BiSeNet\nfrom uniface.spoofing import MiniFASNet\nfrom uniface.draw import draw_detections, draw_gaze\n\nclass FaceAnalysisPipeline:\n    def __init__(self):\n        # Initialize all models\n        self.detector = RetinaFace()\n        self.recognizer = ArcFace()\n        self.age_gender = AgeGender()\n        self.fairface = FairFace()\n        self.landmarker = Landmark106()\n        self.gaze = MobileGaze()\n        self.parser = BiSeNet()\n        self.spoofer = MiniFASNet()\n\n    def analyze(self, image):\n        \"\"\"Run full analysis pipeline.\"\"\"\n        faces = self.detector.detect(image)\n        results = []\n\n        for face in faces:\n            result = {\n                'bbox': face.bbox,\n                'confidence': face.confidence,\n                'landmarks_5': face.landmarks\n            }\n\n            # Recognition embedding\n            result['embedding'] = self.recognizer.get_normalized_embedding(\n                image, face.landmarks\n            )\n\n            # Attributes\n            ag_result = self.age_gender.predict(image, face.bbox)\n            result['age'] = ag_result.age\n            result['gender'] = ag_result.sex\n\n            # FairFace attributes\n            ff_result = self.fairface.predict(image, face.bbox)\n            result['age_group'] = ff_result.age_group\n            result['race'] = ff_result.race\n\n            # 106-point landmarks\n            result['landmarks_106'] = self.landmarker.get_landmarks(\n                image, face.bbox\n            )\n\n            # Gaze estimation\n            x1, y1, x2, y2 = map(int, face.bbox)\n            face_crop = image[y1:y2, x1:x2]\n            if face_crop.size &gt; 0:\n                gaze_result = self.gaze.estimate(face_crop)\n                result['gaze_pitch'] = gaze_result.pitch\n                result['gaze_yaw'] = gaze_result.yaw\n\n            # Face parsing\n            if face_crop.size &gt; 0:\n                result['parsing_mask'] = self.parser.parse(face_crop)\n\n            # Anti-spoofing\n            spoof_result = self.spoofer.predict(image, face.bbox)\n            result['is_real'] = spoof_result.is_real\n            result['spoof_confidence'] = spoof_result.confidence\n\n            results.append(result)\n\n        return results\n\n# Usage\npipeline = FaceAnalysisPipeline()\nresults = pipeline.analyze(cv2.imread(\"photo.jpg\"))\n\nfor i, r in enumerate(results):\n    print(f\"\\nFace {i+1}:\")\n    print(f\"  Gender: {r['gender']}, Age: {r['age']}\")\n    print(f\"  Race: {r['race']}, Age Group: {r['age_group']}\")\n    print(f\"  Gaze: pitch={np.degrees(r['gaze_pitch']):.1f}\u00b0\")\n    print(f\"  Real: {r['is_real']} ({r['spoof_confidence']:.1%})\")\n</code></pre>"},{"location":"recipes/image-pipeline/#visualization-pipeline","title":"Visualization Pipeline","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface.attribute import AgeGender\nfrom uniface.detection import RetinaFace\nfrom uniface.gaze import MobileGaze\nfrom uniface.draw import draw_detections, draw_gaze\n\ndef visualize_analysis(image_path, output_path):\n    \"\"\"Create annotated visualization of face analysis.\"\"\"\n    detector = RetinaFace()\n    age_gender = AgeGender()\n    gaze = MobileGaze()\n\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n\n        # Draw bounding box\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # Age and gender\n        attrs = age_gender.predict(image, face.bbox)\n        label = f\"{attrs.sex}, {attrs.age}y\"\n        cv2.putText(image, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n        # Gaze\n        face_crop = image[y1:y2, x1:x2]\n        if face_crop.size &gt; 0:\n            gaze_result = gaze.estimate(face_crop)\n            draw_gaze(image, face.bbox, gaze_result.pitch, gaze_result.yaw)\n\n        # Confidence\n        conf_label = f\"{face.confidence:.0%}\"\n        cv2.putText(image, conf_label, (x1, y2 + 20),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    cv2.imwrite(output_path, image)\n    print(f\"Saved to {output_path}\")\n\n# Usage\nvisualize_analysis(\"input.jpg\", \"output.jpg\")\n</code></pre>"},{"location":"recipes/image-pipeline/#json-output","title":"JSON Output","text":"<p>Export results to JSON:</p> <pre><code>import json\nimport numpy as np\n\ndef results_to_json(results):\n    \"\"\"Convert analysis results to JSON-serializable format.\"\"\"\n    output = []\n\n    for r in results:\n        item = {\n            'bbox': r['bbox'].tolist(),\n            'confidence': float(r['confidence']),\n            'age': int(r['age']) if r.get('age') else None,\n            'gender': r.get('gender'),\n            'race': r.get('race'),\n            'is_real': r.get('is_real'),\n            'gaze': {\n                'pitch_deg': float(np.degrees(r['gaze_pitch'])) if 'gaze_pitch' in r else None,\n                'yaw_deg': float(np.degrees(r['gaze_yaw'])) if 'gaze_yaw' in r else None\n            }\n        }\n        output.append(item)\n\n    return output\n\n# Usage\nresults = pipeline.analyze(image)\njson_data = results_to_json(results)\n\nwith open('results.json', 'w') as f:\n    json.dump(json_data, f, indent=2)\n</code></pre>"},{"location":"recipes/image-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Batch Processing - Process multiple images</li> <li>Video &amp; Webcam - Real-time processing</li> <li>Face Search - Build a search system</li> <li>Detection Module - Detection options</li> <li>Recognition Module - Recognition details</li> </ul>"},{"location":"recipes/video-webcam/","title":"Video &amp; Webcam","text":"<p>Real-time face analysis for video streams.</p> <p>Work in Progress</p> <p>This page contains example code patterns. Test thoroughly before using in production.</p>"},{"location":"recipes/video-webcam/#webcam-detection","title":"Webcam Detection","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\nfrom uniface.draw import draw_detections\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\nprint(\"Press 'q' to quit\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    draw_detections(\n        image=frame,\n        bboxes=[f.bbox for f in faces],\n        scores=[f.confidence for f in faces],\n        landmarks=[f.landmarks for f in faces]\n    )\n\n    cv2.imshow(\"Face Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#video-file-processing","title":"Video File Processing","text":"<pre><code>import cv2\nfrom uniface.detection import RetinaFace\n\ndef process_video(input_path, output_path):\n    \"\"\"Process a video file.\"\"\"\n    detector = RetinaFace()\n    cap = cv2.VideoCapture(input_path)\n\n    # Get video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n    # Setup output\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    while cap.read()[0]:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        faces = detector.detect(frame)\n        # ... process and draw ...\n\n        out.write(frame)\n\n    cap.release()\n    out.release()\n\n# Usage\nprocess_video(\"input.mp4\", \"output.mp4\")\n</code></pre>"},{"location":"recipes/video-webcam/#webcam-tracking","title":"Webcam Tracking","text":"<p>To track faces across frames with persistent IDs, pair a detector with <code>BYTETracker</code>:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface.common import xyxy_to_cxcywh\nfrom uniface.detection import SCRFD\nfrom uniface.tracking import BYTETracker\nfrom uniface.draw import draw_tracks\n\ndetector = SCRFD()\ntracker = BYTETracker(track_thresh=0.5, track_buffer=30)\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    dets = np.array([[*f.bbox, f.confidence] for f in faces])\n    dets = dets if len(dets) &gt; 0 else np.empty((0, 5))\n\n    tracks = tracker.update(dets)\n\n    if len(tracks) &gt; 0 and len(faces) &gt; 0:\n        face_bboxes = np.array([f.bbox for f in faces], dtype=np.float32)\n        track_ids = tracks[:, 4].astype(int)\n\n        face_centers = xyxy_to_cxcywh(face_bboxes)[:, :2]\n        track_centers = xyxy_to_cxcywh(tracks[:, :4])[:, :2]\n\n        for ti in range(len(tracks)):\n            dists = (track_centers[ti, 0] - face_centers[:, 0]) ** 2 + (track_centers[ti, 1] - face_centers[:, 1]) ** 2\n            faces[int(np.argmin(dists))].track_id = track_ids[ti]\n\n    draw_tracks(image=frame, faces=[f for f in faces if f.track_id is not None])\n    cv2.imshow(\"Face Tracking\", frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre> <p>For more details on tracker parameters and tuning, see Tracking.</p>"},{"location":"recipes/video-webcam/#performance-tips","title":"Performance Tips","text":""},{"location":"recipes/video-webcam/#skip-frames","title":"Skip Frames","text":"<pre><code>PROCESS_EVERY_N = 3  # Process every 3rd frame\nframe_count = 0\nlast_faces = []\n\nwhile True:\n    ret, frame = cap.read()\n    if frame_count % PROCESS_EVERY_N == 0:\n        last_faces = detector.detect(frame)\n    frame_count += 1\n    # Draw last_faces...\n</code></pre>"},{"location":"recipes/video-webcam/#fps-counter","title":"FPS Counter","text":"<pre><code>import time\n\nprev_time = time.time()\nwhile True:\n    curr_time = time.time()\n    fps = 1 / (curr_time - prev_time)\n    prev_time = curr_time\n\n    cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n</code></pre>"},{"location":"recipes/video-webcam/#see-also","title":"See Also","text":"<ul> <li>Tracking Module - Face tracking with BYTETracker</li> <li>Anonymize Stream - Privacy protection in video</li> <li>Batch Processing - Process multiple files</li> <li>Detection Module - Detection options</li> <li>Gaze Module - Gaze estimation</li> </ul>"}]}