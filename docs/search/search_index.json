{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":""},{"location":"#uniface","title":"UniFace","text":"<p>A lightweight, production-ready face analysis library built on ONNX Runtime.</p> <p> </p> <p></p> <p>Get Started View on GitHub</p>"},{"location":"#features","title":"Features","text":""},{"location":"#face-detection","title":"Face Detection","text":"<p>ONNX-optimized RetinaFace, SCRFD, and YOLOv5-Face models with 5-point landmarks.</p>"},{"location":"#face-recognition","title":"Face Recognition","text":"<p>ArcFace, MobileFace, and SphereFace embeddings for identity verification.</p>"},{"location":"#landmarks","title":"Landmarks","text":"<p>Accurate 106-point facial landmark localization for detailed face analysis.</p>"},{"location":"#attributes","title":"Attributes","text":"<p>Age, gender, race (FairFace), and emotion detection from faces.</p>"},{"location":"#face-parsing","title":"Face Parsing","text":"<p>BiSeNet semantic segmentation with 19 facial component classes.</p>"},{"location":"#gaze-estimation","title":"Gaze Estimation","text":"<p>Real-time gaze direction prediction with MobileGaze models.</p>"},{"location":"#anti-spoofing","title":"Anti-Spoofing","text":"<p>Face liveness detection with MiniFASNet to prevent fraud.</p>"},{"location":"#privacy","title":"Privacy","text":"<p>Face anonymization with 5 blur methods for privacy protection.</p>"},{"location":"#installation","title":"Installation","text":"StandardGPU (CUDA)From Source <pre><code>pip install uniface\n</code></pre> <pre><code>pip install uniface[gpu]\n</code></pre> <pre><code>git clone https://github.com/yakhyo/uniface.git\ncd uniface\npip install -e .\n</code></pre>"},{"location":"#next-steps","title":"Next Steps","text":""},{"location":"#quickstart","title":"Quickstart","text":"<p>Get up and running in 5 minutes with common use cases.</p> <p>Quickstart Guide \u2192</p>"},{"location":"#concepts","title":"Concepts","text":"<p>Learn about the architecture and design principles.</p> <p>Read Concepts \u2192</p>"},{"location":"#modules","title":"Modules","text":"<p>Explore individual modules and their APIs.</p> <p>Browse Modules \u2192</p>"},{"location":"#recipes","title":"Recipes","text":"<p>Complete examples for common workflows.</p> <p>View Recipes \u2192</p>"},{"location":"#license","title":"License","text":"<p>UniFace is released under the MIT License.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to UniFace.</p>"},{"location":"changelog/#200-2025","title":"[2.0.0] - 2025","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>YOLOv5-Face detection models (N/S/M variants)</li> <li>FairFace attribute prediction (race, gender, age group)</li> <li>Face parsing with BiSeNet (ResNet18/34)</li> <li>Gaze estimation with MobileGaze</li> <li>Anti-spoofing with MiniFASNet</li> <li>Face anonymization with 5 blur methods</li> <li>FaceAnalyzer for combined analysis</li> <li>Type hints throughout</li> <li>Comprehensive documentation</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Unified API across all modules</li> <li>Improved model download with SHA-256 verification</li> <li>Better error messages</li> </ul>"},{"location":"changelog/#100-2024","title":"[1.0.0] - 2024","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>RetinaFace detection</li> <li>SCRFD detection</li> <li>ArcFace recognition</li> <li>MobileFace recognition</li> <li>SphereFace recognition</li> <li>106-point landmarks</li> <li>Age/Gender prediction</li> <li>Emotion detection</li> </ul>"},{"location":"changelog/#contributing","title":"Contributing","text":"<p>See Contributing Guide for how to contribute.</p>"},{"location":"contributing/","title":"Contributing","text":"<p>Thank you for contributing to UniFace!</p>"},{"location":"contributing/#quick-start","title":"Quick Start","text":"<pre><code># Clone\ngit clone https://github.com/yakhyo/uniface.git\ncd uniface\n\n# Install dev dependencies\npip install -e \".[dev]\"\n\n# Run tests\npytest\n</code></pre>"},{"location":"contributing/#code-style","title":"Code Style","text":"<p>We use Ruff for formatting:</p> <pre><code>ruff format .\nruff check . --fix\n</code></pre> <p>Guidelines:</p> <ul> <li>Line length: 120</li> <li>Python 3.11+ type hints</li> <li>Google-style docstrings</li> </ul>"},{"location":"contributing/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<pre><code>pip install pre-commit\npre-commit install\npre-commit run --all-files\n</code></pre>"},{"location":"contributing/#pull-request-process","title":"Pull Request Process","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Write tests for new features</li> <li>Ensure tests pass</li> <li>Submit PR with clear description</li> </ol>"},{"location":"contributing/#adding-new-models","title":"Adding New Models","text":"<ol> <li>Create model class in appropriate submodule</li> <li>Add weight constants to <code>uniface/constants.py</code></li> <li>Export in <code>__init__.py</code> files</li> <li>Write tests in <code>tests/</code></li> <li>Add example in <code>tools/</code> or notebooks</li> </ol>"},{"location":"contributing/#questions","title":"Questions?","text":"<p>Open an issue on GitHub.</p>"},{"location":"faq/","title":"FAQ","text":"<p>Frequently asked questions.</p>"},{"location":"faq/#general","title":"General","text":""},{"location":"faq/#what-is-uniface","title":"What is UniFace?","text":"<p>A Python library for face analysis: detection, recognition, landmarks, attributes, parsing, gaze estimation, anti-spoofing, and privacy protection.</p>"},{"location":"faq/#what-are-the-requirements","title":"What are the requirements?","text":"<ul> <li>Python 3.11+</li> <li>Works on macOS, Linux, Windows</li> </ul>"},{"location":"faq/#is-gpu-required","title":"Is GPU required?","text":"<p>No. CPU works fine. GPU (CUDA) provides faster inference.</p>"},{"location":"faq/#models","title":"Models","text":""},{"location":"faq/#where-are-models-stored","title":"Where are models stored?","text":"<pre><code>~/.uniface/models/\n</code></pre>"},{"location":"faq/#how-to-use-offline","title":"How to use offline?","text":"<p>Pre-download models:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\nverify_model_weights(RetinaFaceWeights.MNET_V2)\n</code></pre>"},{"location":"faq/#which-detection-model-is-best","title":"Which detection model is best?","text":"Use Case Model Balanced RetinaFace MNET_V2 Accuracy SCRFD 10G Speed YOLOv5n-Face"},{"location":"faq/#usage","title":"Usage","text":""},{"location":"faq/#what-image-format","title":"What image format?","text":"<p>BGR (OpenCV default):</p> <pre><code>image = cv2.imread(\"photo.jpg\")  # BGR\n</code></pre>"},{"location":"faq/#how-to-compare-faces","title":"How to compare faces?","text":"<pre><code>from uniface import compute_similarity\n\nsimilarity = compute_similarity(emb1, emb2)\nif similarity &gt; 0.6:\n    print(\"Same person\")\n</code></pre>"},{"location":"faq/#how-to-get-age-and-gender","title":"How to get age and gender?","text":"<pre><code>from uniface import AgeGender\n\npredictor = AgeGender()\nresult = predictor.predict(image, face.bbox)\nprint(f\"{result.sex}, {result.age}\")\n</code></pre>"},{"location":"faq/#performance","title":"Performance","text":""},{"location":"faq/#how-to-speed-up-detection","title":"How to speed up detection?","text":"<ol> <li> <p>Use smaller input:    <pre><code>detector = RetinaFace(input_size=(320, 320))\n</code></pre></p> </li> <li> <p>Skip frames in video:    <pre><code>if frame_count % 3 == 0:\n    faces = detector.detect(frame)\n</code></pre></p> </li> <li> <p>Use GPU:    <pre><code>pip install uniface[gpu]\n</code></pre></p> </li> </ol>"},{"location":"faq/#accuracy","title":"Accuracy","text":""},{"location":"faq/#detection-threshold","title":"Detection threshold?","text":"<p>Default: 0.5</p> <ul> <li>Higher (0.7+): Fewer false positives</li> <li>Lower (0.3): More detections</li> </ul>"},{"location":"faq/#similarity-threshold","title":"Similarity threshold?","text":"Threshold Meaning &gt; 0.6 Same person 0.4-0.6 Uncertain &lt; 0.4 Different"},{"location":"faq/#privacy","title":"Privacy","text":""},{"location":"faq/#how-to-blur-faces","title":"How to blur faces?","text":"<pre><code>from uniface.privacy import anonymize_faces\n\nresult = anonymize_faces(image, method='pixelate')\n</code></pre>"},{"location":"faq/#available-blur-methods","title":"Available blur methods?","text":"<p><code>pixelate</code>, <code>gaussian</code>, <code>blackout</code>, <code>elliptical</code>, <code>median</code></p>"},{"location":"installation/","title":"Installation","text":"<p>This guide covers all installation options for UniFace.</p>"},{"location":"installation/#requirements","title":"Requirements","text":"<ul> <li>Python: 3.11 or higher</li> <li>Operating Systems: macOS, Linux, Windows</li> </ul>"},{"location":"installation/#quick-install","title":"Quick Install","text":"<p>The simplest way to install UniFace:</p> <pre><code>pip install uniface\n</code></pre> <p>This installs the CPU version with all core dependencies.</p>"},{"location":"installation/#platform-specific-installation","title":"Platform-Specific Installation","text":""},{"location":"installation/#macos-apple-silicon-m1m2m3m4","title":"macOS (Apple Silicon - M1/M2/M3/M4)","text":"<p>For Apple Silicon Macs, the standard installation automatically includes ARM64 optimizations:</p> <pre><code>pip install uniface\n</code></pre> <p>Native Performance</p> <p>The base <code>onnxruntime</code> package has native Apple Silicon support with ARM64 optimizations built-in since version 1.13+. No additional configuration needed.</p> <p>Verify ARM64 installation:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64\n</code></pre>"},{"location":"installation/#linuxwindows-with-nvidia-gpu","title":"Linux/Windows with NVIDIA GPU","text":"<p>For CUDA acceleration on NVIDIA GPUs:</p> <pre><code>pip install uniface[gpu]\n</code></pre> <p>Requirements:</p> <ul> <li>CUDA 11.x or 12.x</li> <li>cuDNN 8.x</li> </ul> <p>CUDA Compatibility</p> <p>See ONNX Runtime GPU requirements for detailed compatibility matrix.</p> <p>Verify GPU installation:</p> <pre><code>import onnxruntime as ort\nprint(\"Available providers:\", ort.get_available_providers())\n# Should include: 'CUDAExecutionProvider'\n</code></pre>"},{"location":"installation/#cpu-only-all-platforms","title":"CPU-Only (All Platforms)","text":"<pre><code>pip install uniface\n</code></pre> <p>Works on all platforms with automatic CPU fallback.</p>"},{"location":"installation/#install-from-source","title":"Install from Source","text":"<p>For development or the latest features:</p> <pre><code>git clone https://github.com/yakhyo/uniface.git\ncd uniface\npip install -e .\n</code></pre> <p>With development dependencies:</p> <pre><code>pip install -e \".[dev]\"\n</code></pre>"},{"location":"installation/#dependencies","title":"Dependencies","text":"<p>UniFace has minimal dependencies:</p> Package Purpose <code>numpy</code> Array operations <code>opencv-python</code> Image processing <code>onnxruntime</code> Model inference <code>requests</code> Model download <code>tqdm</code> Progress bars"},{"location":"installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation:</p> <pre><code>import uniface\nprint(f\"UniFace version: {uniface.__version__}\")\n\n# Check available ONNX providers\nimport onnxruntime as ort\nprint(f\"Available providers: {ort.get_available_providers()}\")\n\n# Quick test\nfrom uniface import RetinaFace\ndetector = RetinaFace()\nprint(\"Installation successful!\")\n</code></pre>"},{"location":"installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"installation/#import-errors","title":"Import Errors","text":"<p>If you encounter import errors, ensure you're using Python 3.11+:</p> <pre><code>python --version\n# Should show: Python 3.11.x or higher\n</code></pre>"},{"location":"installation/#model-download-issues","title":"Model Download Issues","text":"<p>Models are automatically downloaded on first use. If downloads fail:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\n# Manually download a model\nmodel_path = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Model downloaded to: {model_path}\")\n</code></pre>"},{"location":"installation/#performance-issues-on-mac","title":"Performance Issues on Mac","text":"<p>Verify you're using the ARM64 build (not x86_64 via Rosetta):</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64 (not x86_64)\n</code></pre>"},{"location":"installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quickstart - Get started with common use cases</li> <li>Concepts Overview - Understand the architecture</li> </ul>"},{"location":"license-attribution/","title":"Licenses &amp; Attribution","text":""},{"location":"license-attribution/#uniface-license","title":"UniFace License","text":"<p>UniFace is released under the MIT License.</p>"},{"location":"license-attribution/#model-credits","title":"Model Credits","text":"Model Source License RetinaFace yakhyo/retinaface-pytorch MIT SCRFD InsightFace MIT YOLOv5-Face deepcam-cn/yolov5-face GPL-3.0 ArcFace InsightFace MIT MobileFace yakhyo/face-recognition MIT SphereFace yakhyo/face-recognition MIT BiSeNet yakhyo/face-parsing MIT MobileGaze yakhyo/gaze-estimation MIT MiniFASNet minivision-ai/Silent-Face-Anti-Spoofing Apache-2.0 FairFace yakhyo/fairface-onnx CC BY 4.0"},{"location":"license-attribution/#papers","title":"Papers","text":"<ul> <li>RetinaFace: arXiv:1905.00641</li> <li>SCRFD: arXiv:2105.04714</li> <li>YOLOv5-Face: arXiv:2105.12931</li> <li>ArcFace: arXiv:1801.07698</li> <li>SphereFace: arXiv:1704.08063</li> <li>BiSeNet: arXiv:1808.00897</li> </ul>"},{"location":"license-attribution/#third-party-libraries","title":"Third-Party Libraries","text":"Library License ONNX Runtime MIT OpenCV Apache-2.0 NumPy BSD-3-Clause"},{"location":"quickstart/","title":"Quickstart","text":"<p>Get up and running with UniFace in 5 minutes. This guide covers the most common use cases.</p>"},{"location":"quickstart/#1-face-detection","title":"1. Face Detection","text":"<p>Detect faces in an image:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\n\n# Load image\nimage = cv2.imread(\"photo.jpg\")\n\n# Initialize detector (models auto-download on first use)\ndetector = RetinaFace()\n\n# Detect faces\nfaces = detector.detect(image)\n\n# Print results\nfor i, face in enumerate(faces):\n    print(f\"Face {i+1}:\")\n    print(f\"  Confidence: {face.confidence:.2f}\")\n    print(f\"  BBox: {face.bbox}\")\n    print(f\"  Landmarks: {len(face.landmarks)} points\")\n</code></pre> <p>Output:</p> <pre><code>Face 1:\n  Confidence: 0.99\n  BBox: [120.5, 85.3, 245.8, 210.6]\n  Landmarks: 5 points\n</code></pre>"},{"location":"quickstart/#2-visualize-detections","title":"2. Visualize Detections","text":"<p>Draw bounding boxes and landmarks:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.visualization import draw_detections\n\n# Detect faces\ndetector = RetinaFace()\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\n# Extract visualization data\nbboxes = [f.bbox for f in faces]\nscores = [f.confidence for f in faces]\nlandmarks = [f.landmarks for f in faces]\n\n# Draw on image\ndraw_detections(\n    image=image,\n    bboxes=bboxes,\n    scores=scores,\n    landmarks=landmarks,\n    vis_threshold=0.6,\n)\n\n# Save result\ncv2.imwrite(\"output.jpg\", image)\n</code></pre>"},{"location":"quickstart/#3-face-recognition","title":"3. Face Recognition","text":"<p>Compare two faces:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, ArcFace\n\n# Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Load two images\nimage1 = cv2.imread(\"person1.jpg\")\nimage2 = cv2.imread(\"person2.jpg\")\n\n# Detect faces\nfaces1 = detector.detect(image1)\nfaces2 = detector.detect(image2)\n\nif faces1 and faces2:\n    # Extract embeddings\n    emb1 = recognizer.get_normalized_embedding(image1, faces1[0].landmarks)\n    emb2 = recognizer.get_normalized_embedding(image2, faces2[0].landmarks)\n\n    # Compute similarity (cosine similarity)\n    similarity = np.dot(emb1, emb2.T)[0][0]\n\n    # Interpret result\n    if similarity &gt; 0.6:\n        print(f\"Same person (similarity: {similarity:.3f})\")\n    else:\n        print(f\"Different people (similarity: {similarity:.3f})\")\n</code></pre> <p>Similarity Thresholds</p> <ul> <li><code>&gt; 0.6</code>: Same person (high confidence)</li> <li><code>0.4 - 0.6</code>: Uncertain (manual review)</li> <li><code>&lt; 0.4</code>: Different people</li> </ul>"},{"location":"quickstart/#4-age-gender-detection","title":"4. Age &amp; Gender Detection","text":"<pre><code>import cv2\nfrom uniface import RetinaFace, AgeGender\n\n# Initialize models\ndetector = RetinaFace()\nage_gender = AgeGender()\n\n# Load image\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\n# Predict attributes\nfor i, face in enumerate(faces):\n    result = age_gender.predict(image, face.bbox)\n    print(f\"Face {i+1}: {result.sex}, {result.age} years old\")\n</code></pre> <p>Output:</p> <pre><code>Face 1: Male, 32 years old\nFace 2: Female, 28 years old\n</code></pre>"},{"location":"quickstart/#5-fairface-attributes","title":"5. FairFace Attributes","text":"<p>Detect race, gender, and age group:</p> <pre><code>import cv2\nfrom uniface import RetinaFace, FairFace\n\ndetector = RetinaFace()\nfairface = FairFace()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    result = fairface.predict(image, face.bbox)\n    print(f\"Face {i+1}: {result.sex}, {result.age_group}, {result.race}\")\n</code></pre> <p>Output:</p> <pre><code>Face 1: Male, 30-39, East Asian\nFace 2: Female, 20-29, White\n</code></pre>"},{"location":"quickstart/#6-facial-landmarks-106-points","title":"6. Facial Landmarks (106 Points)","text":"<pre><code>import cv2\nfrom uniface import RetinaFace, Landmark106\n\ndetector = RetinaFace()\nlandmarker = Landmark106()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nif faces:\n    landmarks = landmarker.get_landmarks(image, faces[0].bbox)\n    print(f\"Detected {len(landmarks)} landmarks\")\n\n    # Draw landmarks\n    for x, y in landmarks.astype(int):\n        cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n\n    cv2.imwrite(\"landmarks.jpg\", image)\n</code></pre>"},{"location":"quickstart/#7-gaze-estimation","title":"7. Gaze Estimation","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, MobileGaze\nfrom uniface.visualization import draw_gaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    x1, y1, x2, y2 = map(int, face.bbox[:4])\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        result = gaze_estimator.estimate(face_crop)\n        print(f\"Face {i+1}: pitch={np.degrees(result.pitch):.1f}\u00b0, yaw={np.degrees(result.yaw):.1f}\u00b0\")\n\n        # Draw gaze direction\n        draw_gaze(image, face.bbox, result.pitch, result.yaw)\n\ncv2.imwrite(\"gaze_output.jpg\", image)\n</code></pre>"},{"location":"quickstart/#8-face-parsing","title":"8. Face Parsing","text":"<p>Segment face into semantic components:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface.parsing import BiSeNet\nfrom uniface.visualization import vis_parsing_maps\n\nparser = BiSeNet()\n\n# Load face image (already cropped)\nface_image = cv2.imread(\"face.jpg\")\n\n# Parse face into 19 components\nmask = parser.parse(face_image)\n\n# Visualize with overlay\nface_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\nvis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\nprint(f\"Detected {len(np.unique(mask))} facial components\")\n</code></pre>"},{"location":"quickstart/#9-face-anonymization","title":"9. Face Anonymization","text":"<p>Blur faces for privacy protection:</p> <pre><code>from uniface.privacy import anonymize_faces\nimport cv2\n\n# One-liner: automatic detection and blurring\nimage = cv2.imread(\"group_photo.jpg\")\nanonymized = anonymize_faces(image, method='pixelate')\ncv2.imwrite(\"anonymized.jpg\", anonymized)\n</code></pre> <p>Manual control:</p> <pre><code>from uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian', blur_strength=5.0)\n\nfaces = detector.detect(image)\nanonymized = blurrer.anonymize(image, faces)\n</code></pre> <p>Available methods:</p> Method Description <code>pixelate</code> Blocky effect (news media standard) <code>gaussian</code> Smooth, natural blur <code>blackout</code> Solid color boxes (maximum privacy) <code>elliptical</code> Soft oval blur (natural face shape) <code>median</code> Edge-preserving blur"},{"location":"quickstart/#10-face-anti-spoofing","title":"10. Face Anti-Spoofing","text":"<p>Detect real vs. fake faces:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    result = spoofer.predict(image, face.bbox)\n    label = 'Real' if result.is_real else 'Fake'\n    print(f\"Face {i+1}: {label} ({result.confidence:.1%})\")\n</code></pre>"},{"location":"quickstart/#11-webcam-demo","title":"11. Webcam Demo","text":"<p>Real-time face detection:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.visualization import draw_detections\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\nprint(\"Press 'q' to quit\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    bboxes = [f.bbox for f in faces]\n    scores = [f.confidence for f in faces]\n    landmarks = [f.landmarks for f in faces]\n    draw_detections(image=frame, bboxes=bboxes, scores=scores, landmarks=landmarks)\n\n    cv2.imshow(\"UniFace - Press 'q' to quit\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Concepts Overview - Understand the architecture</li> <li>Detection Module - Deep dive into detection models</li> <li>Recipes - Complete workflow examples</li> </ul>"},{"location":"troubleshooting/","title":"Troubleshooting","text":"<p>Common issues and solutions.</p>"},{"location":"troubleshooting/#installation-issues","title":"Installation Issues","text":""},{"location":"troubleshooting/#import-error","title":"Import Error","text":"<pre><code>ModuleNotFoundError: No module named 'uniface'\n</code></pre> <p>Solution: Install the package:</p> <pre><code>pip install uniface\n</code></pre>"},{"location":"troubleshooting/#python-version","title":"Python Version","text":"<pre><code>Python 3.10+ required\n</code></pre> <p>Solution: Check your Python version:</p> <pre><code>python --version  # Should be 3.11+\n</code></pre>"},{"location":"troubleshooting/#model-issues","title":"Model Issues","text":""},{"location":"troubleshooting/#model-download-failed","title":"Model Download Failed","text":"<pre><code>Failed to download model\n</code></pre> <p>Solution: Manually download:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\npath = verify_model_weights(RetinaFaceWeights.MNET_V2)\n</code></pre>"},{"location":"troubleshooting/#model-not-found","title":"Model Not Found","text":"<p>Solution: Check cache directory:</p> <pre><code>ls ~/.uniface/models/\n</code></pre>"},{"location":"troubleshooting/#performance-issues","title":"Performance Issues","text":""},{"location":"troubleshooting/#slow-on-mac","title":"Slow on Mac","text":"<p>Check: Verify ARM64 Python:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64\n</code></pre>"},{"location":"troubleshooting/#no-gpu-acceleration","title":"No GPU Acceleration","text":"<p>Check: Verify CUDA:</p> <pre><code>import onnxruntime as ort\nprint(ort.get_available_providers())\n# Should include 'CUDAExecutionProvider'\n</code></pre> <p>Solution: Install GPU version:</p> <pre><code>pip install uniface[gpu]\n</code></pre>"},{"location":"troubleshooting/#detection-issues","title":"Detection Issues","text":""},{"location":"troubleshooting/#no-faces-detected","title":"No Faces Detected","text":"<p>Try:</p> <ol> <li> <p>Lower confidence threshold:    <pre><code>detector = RetinaFace(confidence_threshold=0.3)\n</code></pre></p> </li> <li> <p>Check image format (should be BGR):    <pre><code>image = cv2.imread(\"photo.jpg\")  # BGR format\n</code></pre></p> </li> </ol>"},{"location":"troubleshooting/#wrong-bounding-boxes","title":"Wrong Bounding Boxes","text":"<p>Check: Image orientation. Some cameras return rotated images.</p>"},{"location":"troubleshooting/#recognition-issues","title":"Recognition Issues","text":""},{"location":"troubleshooting/#low-similarity-scores","title":"Low Similarity Scores","text":"<p>Try:</p> <ol> <li>Ensure face alignment is working</li> <li>Use higher quality images</li> <li>Check lighting conditions</li> </ol>"},{"location":"troubleshooting/#different-results-each-time","title":"Different Results Each Time","text":"<p>Note: Results should be deterministic. If not, check:</p> <ul> <li>Image preprocessing</li> <li>Model loading</li> </ul>"},{"location":"troubleshooting/#memory-issues","title":"Memory Issues","text":""},{"location":"troubleshooting/#out-of-memory","title":"Out of Memory","text":"<p>Solutions:</p> <ol> <li>Process images in batches</li> <li>Use smaller input size:    <pre><code>detector = RetinaFace(input_size=(320, 320))\n</code></pre></li> <li>Release resources:    <pre><code>del detector\nimport gc\ngc.collect()\n</code></pre></li> </ol>"},{"location":"troubleshooting/#still-having-issues","title":"Still Having Issues?","text":"<ol> <li>Check GitHub Issues</li> <li>Open a new issue with:</li> <li>Python version</li> <li>UniFace version</li> <li>Error message</li> <li>Minimal code to reproduce</li> </ol>"},{"location":"api/reference/","title":"API Reference","text":"<p>Quick reference for all UniFace classes and functions.</p>"},{"location":"api/reference/#detection","title":"Detection","text":""},{"location":"api/reference/#retinaface","title":"RetinaFace","text":"<pre><code>from uniface import RetinaFace\n\ndetector = RetinaFace(\n    model_name=RetinaFaceWeights.MNET_V2,  # Model variant\n    confidence_threshold=0.5,               # Min confidence\n    nms_threshold=0.4,                       # NMS IoU threshold\n    input_size=(640, 640)                    # Input resolution\n)\n\nfaces = detector.detect(image)  # Returns list[Face]\n</code></pre>"},{"location":"api/reference/#scrfd","title":"SCRFD","text":"<pre><code>from uniface import SCRFD\n\ndetector = SCRFD(\n    model_name=SCRFDWeights.SCRFD_10G_KPS,\n    confidence_threshold=0.5,\n    nms_threshold=0.4,\n    input_size=(640, 640)\n)\n</code></pre>"},{"location":"api/reference/#yolov5face","title":"YOLOv5Face","text":"<pre><code>from uniface import YOLOv5Face\n\ndetector = YOLOv5Face(\n    model_name=YOLOv5FaceWeights.YOLOV5S,\n    confidence_threshold=0.6,\n    nms_threshold=0.5\n)\n</code></pre>"},{"location":"api/reference/#recognition","title":"Recognition","text":""},{"location":"api/reference/#arcface","title":"ArcFace","text":"<pre><code>from uniface import ArcFace\n\nrecognizer = ArcFace(model_name=ArcFaceWeights.MNET)\n\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n# Returns: np.ndarray (1, 512)\n</code></pre>"},{"location":"api/reference/#mobileface-sphereface","title":"MobileFace / SphereFace","text":"<pre><code>from uniface import MobileFace, SphereFace\n\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_V2)\nrecognizer = SphereFace(model_name=SphereFaceWeights.SPHERE20)\n</code></pre>"},{"location":"api/reference/#landmarks","title":"Landmarks","text":"<pre><code>from uniface import Landmark106\n\nlandmarker = Landmark106()\nlandmarks = landmarker.get_landmarks(image, bbox)\n# Returns: np.ndarray (106, 2)\n</code></pre>"},{"location":"api/reference/#attributes","title":"Attributes","text":""},{"location":"api/reference/#agegender","title":"AgeGender","text":"<pre><code>from uniface import AgeGender\n\npredictor = AgeGender()\nresult = predictor.predict(image, bbox)\n# Returns: AttributeResult(gender, age, sex)\n</code></pre>"},{"location":"api/reference/#fairface","title":"FairFace","text":"<pre><code>from uniface import FairFace\n\npredictor = FairFace()\nresult = predictor.predict(image, bbox)\n# Returns: AttributeResult(gender, age_group, race, sex)\n</code></pre>"},{"location":"api/reference/#gaze","title":"Gaze","text":"<pre><code>from uniface import MobileGaze\n\ngaze = MobileGaze(model_name=GazeWeights.RESNET34)\nresult = gaze.estimate(face_crop)\n# Returns: GazeResult(pitch, yaw) in radians\n</code></pre>"},{"location":"api/reference/#parsing","title":"Parsing","text":"<pre><code>from uniface.parsing import BiSeNet\n\nparser = BiSeNet(model_name=ParsingWeights.RESNET18)\nmask = parser.parse(face_image)\n# Returns: np.ndarray (H, W) with values 0-18\n</code></pre>"},{"location":"api/reference/#anti-spoofing","title":"Anti-Spoofing","text":"<pre><code>from uniface.spoofing import MiniFASNet\n\nspoofer = MiniFASNet(model_name=MiniFASNetWeights.V2)\nresult = spoofer.predict(image, bbox)\n# Returns: SpoofingResult(is_real, confidence)\n</code></pre>"},{"location":"api/reference/#privacy","title":"Privacy","text":"<pre><code>from uniface.privacy import BlurFace, anonymize_faces\n\n# One-liner\nanonymized = anonymize_faces(image, method='pixelate')\n\n# Manual control\nblurrer = BlurFace(method='gaussian', blur_strength=3.0)\nanonymized = blurrer.anonymize(image, faces)\n</code></pre>"},{"location":"api/reference/#types","title":"Types","text":""},{"location":"api/reference/#face","title":"Face","text":"<pre><code>@dataclass\nclass Face:\n    bbox: np.ndarray        # [x1, y1, x2, y2]\n    confidence: float       # 0.0 to 1.0\n    landmarks: np.ndarray   # (5, 2)\n    embedding: np.ndarray | None = None\n    gender: int | None = None\n    age: int | None = None\n    age_group: str | None = None\n    race: str | None = None\n</code></pre>"},{"location":"api/reference/#result-types","title":"Result Types","text":"<pre><code>GazeResult(pitch: float, yaw: float)\nSpoofingResult(is_real: bool, confidence: float)\nAttributeResult(gender: int, age: int, age_group: str, race: str)\nEmotionResult(emotion: str, confidence: float)\n</code></pre>"},{"location":"api/reference/#utilities","title":"Utilities","text":"<pre><code>from uniface import (\n    compute_similarity,      # Compare embeddings\n    face_alignment,          # Align face for recognition\n    draw_detections,         # Visualize detections\n    vis_parsing_maps,        # Visualize parsing\n    verify_model_weights,    # Download/verify models\n)\n</code></pre>"},{"location":"concepts/coordinate-systems/","title":"Coordinate Systems","text":"<p>This page explains the coordinate formats used in UniFace.</p>"},{"location":"concepts/coordinate-systems/#image-coordinates","title":"Image Coordinates","text":"<p>All coordinates use pixel-based, top-left origin:</p> <pre><code>(0, 0) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba x (width)\n   \u2502\n   \u2502    Image\n   \u2502\n   \u25bc\n   y (height)\n</code></pre>"},{"location":"concepts/coordinate-systems/#bounding-box-format","title":"Bounding Box Format","text":"<p>Bounding boxes use <code>[x1, y1, x2, y2]</code> format (top-left and bottom-right corners):</p> <pre><code>(x1, y1) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502                     \u2502\n    \u2502      Face           \u2502\n    \u2502                     \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 (x2, y2)\n</code></pre>"},{"location":"concepts/coordinate-systems/#accessing-coordinates","title":"Accessing Coordinates","text":"<pre><code>face = faces[0]\n\n# Direct access\nx1, y1, x2, y2 = face.bbox\n\n# As properties\nbbox_xyxy = face.bbox_xyxy  # [x1, y1, x2, y2]\nbbox_xywh = face.bbox_xywh  # [x1, y1, width, height]\n</code></pre>"},{"location":"concepts/coordinate-systems/#conversion","title":"Conversion","text":"<pre><code>import numpy as np\n\n# xyxy \u2192 xywh\ndef xyxy_to_xywh(bbox):\n    x1, y1, x2, y2 = bbox\n    return np.array([x1, y1, x2 - x1, y2 - y1])\n\n# xywh \u2192 xyxy\ndef xywh_to_xyxy(bbox):\n    x, y, w, h = bbox\n    return np.array([x, y, x + w, y + h])\n</code></pre>"},{"location":"concepts/coordinate-systems/#landmarks","title":"Landmarks","text":""},{"location":"concepts/coordinate-systems/#5-point-landmarks-detection","title":"5-Point Landmarks (Detection)","text":"<p>Returned by all detection models:</p> <pre><code>landmarks = face.landmarks  # Shape: (5, 2)\n</code></pre> Index Point 0 Left Eye 1 Right Eye 2 Nose Tip 3 Left Mouth Corner 4 Right Mouth Corner <pre><code>      0 \u25cf           \u25cf 1\n\n            \u25cf 2\n\n        3 \u25cf     \u25cf 4\n</code></pre>"},{"location":"concepts/coordinate-systems/#106-point-landmarks","title":"106-Point Landmarks","text":"<p>Returned by <code>Landmark106</code>:</p> <pre><code>from uniface import Landmark106\n\nlandmarker = Landmark106()\nlandmarks = landmarker.get_landmarks(image, face.bbox)\n# Shape: (106, 2)\n</code></pre> <p>Landmark Groups:</p> Range Group Points 0-32 Face Contour 33 33-50 Eyebrows 18 51-62 Nose 12 63-86 Eyes 24 87-105 Mouth 19"},{"location":"concepts/coordinate-systems/#face-crop","title":"Face Crop","text":"<p>To crop a face from an image:</p> <pre><code>def crop_face(image, bbox, margin=0):\n    \"\"\"Crop face with optional margin.\"\"\"\n    h, w = image.shape[:2]\n    x1, y1, x2, y2 = map(int, bbox)\n\n    # Add margin\n    if margin &gt; 0:\n        bw, bh = x2 - x1, y2 - y1\n        x1 = max(0, x1 - int(bw * margin))\n        y1 = max(0, y1 - int(bh * margin))\n        x2 = min(w, x2 + int(bw * margin))\n        y2 = min(h, y2 + int(bh * margin))\n\n    return image[y1:y2, x1:x2]\n\n# Usage\nface_crop = crop_face(image, face.bbox, margin=0.1)\n</code></pre>"},{"location":"concepts/coordinate-systems/#gaze-angles","title":"Gaze Angles","text":"<p>Gaze estimation returns pitch and yaw in radians:</p> <pre><code>result = gaze_estimator.estimate(face_crop)\n\n# Angles in radians\npitch = result.pitch  # Vertical: + = up, - = down\nyaw = result.yaw      # Horizontal: + = right, - = left\n\n# Convert to degrees\nimport numpy as np\npitch_deg = np.degrees(pitch)\nyaw_deg = np.degrees(yaw)\n</code></pre> <p>Angle Reference:</p> <pre><code>          pitch = +90\u00b0 (up)\n               \u2502\n               \u2502\nyaw = -90\u00b0 \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500 yaw = +90\u00b0\n(left)         \u2502      (right)\n               \u2502\n          pitch = -90\u00b0 (down)\n</code></pre>"},{"location":"concepts/coordinate-systems/#face-alignment","title":"Face Alignment","text":"<p>Face alignment uses 5-point landmarks to normalize face orientation:</p> <pre><code>from uniface import face_alignment\n\n# Align face to standard template\naligned_face = face_alignment(image, face.landmarks)\n# Output: 112x112 aligned face image\n</code></pre> <p>The alignment transforms faces to a canonical pose for better recognition accuracy.</p>"},{"location":"concepts/coordinate-systems/#next-steps","title":"Next Steps","text":"<ul> <li>Inputs &amp; Outputs - Data types reference</li> <li>Recognition Module - Face recognition details</li> </ul>"},{"location":"concepts/execution-providers/","title":"Execution Providers","text":"<p>UniFace uses ONNX Runtime for model inference, which supports multiple hardware acceleration backends.</p>"},{"location":"concepts/execution-providers/#automatic-provider-selection","title":"Automatic Provider Selection","text":"<p>UniFace automatically selects the optimal execution provider based on available hardware:</p> <pre><code>from uniface import RetinaFace\n\n# Automatically uses best available provider\ndetector = RetinaFace()\n</code></pre> <p>Priority order:</p> <ol> <li>CUDAExecutionProvider - NVIDIA GPU</li> <li>CoreMLExecutionProvider - Apple Silicon</li> <li>CPUExecutionProvider - Fallback</li> </ol>"},{"location":"concepts/execution-providers/#check-available-providers","title":"Check Available Providers","text":"<pre><code>import onnxruntime as ort\n\nproviders = ort.get_available_providers()\nprint(\"Available providers:\", providers)\n</code></pre> <p>Example outputs:</p> macOS (Apple Silicon)Linux (NVIDIA GPU)Windows (CPU) <pre><code>['CoreMLExecutionProvider', 'CPUExecutionProvider']\n</code></pre> <pre><code>['CUDAExecutionProvider', 'CPUExecutionProvider']\n</code></pre> <pre><code>['CPUExecutionProvider']\n</code></pre>"},{"location":"concepts/execution-providers/#platform-specific-setup","title":"Platform-Specific Setup","text":""},{"location":"concepts/execution-providers/#apple-silicon-m1m2m3m4","title":"Apple Silicon (M1/M2/M3/M4)","text":"<p>No additional setup required. ARM64 optimizations are built into <code>onnxruntime</code>:</p> <pre><code>pip install uniface\n</code></pre> <p>Verify ARM64:</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64\n</code></pre> <p>Performance</p> <p>Apple Silicon Macs use CoreML acceleration automatically, providing excellent performance for face analysis tasks.</p>"},{"location":"concepts/execution-providers/#nvidia-gpu-cuda","title":"NVIDIA GPU (CUDA)","text":"<p>Install with GPU support:</p> <pre><code>pip install uniface[gpu]\n</code></pre> <p>Requirements:</p> <ul> <li>CUDA 11.x or 12.x</li> <li>cuDNN 8.x</li> <li>Compatible NVIDIA driver</li> </ul> <p>Verify CUDA:</p> <pre><code>import onnxruntime as ort\n\nif 'CUDAExecutionProvider' in ort.get_available_providers():\n    print(\"CUDA is available!\")\nelse:\n    print(\"CUDA not available, using CPU\")\n</code></pre>"},{"location":"concepts/execution-providers/#cpu-fallback","title":"CPU Fallback","text":"<p>CPU execution is always available:</p> <pre><code>pip install uniface\n</code></pre> <p>Works on all platforms without additional configuration.</p>"},{"location":"concepts/execution-providers/#internal-api","title":"Internal API","text":"<p>For advanced use cases, you can access the provider utilities:</p> <pre><code>from uniface.onnx_utils import get_available_providers, create_onnx_session\n\n# Check available providers\nproviders = get_available_providers()\nprint(f\"Available: {providers}\")\n\n# Models use create_onnx_session() internally\n# which auto-selects the best provider\n</code></pre>"},{"location":"concepts/execution-providers/#performance-tips","title":"Performance Tips","text":""},{"location":"concepts/execution-providers/#1-use-gpu-when-available","title":"1. Use GPU When Available","text":"<p>For batch processing or real-time applications, GPU acceleration provides significant speedups:</p> <pre><code>pip install uniface[gpu]\n</code></pre>"},{"location":"concepts/execution-providers/#2-optimize-input-size","title":"2. Optimize Input Size","text":"<p>Smaller input sizes are faster but may reduce accuracy:</p> <pre><code>from uniface import RetinaFace\n\n# Faster, lower accuracy\ndetector = RetinaFace(input_size=(320, 320))\n\n# Balanced (default)\ndetector = RetinaFace(input_size=(640, 640))\n</code></pre>"},{"location":"concepts/execution-providers/#3-batch-processing","title":"3. Batch Processing","text":"<p>Process multiple images to maximize GPU utilization:</p> <pre><code># Process images in batch (GPU-efficient)\nfor image_path in image_paths:\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n    # ...\n</code></pre>"},{"location":"concepts/execution-providers/#troubleshooting","title":"Troubleshooting","text":""},{"location":"concepts/execution-providers/#cuda-not-detected","title":"CUDA Not Detected","text":"<ol> <li> <p>Verify CUDA installation:    <pre><code>nvidia-smi\n</code></pre></p> </li> <li> <p>Check CUDA version compatibility with ONNX Runtime</p> </li> <li> <p>Reinstall with GPU support:    <pre><code>pip uninstall onnxruntime onnxruntime-gpu\npip install uniface[gpu]\n</code></pre></p> </li> </ol>"},{"location":"concepts/execution-providers/#slow-performance-on-mac","title":"Slow Performance on Mac","text":"<p>Verify you're using ARM64 Python (not Rosetta):</p> <pre><code>python -c \"import platform; print(platform.machine())\"\n# Should show: arm64 (not x86_64)\n</code></pre>"},{"location":"concepts/execution-providers/#next-steps","title":"Next Steps","text":"<ul> <li>Model Cache &amp; Offline - Model management</li> <li>Thresholds &amp; Calibration - Tuning parameters</li> </ul>"},{"location":"concepts/inputs-outputs/","title":"Inputs &amp; Outputs","text":"<p>This page describes the data types used throughout UniFace.</p>"},{"location":"concepts/inputs-outputs/#input-images","title":"Input: Images","text":"<p>All models accept NumPy arrays in BGR format (OpenCV default):</p> <pre><code>import cv2\n\n# Load image (BGR format)\nimage = cv2.imread(\"photo.jpg\")\nprint(f\"Shape: {image.shape}\")  # (H, W, 3)\nprint(f\"Dtype: {image.dtype}\")  # uint8\n</code></pre> <p>Color Format</p> <p>UniFace expects BGR format (OpenCV default). If using PIL or other libraries, convert first:</p> <pre><code>from PIL import Image\nimport numpy as np\n\npil_image = Image.open(\"photo.jpg\")\nbgr_image = np.array(pil_image)[:, :, ::-1]  # RGB \u2192 BGR\n</code></pre>"},{"location":"concepts/inputs-outputs/#output-face-dataclass","title":"Output: Face Dataclass","text":"<p>Detection returns a list of <code>Face</code> objects:</p> <pre><code>from dataclasses import dataclass\nimport numpy as np\n\n@dataclass\nclass Face:\n    # Required (from detection)\n    bbox: np.ndarray        # [x1, y1, x2, y2]\n    confidence: float       # 0.0 to 1.0\n    landmarks: np.ndarray   # (5, 2) or (106, 2)\n\n    # Optional (enriched by analyzers)\n    embedding: np.ndarray | None = None\n    gender: int | None = None           # 0=Female, 1=Male\n    age: int | None = None              # Years\n    age_group: str | None = None        # \"20-29\", etc.\n    race: str | None = None             # \"East Asian\", etc.\n    emotion: str | None = None          # \"Happy\", etc.\n    emotion_confidence: float | None = None\n</code></pre>"},{"location":"concepts/inputs-outputs/#properties","title":"Properties","text":"<pre><code>face = faces[0]\n\n# Bounding box formats\nface.bbox_xyxy  # [x1, y1, x2, y2] - same as bbox\nface.bbox_xywh  # [x1, y1, width, height]\n\n# Gender as string\nface.sex  # \"Female\" or \"Male\" (None if not predicted)\n</code></pre>"},{"location":"concepts/inputs-outputs/#methods","title":"Methods","text":"<pre><code># Compute similarity with another face\nsimilarity = face1.compute_similarity(face2)\n\n# Convert to dictionary\nface_dict = face.to_dict()\n</code></pre>"},{"location":"concepts/inputs-outputs/#result-types","title":"Result Types","text":""},{"location":"concepts/inputs-outputs/#gazeresult","title":"GazeResult","text":"<pre><code>from dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass GazeResult:\n    pitch: float  # Vertical angle (radians), + = up\n    yaw: float    # Horizontal angle (radians), + = right\n</code></pre> <p>Usage:</p> <pre><code>import numpy as np\n\nresult = gaze_estimator.estimate(face_crop)\nprint(f\"Pitch: {np.degrees(result.pitch):.1f}\u00b0\")\nprint(f\"Yaw: {np.degrees(result.yaw):.1f}\u00b0\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#spoofingresult","title":"SpoofingResult","text":"<pre><code>@dataclass(frozen=True)\nclass SpoofingResult:\n    is_real: bool      # True = real, False = fake\n    confidence: float  # 0.0 to 1.0\n</code></pre> <p>Usage:</p> <pre><code>result = spoofer.predict(image, face.bbox)\nlabel = \"Real\" if result.is_real else \"Fake\"\nprint(f\"{label}: {result.confidence:.1%}\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#attributeresult","title":"AttributeResult","text":"<pre><code>@dataclass(frozen=True)\nclass AttributeResult:\n    gender: int              # 0=Female, 1=Male\n    age: int | None          # Years (AgeGender model)\n    age_group: str | None    # \"20-29\" (FairFace model)\n    race: str | None         # Race label (FairFace model)\n\n    @property\n    def sex(self) -&gt; str:\n        return \"Female\" if self.gender == 0 else \"Male\"\n</code></pre> <p>Usage:</p> <pre><code># AgeGender model\nresult = age_gender.predict(image, face.bbox)\nprint(f\"{result.sex}, {result.age} years old\")\n\n# FairFace model\nresult = fairface.predict(image, face.bbox)\nprint(f\"{result.sex}, {result.age_group}, {result.race}\")\n</code></pre>"},{"location":"concepts/inputs-outputs/#emotionresult","title":"EmotionResult","text":"<pre><code>@dataclass(frozen=True)\nclass EmotionResult:\n    emotion: str       # \"Happy\", \"Sad\", etc.\n    confidence: float  # 0.0 to 1.0\n</code></pre>"},{"location":"concepts/inputs-outputs/#embeddings","title":"Embeddings","text":"<p>Face recognition models return normalized 512-dimensional embeddings:</p> <pre><code>embedding = recognizer.get_normalized_embedding(image, landmarks)\nprint(f\"Shape: {embedding.shape}\")  # (1, 512)\nprint(f\"Norm: {np.linalg.norm(embedding):.4f}\")  # ~1.0\n</code></pre>"},{"location":"concepts/inputs-outputs/#similarity-computation","title":"Similarity Computation","text":"<pre><code>from uniface import compute_similarity\n\nsimilarity = compute_similarity(embedding1, embedding2)\n# Returns: float between -1 and 1 (cosine similarity)\n</code></pre>"},{"location":"concepts/inputs-outputs/#parsing-masks","title":"Parsing Masks","text":"<p>Face parsing returns a segmentation mask:</p> <pre><code>mask = parser.parse(face_image)\nprint(f\"Shape: {mask.shape}\")  # (H, W)\nprint(f\"Classes: {np.unique(mask)}\")  # [0, 1, 2, ...]\n</code></pre> <p>19 Classes:</p> ID Class ID Class 0 Background 10 Ear Ring 1 Skin 11 Nose 2 Left Eyebrow 12 Mouth 3 Right Eyebrow 13 Upper Lip 4 Left Eye 14 Lower Lip 5 Right Eye 15 Neck 6 Eye Glasses 16 Neck Lace 7 Left Ear 17 Cloth 8 Right Ear 18 Hair 9 Hat"},{"location":"concepts/inputs-outputs/#next-steps","title":"Next Steps","text":"<ul> <li>Coordinate Systems - Bbox and landmark formats</li> <li>Thresholds &amp; Calibration - Tuning confidence thresholds</li> </ul>"},{"location":"concepts/model-cache-offline/","title":"Model Cache &amp; Offline Use","text":"<p>UniFace automatically downloads and caches models. This page explains how model management works.</p>"},{"location":"concepts/model-cache-offline/#automatic-download","title":"Automatic Download","text":"<p>Models are downloaded on first use:</p> <pre><code>from uniface import RetinaFace\n\n# First run: downloads model to cache\ndetector = RetinaFace()  # ~3.5 MB download\n\n# Subsequent runs: loads from cache\ndetector = RetinaFace()  # Instant\n</code></pre>"},{"location":"concepts/model-cache-offline/#cache-location","title":"Cache Location","text":"<p>Default cache directory:</p> <pre><code>~/.uniface/models/\n</code></pre> <p>Example structure:</p> <pre><code>~/.uniface/models/\n\u251c\u2500\u2500 retinaface_mv2.onnx\n\u251c\u2500\u2500 w600k_mbf.onnx\n\u251c\u2500\u2500 2d106det.onnx\n\u251c\u2500\u2500 gaze_resnet34.onnx\n\u251c\u2500\u2500 parsing_resnet18.onnx\n\u2514\u2500\u2500 ...\n</code></pre>"},{"location":"concepts/model-cache-offline/#custom-cache-directory","title":"Custom Cache Directory","text":"<p>Specify a custom cache location:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\n# Download to custom directory\nmodel_path = verify_model_weights(\n    RetinaFaceWeights.MNET_V2,\n    root='./my_models'\n)\nprint(f\"Model at: {model_path}\")\n</code></pre>"},{"location":"concepts/model-cache-offline/#pre-download-models","title":"Pre-Download Models","text":"<p>Download models before deployment:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import (\n    RetinaFaceWeights,\n    ArcFaceWeights,\n    AgeGenderWeights,\n)\n\n# Download all needed models\nmodels = [\n    RetinaFaceWeights.MNET_V2,\n    ArcFaceWeights.MNET,\n    AgeGenderWeights.DEFAULT,\n]\n\nfor model in models:\n    path = verify_model_weights(model)\n    print(f\"Downloaded: {path}\")\n</code></pre> <p>Or use the CLI tool:</p> <pre><code>python tools/download_model.py\n</code></pre>"},{"location":"concepts/model-cache-offline/#offline-use","title":"Offline Use","text":"<p>For air-gapped or offline environments:</p>"},{"location":"concepts/model-cache-offline/#1-pre-download-models","title":"1. Pre-download models","text":"<p>On a connected machine:</p> <pre><code>from uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\npath = verify_model_weights(RetinaFaceWeights.MNET_V2)\nprint(f\"Copy from: {path}\")\n</code></pre>"},{"location":"concepts/model-cache-offline/#2-copy-to-target-machine","title":"2. Copy to target machine","text":"<pre><code># Copy the entire cache directory\nscp -r ~/.uniface/models/ user@offline-machine:~/.uniface/models/\n</code></pre>"},{"location":"concepts/model-cache-offline/#3-use-normally","title":"3. Use normally","text":"<pre><code># Models load from local cache\nfrom uniface import RetinaFace\ndetector = RetinaFace()  # No network required\n</code></pre>"},{"location":"concepts/model-cache-offline/#model-verification","title":"Model Verification","text":"<p>Models are verified with SHA-256 checksums:</p> <pre><code>from uniface.constants import MODEL_SHA256, RetinaFaceWeights\n\n# Check expected checksum\nexpected = MODEL_SHA256[RetinaFaceWeights.MNET_V2]\nprint(f\"Expected SHA256: {expected}\")\n</code></pre> <p>If a model fails verification, it's re-downloaded automatically.</p>"},{"location":"concepts/model-cache-offline/#available-models","title":"Available Models","text":""},{"location":"concepts/model-cache-offline/#detection-models","title":"Detection Models","text":"Model Size Download RetinaFace MNET_025 1.7 MB \u2705 RetinaFace MNET_V2 3.5 MB \u2705 RetinaFace RESNET34 56 MB \u2705 SCRFD 500M 2.5 MB \u2705 SCRFD 10G 17 MB \u2705 YOLOv5n-Face 11 MB \u2705 YOLOv5s-Face 28 MB \u2705 YOLOv5m-Face 82 MB \u2705"},{"location":"concepts/model-cache-offline/#recognition-models","title":"Recognition Models","text":"Model Size Download ArcFace MNET 8 MB \u2705 ArcFace RESNET 166 MB \u2705 MobileFace MNET_V2 4 MB \u2705 SphereFace SPHERE20 50 MB \u2705"},{"location":"concepts/model-cache-offline/#other-models","title":"Other Models","text":"Model Size Download Landmark106 14 MB \u2705 AgeGender 8 MB \u2705 FairFace 44 MB \u2705 Gaze ResNet34 82 MB \u2705 BiSeNet ResNet18 51 MB \u2705 MiniFASNet V2 1.2 MB \u2705"},{"location":"concepts/model-cache-offline/#clear-cache","title":"Clear Cache","text":"<p>Remove cached models:</p> <pre><code># Remove all cached models\nrm -rf ~/.uniface/models/\n\n# Remove specific model\nrm ~/.uniface/models/retinaface_mv2.onnx\n</code></pre> <p>Models will be re-downloaded on next use.</p>"},{"location":"concepts/model-cache-offline/#environment-variables","title":"Environment Variables","text":"<p>Set custom cache location via environment variable:</p> <pre><code>export UNIFACE_CACHE_DIR=/path/to/custom/cache\n</code></pre> <pre><code>import os\nos.environ['UNIFACE_CACHE_DIR'] = '/path/to/custom/cache'\n\nfrom uniface import RetinaFace\ndetector = RetinaFace()  # Uses custom cache\n</code></pre>"},{"location":"concepts/model-cache-offline/#next-steps","title":"Next Steps","text":"<ul> <li>Thresholds &amp; Calibration - Tune model parameters</li> <li>Detection Module - Detection model details</li> </ul>"},{"location":"concepts/overview/","title":"Overview","text":"<p>UniFace is designed as a modular, production-ready face analysis library. This page explains the architecture and design principles.</p>"},{"location":"concepts/overview/#architecture","title":"Architecture","text":"<p>UniFace follows a modular architecture where each face analysis task is handled by a dedicated module:</p> <pre><code>graph TB\n    subgraph Input\n        IMG[Image/Frame]\n    end\n\n    subgraph Detection\n        DET[RetinaFace / SCRFD / YOLOv5Face]\n    end\n\n    subgraph Analysis\n        REC[Recognition]\n        LMK[Landmarks]\n        ATTR[Attributes]\n        GAZE[Gaze]\n        PARSE[Parsing]\n        SPOOF[Anti-Spoofing]\n        PRIV[Privacy]\n    end\n\n    subgraph Output\n        FACE[Face Objects]\n    end\n\n    IMG --&gt; DET\n    DET --&gt; REC\n    DET --&gt; LMK\n    DET --&gt; ATTR\n    DET --&gt; GAZE\n    DET --&gt; PARSE\n    DET --&gt; SPOOF\n    DET --&gt; PRIV\n    REC --&gt; FACE\n    LMK --&gt; FACE\n    ATTR --&gt; FACE</code></pre>"},{"location":"concepts/overview/#design-principles","title":"Design Principles","text":""},{"location":"concepts/overview/#1-onnx-first","title":"1. ONNX-First","text":"<p>All models use ONNX Runtime for inference:</p> <ul> <li>Cross-platform: Same models work on macOS, Linux, Windows</li> <li>Hardware acceleration: Automatic selection of optimal provider</li> <li>Production-ready: No Python-only dependencies for inference</li> </ul>"},{"location":"concepts/overview/#2-minimal-dependencies","title":"2. Minimal Dependencies","text":"<p>Core dependencies are kept minimal:</p> <pre><code>numpy         # Array operations\nopencv-python # Image processing\nonnxruntime   # Model inference\nrequests      # Model download\ntqdm          # Progress bars\n</code></pre>"},{"location":"concepts/overview/#3-simple-api","title":"3. Simple API","text":"<p>Factory functions and direct instantiation:</p> <pre><code># Factory function\ndetector = create_detector('retinaface')\n\n# Direct instantiation (recommended)\nfrom uniface import RetinaFace\ndetector = RetinaFace()\n</code></pre>"},{"location":"concepts/overview/#4-type-safety","title":"4. Type Safety","text":"<p>Full type hints throughout:</p> <pre><code>def detect(self, image: np.ndarray) -&gt; list[Face]:\n    ...\n</code></pre>"},{"location":"concepts/overview/#module-structure","title":"Module Structure","text":"<pre><code>uniface/\n\u251c\u2500\u2500 detection/      # Face detection (RetinaFace, SCRFD, YOLOv5Face)\n\u251c\u2500\u2500 recognition/    # Face recognition (ArcFace, MobileFace, SphereFace)\n\u251c\u2500\u2500 landmark/       # 106-point landmarks\n\u251c\u2500\u2500 attribute/      # Age, gender, emotion, race\n\u251c\u2500\u2500 parsing/        # Face semantic segmentation\n\u251c\u2500\u2500 gaze/           # Gaze estimation\n\u251c\u2500\u2500 spoofing/       # Anti-spoofing\n\u251c\u2500\u2500 privacy/        # Face anonymization\n\u251c\u2500\u2500 types.py        # Dataclasses (Face, GazeResult, etc.)\n\u251c\u2500\u2500 constants.py    # Model weights and URLs\n\u251c\u2500\u2500 model_store.py  # Model download and caching\n\u251c\u2500\u2500 onnx_utils.py   # ONNX Runtime utilities\n\u2514\u2500\u2500 visualization.py # Drawing utilities\n</code></pre>"},{"location":"concepts/overview/#workflow","title":"Workflow","text":"<p>A typical face analysis workflow:</p> <pre><code>import cv2\nfrom uniface import RetinaFace, ArcFace, AgeGender\n\n# 1. Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\nage_gender = AgeGender()\n\n# 2. Load image\nimage = cv2.imread(\"photo.jpg\")\n\n# 3. Detect faces\nfaces = detector.detect(image)\n\n# 4. Analyze each face\nfor face in faces:\n    # Recognition embedding\n    embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n    # Attributes\n    attrs = age_gender.predict(image, face.bbox)\n\n    print(f\"Face: {attrs.sex}, {attrs.age} years\")\n</code></pre>"},{"location":"concepts/overview/#faceanalyzer","title":"FaceAnalyzer","text":"<p>For convenience, <code>FaceAnalyzer</code> combines multiple modules:</p> <pre><code>from uniface import FaceAnalyzer\n\nanalyzer = FaceAnalyzer(\n    detect=True,\n    recognize=True,\n    attributes=True\n)\n\nfaces = analyzer.analyze(image)\nfor face in faces:\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n    print(f\"Embedding: {face.embedding.shape}\")\n</code></pre>"},{"location":"concepts/overview/#model-lifecycle","title":"Model Lifecycle","text":"<ol> <li>First use: Model is downloaded from GitHub releases</li> <li>Cached: Stored in <code>~/.uniface/models/</code></li> <li>Verified: SHA-256 checksum validation</li> <li>Loaded: ONNX Runtime session created</li> <li>Inference: Hardware-accelerated execution</li> </ol> <pre><code># Models auto-download on first use\ndetector = RetinaFace()  # Downloads if not cached\n\n# Or manually pre-download\nfrom uniface.model_store import verify_model_weights\nfrom uniface.constants import RetinaFaceWeights\n\npath = verify_model_weights(RetinaFaceWeights.MNET_V2)\n</code></pre>"},{"location":"concepts/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Inputs &amp; Outputs - Understand data types</li> <li>Execution Providers - Hardware acceleration</li> <li>Detection Module - Start with face detection</li> </ul>"},{"location":"concepts/thresholds-calibration/","title":"Thresholds &amp; Calibration","text":"<p>This page explains how to tune detection and recognition thresholds for your use case.</p>"},{"location":"concepts/thresholds-calibration/#detection-thresholds","title":"Detection Thresholds","text":""},{"location":"concepts/thresholds-calibration/#confidence-threshold","title":"Confidence Threshold","text":"<p>Controls minimum confidence for face detection:</p> <pre><code>from uniface import RetinaFace\n\n# Default (balanced)\ndetector = RetinaFace(confidence_threshold=0.5)\n\n# High precision (fewer false positives)\ndetector = RetinaFace(confidence_threshold=0.8)\n\n# High recall (catch more faces)\ndetector = RetinaFace(confidence_threshold=0.3)\n</code></pre> <p>Guidelines:</p> Threshold Use Case 0.3 - 0.4 Maximum recall (research, analysis) 0.5 - 0.6 Balanced (default, general use) 0.7 - 0.9 High precision (production, security)"},{"location":"concepts/thresholds-calibration/#nms-threshold","title":"NMS Threshold","text":"<p>Non-Maximum Suppression removes overlapping detections:</p> <pre><code># Default\ndetector = RetinaFace(nms_threshold=0.4)\n\n# Stricter (fewer overlapping boxes)\ndetector = RetinaFace(nms_threshold=0.3)\n\n# Looser (for crowded scenes)\ndetector = RetinaFace(nms_threshold=0.5)\n</code></pre>"},{"location":"concepts/thresholds-calibration/#input-size","title":"Input Size","text":"<p>Affects detection accuracy and speed:</p> <pre><code># Faster, lower accuracy\ndetector = RetinaFace(input_size=(320, 320))\n\n# Balanced (default)\ndetector = RetinaFace(input_size=(640, 640))\n\n# Higher accuracy, slower\ndetector = RetinaFace(input_size=(1280, 1280))\n</code></pre> <p>Dynamic Size</p> <p>For RetinaFace, enable dynamic input for variable image sizes: <pre><code>detector = RetinaFace(dynamic_size=True)\n</code></pre></p>"},{"location":"concepts/thresholds-calibration/#recognition-thresholds","title":"Recognition Thresholds","text":""},{"location":"concepts/thresholds-calibration/#similarity-threshold","title":"Similarity Threshold","text":"<p>For identity verification (same person check):</p> <pre><code>import numpy as np\nfrom uniface import compute_similarity\n\nsimilarity = compute_similarity(embedding1, embedding2)\n\n# Threshold interpretation\nif similarity &gt; 0.6:\n    print(\"Same person (high confidence)\")\nelif similarity &gt; 0.4:\n    print(\"Uncertain (manual review)\")\nelse:\n    print(\"Different people\")\n</code></pre> <p>Recommended thresholds:</p> Threshold Decision False Accept Rate 0.4 Low security Higher FAR 0.5 Balanced Moderate FAR 0.6 High security Lower FAR 0.7 Very strict Very low FAR"},{"location":"concepts/thresholds-calibration/#calibration-for-your-dataset","title":"Calibration for Your Dataset","text":"<p>Test on your data to find optimal thresholds:</p> <pre><code>import numpy as np\n\ndef calibrate_threshold(same_pairs, diff_pairs, recognizer, detector):\n    \"\"\"Find optimal threshold for your dataset.\"\"\"\n    same_scores = []\n    diff_scores = []\n\n    # Compute similarities for same-person pairs\n    for img1_path, img2_path in same_pairs:\n        img1 = cv2.imread(img1_path)\n        img2 = cv2.imread(img2_path)\n\n        faces1 = detector.detect(img1)\n        faces2 = detector.detect(img2)\n\n        if faces1 and faces2:\n            emb1 = recognizer.get_normalized_embedding(img1, faces1[0].landmarks)\n            emb2 = recognizer.get_normalized_embedding(img2, faces2[0].landmarks)\n            same_scores.append(np.dot(emb1, emb2.T)[0][0])\n\n    # Compute similarities for different-person pairs\n    for img1_path, img2_path in diff_pairs:\n        # ... similar process\n        diff_scores.append(similarity)\n\n    # Find optimal threshold\n    thresholds = np.arange(0.3, 0.8, 0.05)\n    best_threshold = 0.5\n    best_accuracy = 0\n\n    for thresh in thresholds:\n        tp = sum(1 for s in same_scores if s &gt;= thresh)\n        tn = sum(1 for s in diff_scores if s &lt; thresh)\n        accuracy = (tp + tn) / (len(same_scores) + len(diff_scores))\n\n        if accuracy &gt; best_accuracy:\n            best_accuracy = accuracy\n            best_threshold = thresh\n\n    return best_threshold, best_accuracy\n</code></pre>"},{"location":"concepts/thresholds-calibration/#anti-spoofing-thresholds","title":"Anti-Spoofing Thresholds","text":"<p>The MiniFASNet model returns a confidence score:</p> <pre><code>from uniface.spoofing import MiniFASNet\n\nspoofer = MiniFASNet()\nresult = spoofer.predict(image, face.bbox)\n\n# Default threshold (0.5)\nif result.is_real:  # confidence &gt; 0.5\n    print(\"Real face\")\n\n# Custom threshold for high security\nSPOOF_THRESHOLD = 0.7\nif result.confidence &gt; SPOOF_THRESHOLD:\n    print(\"Real face (high confidence)\")\nelse:\n    print(\"Potentially fake\")\n</code></pre>"},{"location":"concepts/thresholds-calibration/#attribute-model-confidence","title":"Attribute Model Confidence","text":""},{"location":"concepts/thresholds-calibration/#emotion","title":"Emotion","text":"<pre><code>result = emotion_predictor.predict(image, landmarks)\n\n# Filter low-confidence predictions\nif result.confidence &gt; 0.6:\n    print(f\"Emotion: {result.emotion}\")\nelse:\n    print(\"Uncertain emotion\")\n</code></pre>"},{"location":"concepts/thresholds-calibration/#visualization-threshold","title":"Visualization Threshold","text":"<p>For drawing detections, filter by confidence:</p> <pre><code>from uniface.visualization import draw_detections\n\n# Only draw high-confidence detections\nbboxes = [f.bbox for f in faces if f.confidence &gt; 0.7]\nscores = [f.confidence for f in faces if f.confidence &gt; 0.7]\nlandmarks = [f.landmarks for f in faces if f.confidence &gt; 0.7]\n\ndraw_detections(\n    image=image,\n    bboxes=bboxes,\n    scores=scores,\n    landmarks=landmarks,\n    vis_threshold=0.6  # Additional visualization filter\n)\n</code></pre>"},{"location":"concepts/thresholds-calibration/#summary","title":"Summary","text":"Parameter Default Range Lower = Higher = <code>confidence_threshold</code> 0.5 0.1-0.9 More detections Fewer false positives <code>nms_threshold</code> 0.4 0.1-0.7 Fewer overlaps More overlapping boxes Similarity threshold 0.6 0.3-0.8 More matches (FAR\u2191) Fewer matches (FRR\u2191) Spoof confidence 0.5 0.3-0.9 More \"real\" Stricter liveness"},{"location":"concepts/thresholds-calibration/#next-steps","title":"Next Steps","text":"<ul> <li>Detection Module - Detection model options</li> <li>Recognition Module - Recognition model options</li> </ul>"},{"location":"modules/attributes/","title":"Attributes","text":"<p>Facial attribute analysis for age, gender, race, and emotion detection.</p>"},{"location":"modules/attributes/#available-models","title":"Available Models","text":"Model Attributes Size Notes AgeGender Age, Gender 8 MB Exact age prediction FairFace Gender, Age Group, Race 44 MB Balanced demographics Emotion 7-8 emotions 2 MB Requires PyTorch"},{"location":"modules/attributes/#agegender","title":"AgeGender","text":"<p>Predicts exact age and binary gender.</p>"},{"location":"modules/attributes/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace, AgeGender\n\ndetector = RetinaFace()\nage_gender = AgeGender()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = age_gender.predict(image, face.bbox)\n    print(f\"Gender: {result.sex}\")  # \"Female\" or \"Male\"\n    print(f\"Age: {result.age} years\")\n</code></pre>"},{"location":"modules/attributes/#output","title":"Output","text":"<pre><code># AttributeResult fields\nresult.gender     # 0=Female, 1=Male\nresult.sex        # \"Female\" or \"Male\" (property)\nresult.age        # int, age in years\nresult.age_group  # None (not provided by this model)\nresult.race       # None (not provided by this model)\n</code></pre>"},{"location":"modules/attributes/#fairface","title":"FairFace","text":"<p>Predicts gender, age group, and race with balanced demographics.</p>"},{"location":"modules/attributes/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace, FairFace\n\ndetector = RetinaFace()\nfairface = FairFace()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = fairface.predict(image, face.bbox)\n    print(f\"Gender: {result.sex}\")\n    print(f\"Age Group: {result.age_group}\")\n    print(f\"Race: {result.race}\")\n</code></pre>"},{"location":"modules/attributes/#output_1","title":"Output","text":"<pre><code># AttributeResult fields\nresult.gender     # 0=Female, 1=Male\nresult.sex        # \"Female\" or \"Male\"\nresult.age        # None (not provided by this model)\nresult.age_group  # \"20-29\", \"30-39\", etc.\nresult.race       # Race/ethnicity label\n</code></pre>"},{"location":"modules/attributes/#race-categories","title":"Race Categories","text":"Label White Black Latino Hispanic East Asian Southeast Asian Indian Middle Eastern"},{"location":"modules/attributes/#age-groups","title":"Age Groups","text":"Group 0-2 3-9 10-19 20-29 30-39 40-49 50-59 60-69 70+"},{"location":"modules/attributes/#emotion","title":"Emotion","text":"<p>Predicts facial emotions. Requires PyTorch.</p> <p>Optional Dependency</p> <p>Emotion detection requires PyTorch. Install with: <pre><code>pip install torch\n</code></pre></p>"},{"location":"modules/attributes/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace\nfrom uniface.attribute import Emotion\nfrom uniface.constants import DDAMFNWeights\n\ndetector = RetinaFace()\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET7)\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = emotion.predict(image, face.landmarks)\n    print(f\"Emotion: {result.emotion}\")\n    print(f\"Confidence: {result.confidence:.2%}\")\n</code></pre>"},{"location":"modules/attributes/#emotion-classes","title":"Emotion Classes","text":"7-Class (AFFECNET7)8-Class (AFFECNET8) Label Neutral Happy Sad Surprise Fear Disgust Anger Label Neutral Happy Sad Surprise Fear Disgust Anger Contempt"},{"location":"modules/attributes/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.attribute import Emotion\nfrom uniface.constants import DDAMFNWeights\n\n# 7-class emotion\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET7)\n\n# 8-class emotion\nemotion = Emotion(model_name=DDAMFNWeights.AFFECNET8)\n</code></pre>"},{"location":"modules/attributes/#combining-models","title":"Combining Models","text":""},{"location":"modules/attributes/#full-attribute-analysis","title":"Full Attribute Analysis","text":"<pre><code>from uniface import RetinaFace, AgeGender, FairFace\n\ndetector = RetinaFace()\nage_gender = AgeGender()\nfairface = FairFace()\n\nfaces = detector.detect(image)\n\nfor face in faces:\n    # Get exact age from AgeGender\n    ag_result = age_gender.predict(image, face.bbox)\n\n    # Get race from FairFace\n    ff_result = fairface.predict(image, face.bbox)\n\n    print(f\"Gender: {ag_result.sex}\")\n    print(f\"Exact Age: {ag_result.age}\")\n    print(f\"Age Group: {ff_result.age_group}\")\n    print(f\"Race: {ff_result.race}\")\n</code></pre>"},{"location":"modules/attributes/#using-faceanalyzer","title":"Using FaceAnalyzer","text":"<pre><code>from uniface import FaceAnalyzer\n\nanalyzer = FaceAnalyzer(\n    detect=True,\n    recognize=False,\n    attributes=True  # Uses AgeGender\n)\n\nfaces = analyzer.analyze(image)\n\nfor face in faces:\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n</code></pre>"},{"location":"modules/attributes/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef draw_attributes(image, face, result):\n    \"\"\"Draw attributes on image.\"\"\"\n    x1, y1, x2, y2 = map(int, face.bbox)\n\n    # Draw bounding box\n    cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    # Build label\n    label = f\"{result.sex}\"\n    if result.age:\n        label += f\", {result.age}y\"\n    if result.age_group:\n        label += f\", {result.age_group}\"\n    if result.race:\n        label += f\", {result.race}\"\n\n    # Draw label\n    cv2.putText(\n        image, label, (x1, y1 - 10),\n        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2\n    )\n\n    return image\n\n# Usage\nfor face in faces:\n    result = age_gender.predict(image, face.bbox)\n    image = draw_attributes(image, face, result)\n\ncv2.imwrite(\"attributes.jpg\", image)\n</code></pre>"},{"location":"modules/attributes/#accuracy-notes","title":"Accuracy Notes","text":"<p>Model Limitations</p> <ul> <li>AgeGender: Trained on CelebA; accuracy varies by demographic</li> <li>FairFace: Trained for balanced demographics; better cross-racial accuracy</li> <li>Emotion: Accuracy depends on facial expression clarity</li> </ul> <p>Always test on your specific use case and consider cultural context.</p>"},{"location":"modules/attributes/#next-steps","title":"Next Steps","text":"<ul> <li>Parsing - Face semantic segmentation</li> <li>Gaze - Gaze estimation</li> <li>Image Pipeline Recipe - Complete workflow</li> </ul>"},{"location":"modules/detection/","title":"Detection","text":"<p>Face detection is the first step in any face analysis pipeline. UniFace provides three detection models.</p>"},{"location":"modules/detection/#available-models","title":"Available Models","text":"Model Backbone Size WIDER FACE (Easy/Medium/Hard) Best For RetinaFace MobileNet V2 3.5 MB 91.7% / 91.0% / 86.6% Balanced (recommended) SCRFD SCRFD-10G 17 MB 95.2% / 93.9% / 83.1% High accuracy YOLOv5-Face YOLOv5s 28 MB 94.3% / 92.6% / 83.2% Real-time"},{"location":"modules/detection/#retinaface","title":"RetinaFace","text":"<p>The recommended detector for most use cases.</p>"},{"location":"modules/detection/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace\n\ndetector = RetinaFace()\nfaces = detector.detect(image)\n\nfor face in faces:\n    print(f\"Confidence: {face.confidence:.2f}\")\n    print(f\"BBox: {face.bbox}\")\n    print(f\"Landmarks: {face.landmarks.shape}\")  # (5, 2)\n</code></pre>"},{"location":"modules/detection/#model-variants","title":"Model Variants","text":"<pre><code>from uniface import RetinaFace\nfrom uniface.constants import RetinaFaceWeights\n\n# Lightweight (mobile/edge)\ndetector = RetinaFace(model_name=RetinaFaceWeights.MNET_025)\n\n# Balanced (default)\ndetector = RetinaFace(model_name=RetinaFaceWeights.MNET_V2)\n\n# High accuracy\ndetector = RetinaFace(model_name=RetinaFaceWeights.RESNET34)\n</code></pre> Variant Params Size Easy Medium Hard MNET_025 0.4M 1.7 MB 88.5% 87.0% 80.6% MNET_050 1.0M 2.6 MB 89.4% 88.0% 82.4% MNET_V1 3.5M 3.8 MB 90.6% 89.1% 84.1% MNET_V2 \u2b50 3.2M 3.5 MB 91.7% 91.0% 86.6% RESNET18 11.7M 27 MB 92.5% 91.0% 86.6% RESNET34 24.8M 56 MB 94.2% 93.1% 88.9%"},{"location":"modules/detection/#configuration","title":"Configuration","text":"<pre><code>detector = RetinaFace(\n    model_name=RetinaFaceWeights.MNET_V2,\n    confidence_threshold=0.5,  # Min confidence\n    nms_threshold=0.4,         # NMS IoU threshold\n    input_size=(640, 640),     # Input resolution\n    dynamic_size=False         # Enable dynamic input size\n)\n</code></pre>"},{"location":"modules/detection/#scrfd","title":"SCRFD","text":"<p>State-of-the-art detection with excellent accuracy-speed tradeoff.</p>"},{"location":"modules/detection/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface import SCRFD\n\ndetector = SCRFD()\nfaces = detector.detect(image)\n</code></pre>"},{"location":"modules/detection/#model-variants_1","title":"Model Variants","text":"<pre><code>from uniface import SCRFD\nfrom uniface.constants import SCRFDWeights\n\n# Real-time (lightweight)\ndetector = SCRFD(model_name=SCRFDWeights.SCRFD_500M_KPS)\n\n# High accuracy (default)\ndetector = SCRFD(model_name=SCRFDWeights.SCRFD_10G_KPS)\n</code></pre> Variant Params Size Easy Medium Hard SCRFD_500M_KPS 0.6M 2.5 MB 90.6% 88.1% 68.5% SCRFD_10G_KPS \u2b50 4.2M 17 MB 95.2% 93.9% 83.1%"},{"location":"modules/detection/#configuration_1","title":"Configuration","text":"<pre><code>detector = SCRFD(\n    model_name=SCRFDWeights.SCRFD_10G_KPS,\n    confidence_threshold=0.5,\n    nms_threshold=0.4,\n    input_size=(640, 640)\n)\n</code></pre>"},{"location":"modules/detection/#yolov5-face","title":"YOLOv5-Face","text":"<p>YOLO-based detection optimized for faces.</p>"},{"location":"modules/detection/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface import YOLOv5Face\n\ndetector = YOLOv5Face()\nfaces = detector.detect(image)\n</code></pre>"},{"location":"modules/detection/#model-variants_2","title":"Model Variants","text":"<pre><code>from uniface import YOLOv5Face\nfrom uniface.constants import YOLOv5FaceWeights\n\n# Lightweight\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5N)\n\n# Balanced (default)\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5S)\n\n# High accuracy\ndetector = YOLOv5Face(model_name=YOLOv5FaceWeights.YOLOV5M)\n</code></pre> Variant Size Easy Medium Hard YOLOV5N 11 MB 93.6% 91.5% 80.5% YOLOV5S \u2b50 28 MB 94.3% 92.6% 83.2% YOLOV5M 82 MB 95.3% 93.8% 85.3% <p>Fixed Input Size</p> <p>YOLOv5-Face uses a fixed input size of 640\u00d7640.</p>"},{"location":"modules/detection/#configuration_2","title":"Configuration","text":"<pre><code>detector = YOLOv5Face(\n    model_name=YOLOv5FaceWeights.YOLOV5S,\n    confidence_threshold=0.6,\n    nms_threshold=0.5\n)\n</code></pre>"},{"location":"modules/detection/#factory-function","title":"Factory Function","text":"<p>Create detectors dynamically:</p> <pre><code>from uniface import create_detector\n\ndetector = create_detector('retinaface')\n# or\ndetector = create_detector('scrfd')\n# or\ndetector = create_detector('yolov5face')\n</code></pre>"},{"location":"modules/detection/#high-level-api","title":"High-Level API","text":"<p>One-line detection:</p> <pre><code>from uniface import detect_faces\n\nfaces = detect_faces(\n    image,\n    method='retinaface',\n    confidence_threshold=0.5\n)\n</code></pre>"},{"location":"modules/detection/#output-format","title":"Output Format","text":"<p>All detectors return <code>list[Face]</code>:</p> <pre><code>for face in faces:\n    # Bounding box [x1, y1, x2, y2]\n    bbox = face.bbox\n\n    # Detection confidence (0-1)\n    confidence = face.confidence\n\n    # 5-point landmarks (5, 2)\n    landmarks = face.landmarks\n    # [left_eye, right_eye, nose, left_mouth, right_mouth]\n</code></pre>"},{"location":"modules/detection/#visualization","title":"Visualization","text":"<pre><code>from uniface.visualization import draw_detections\n\ndraw_detections(\n    image=image,\n    bboxes=[f.bbox for f in faces],\n    scores=[f.confidence for f in faces],\n    landmarks=[f.landmarks for f in faces],\n    vis_threshold=0.6\n)\n\ncv2.imwrite(\"result.jpg\", image)\n</code></pre>"},{"location":"modules/detection/#performance-comparison","title":"Performance Comparison","text":"<p>Benchmark on your hardware:</p> <pre><code>python tools/detection.py --source image.jpg --iterations 100\n</code></pre>"},{"location":"modules/detection/#next-steps","title":"Next Steps","text":"<ul> <li>Recognition - Extract face embeddings</li> <li>Landmarks - 106-point landmarks</li> <li>Image Pipeline Recipe - Complete workflow</li> </ul>"},{"location":"modules/gaze/","title":"Gaze Estimation","text":"<p>Gaze estimation predicts where a person is looking (pitch and yaw angles).</p>"},{"location":"modules/gaze/#available-models","title":"Available Models","text":"Model Backbone Size MAE* Best For ResNet18 ResNet18 43 MB 12.84\u00b0 Balanced ResNet34 \u2b50 ResNet34 82 MB 11.33\u00b0 Recommended ResNet50 ResNet50 91 MB 11.34\u00b0 High accuracy MobileNetV2 MobileNetV2 9.6 MB 13.07\u00b0 Mobile MobileOne-S0 MobileOne 4.8 MB 12.58\u00b0 Lightweight <p>*MAE = Mean Absolute Error on Gaze360 test set (lower is better)</p>"},{"location":"modules/gaze/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, MobileGaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor face in faces:\n    # Crop face\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        # Estimate gaze\n        result = gaze_estimator.estimate(face_crop)\n\n        # Convert to degrees\n        pitch_deg = np.degrees(result.pitch)\n        yaw_deg = np.degrees(result.yaw)\n\n        print(f\"Pitch: {pitch_deg:.1f}\u00b0, Yaw: {yaw_deg:.1f}\u00b0\")\n</code></pre>"},{"location":"modules/gaze/#model-variants","title":"Model Variants","text":"<pre><code>from uniface import MobileGaze\nfrom uniface.constants import GazeWeights\n\n# Default (ResNet34, recommended)\ngaze = MobileGaze()\n\n# Lightweight for mobile/edge\ngaze = MobileGaze(model_name=GazeWeights.MOBILEONE_S0)\n\n# Higher accuracy\ngaze = MobileGaze(model_name=GazeWeights.RESNET50)\n</code></pre>"},{"location":"modules/gaze/#output-format","title":"Output Format","text":"<pre><code>result = gaze_estimator.estimate(face_crop)\n\n# GazeResult dataclass\nresult.pitch  # Vertical angle in radians\nresult.yaw    # Horizontal angle in radians\n</code></pre>"},{"location":"modules/gaze/#angle-convention","title":"Angle Convention","text":"<pre><code>          pitch = +90\u00b0 (looking up)\n               \u2502\n               \u2502\nyaw = -90\u00b0 \u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500 yaw = +90\u00b0\n(looking left) \u2502     (looking right)\n               \u2502\n          pitch = -90\u00b0 (looking down)\n</code></pre> <ul> <li>Pitch: Vertical gaze angle</li> <li>Positive = looking up</li> <li> <p>Negative = looking down</p> </li> <li> <p>Yaw: Horizontal gaze angle</p> </li> <li>Positive = looking right</li> <li>Negative = looking left</li> </ul>"},{"location":"modules/gaze/#visualization","title":"Visualization","text":"<pre><code>from uniface.visualization import draw_gaze\n\n# Detect faces\nfaces = detector.detect(image)\n\nfor face in faces:\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    if face_crop.size &gt; 0:\n        result = gaze_estimator.estimate(face_crop)\n\n        # Draw gaze arrow on image\n        draw_gaze(image, face.bbox, result.pitch, result.yaw)\n\ncv2.imwrite(\"gaze_output.jpg\", image)\n</code></pre>"},{"location":"modules/gaze/#custom-visualization","title":"Custom Visualization","text":"<pre><code>import cv2\nimport numpy as np\n\ndef draw_gaze_custom(image, bbox, pitch, yaw, length=100, color=(0, 255, 0)):\n    \"\"\"Draw custom gaze arrow.\"\"\"\n    x1, y1, x2, y2 = map(int, bbox)\n\n    # Face center\n    cx = (x1 + x2) // 2\n    cy = (y1 + y2) // 2\n\n    # Calculate endpoint\n    dx = -length * np.sin(yaw) * np.cos(pitch)\n    dy = -length * np.sin(pitch)\n\n    # Draw arrow\n    end_x = int(cx + dx)\n    end_y = int(cy + dy)\n\n    cv2.arrowedLine(image, (cx, cy), (end_x, end_y), color, 2, tipLength=0.3)\n\n    return image\n</code></pre>"},{"location":"modules/gaze/#real-time-gaze-tracking","title":"Real-Time Gaze Tracking","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, MobileGaze\nfrom uniface.visualization import draw_gaze\n\ndetector = RetinaFace()\ngaze_estimator = MobileGaze()\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n        face_crop = frame[y1:y2, x1:x2]\n\n        if face_crop.size &gt; 0:\n            result = gaze_estimator.estimate(face_crop)\n\n            # Draw bounding box\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Draw gaze\n            draw_gaze(frame, face.bbox, result.pitch, result.yaw)\n\n            # Display angles\n            pitch_deg = np.degrees(result.pitch)\n            yaw_deg = np.degrees(result.yaw)\n            label = f\"P:{pitch_deg:.0f} Y:{yaw_deg:.0f}\"\n            cv2.putText(frame, label, (x1, y1 - 10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    cv2.imshow(\"Gaze Estimation\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/gaze/#use-cases","title":"Use Cases","text":""},{"location":"modules/gaze/#attention-detection","title":"Attention Detection","text":"<pre><code>def is_looking_at_camera(result, threshold=15):\n    \"\"\"Check if person is looking at camera.\"\"\"\n    pitch_deg = abs(np.degrees(result.pitch))\n    yaw_deg = abs(np.degrees(result.yaw))\n\n    return pitch_deg &lt; threshold and yaw_deg &lt; threshold\n\n# Usage\nresult = gaze_estimator.estimate(face_crop)\nif is_looking_at_camera(result):\n    print(\"Looking at camera\")\nelse:\n    print(\"Looking away\")\n</code></pre>"},{"location":"modules/gaze/#gaze-direction-classification","title":"Gaze Direction Classification","text":"<pre><code>def classify_gaze_direction(result, threshold=20):\n    \"\"\"Classify gaze into directions.\"\"\"\n    pitch_deg = np.degrees(result.pitch)\n    yaw_deg = np.degrees(result.yaw)\n\n    directions = []\n\n    if pitch_deg &gt; threshold:\n        directions.append(\"up\")\n    elif pitch_deg &lt; -threshold:\n        directions.append(\"down\")\n\n    if yaw_deg &gt; threshold:\n        directions.append(\"right\")\n    elif yaw_deg &lt; -threshold:\n        directions.append(\"left\")\n\n    if not directions:\n        return \"center\"\n\n    return \" \".join(directions)\n\n# Usage\nresult = gaze_estimator.estimate(face_crop)\ndirection = classify_gaze_direction(result)\nprint(f\"Looking: {direction}\")\n</code></pre>"},{"location":"modules/gaze/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_gaze_estimator\n\ngaze = create_gaze_estimator()  # Returns MobileGaze\n</code></pre>"},{"location":"modules/gaze/#next-steps","title":"Next Steps","text":"<ul> <li>Anti-Spoofing - Face liveness detection</li> <li>Privacy - Face anonymization</li> <li>Video Recipe - Real-time processing</li> </ul>"},{"location":"modules/landmarks/","title":"Landmarks","text":"<p>Facial landmark detection provides precise localization of facial features.</p>"},{"location":"modules/landmarks/#available-models","title":"Available Models","text":"Model Points Size Use Case Landmark106 106 14 MB Detailed face analysis <p>5-Point Landmarks</p> <p>Basic 5-point landmarks are included with all detection models (RetinaFace, SCRFD, YOLOv5-Face).</p>"},{"location":"modules/landmarks/#106-point-landmarks","title":"106-Point Landmarks","text":""},{"location":"modules/landmarks/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace, Landmark106\n\ndetector = RetinaFace()\nlandmarker = Landmark106()\n\n# Detect face\nfaces = detector.detect(image)\n\n# Get detailed landmarks\nif faces:\n    landmarks = landmarker.get_landmarks(image, faces[0].bbox)\n    print(f\"Landmarks shape: {landmarks.shape}\")  # (106, 2)\n</code></pre>"},{"location":"modules/landmarks/#landmark-groups","title":"Landmark Groups","text":"Range Group Points 0-32 Face Contour 33 33-50 Eyebrows 18 51-62 Nose 12 63-86 Eyes 24 87-105 Mouth 19"},{"location":"modules/landmarks/#extract-specific-features","title":"Extract Specific Features","text":"<pre><code>landmarks = landmarker.get_landmarks(image, face.bbox)\n\n# Face contour\ncontour = landmarks[0:33]\n\n# Left eyebrow\nleft_eyebrow = landmarks[33:42]\n\n# Right eyebrow\nright_eyebrow = landmarks[42:51]\n\n# Nose\nnose = landmarks[51:63]\n\n# Left eye\nleft_eye = landmarks[63:72]\n\n# Right eye\nright_eye = landmarks[76:84]\n\n# Mouth\nmouth = landmarks[87:106]\n</code></pre>"},{"location":"modules/landmarks/#5-point-landmarks-detection","title":"5-Point Landmarks (Detection)","text":"<p>All detection models provide 5-point landmarks:</p> <pre><code>from uniface import RetinaFace\n\ndetector = RetinaFace()\nfaces = detector.detect(image)\n\nif faces:\n    landmarks_5 = faces[0].landmarks\n    print(f\"Shape: {landmarks_5.shape}\")  # (5, 2)\n\n    left_eye = landmarks_5[0]\n    right_eye = landmarks_5[1]\n    nose = landmarks_5[2]\n    left_mouth = landmarks_5[3]\n    right_mouth = landmarks_5[4]\n</code></pre>"},{"location":"modules/landmarks/#visualization","title":"Visualization","text":""},{"location":"modules/landmarks/#draw-106-landmarks","title":"Draw 106 Landmarks","text":"<pre><code>import cv2\n\ndef draw_landmarks(image, landmarks, color=(0, 255, 0), radius=2):\n    \"\"\"Draw landmarks on image.\"\"\"\n    for x, y in landmarks.astype(int):\n        cv2.circle(image, (x, y), radius, color, -1)\n    return image\n\n# Usage\nlandmarks = landmarker.get_landmarks(image, face.bbox)\nimage_with_landmarks = draw_landmarks(image.copy(), landmarks)\ncv2.imwrite(\"landmarks.jpg\", image_with_landmarks)\n</code></pre>"},{"location":"modules/landmarks/#draw-with-connections","title":"Draw with Connections","text":"<pre><code>def draw_landmarks_with_connections(image, landmarks):\n    \"\"\"Draw landmarks with facial feature connections.\"\"\"\n    landmarks = landmarks.astype(int)\n\n    # Face contour (0-32)\n    for i in range(32):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (255, 255, 0), 1)\n\n    # Left eyebrow (33-41)\n    for i in range(33, 41):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 255, 0), 1)\n\n    # Right eyebrow (42-50)\n    for i in range(42, 50):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 255, 0), 1)\n\n    # Nose (51-62)\n    for i in range(51, 62):\n        cv2.line(image, tuple(landmarks[i]), tuple(landmarks[i+1]), (0, 0, 255), 1)\n\n    # Draw points\n    for x, y in landmarks:\n        cv2.circle(image, (x, y), 2, (0, 255, 255), -1)\n\n    return image\n</code></pre>"},{"location":"modules/landmarks/#use-cases","title":"Use Cases","text":""},{"location":"modules/landmarks/#face-alignment","title":"Face Alignment","text":"<pre><code>from uniface import face_alignment\n\n# Align face using 5-point landmarks\naligned = face_alignment(image, faces[0].landmarks)\n# Returns: 112x112 aligned face\n</code></pre>"},{"location":"modules/landmarks/#eye-aspect-ratio-blink-detection","title":"Eye Aspect Ratio (Blink Detection)","text":"<pre><code>import numpy as np\n\ndef eye_aspect_ratio(eye_landmarks):\n    \"\"\"Calculate eye aspect ratio for blink detection.\"\"\"\n    # Vertical distances\n    v1 = np.linalg.norm(eye_landmarks[1] - eye_landmarks[5])\n    v2 = np.linalg.norm(eye_landmarks[2] - eye_landmarks[4])\n\n    # Horizontal distance\n    h = np.linalg.norm(eye_landmarks[0] - eye_landmarks[3])\n\n    ear = (v1 + v2) / (2.0 * h)\n    return ear\n\n# Usage with 106-point landmarks\nleft_eye = landmarks[63:72]  # Approximate eye points\near = eye_aspect_ratio(left_eye)\n\nif ear &lt; 0.2:\n    print(\"Eye closed (blink detected)\")\n</code></pre>"},{"location":"modules/landmarks/#head-pose-estimation","title":"Head Pose Estimation","text":"<pre><code>import cv2\nimport numpy as np\n\ndef estimate_head_pose(landmarks, image_shape):\n    \"\"\"Estimate head pose from facial landmarks.\"\"\"\n    # 3D model points (generic face model)\n    model_points = np.array([\n        (0.0, 0.0, 0.0),       # Nose tip\n        (0.0, -330.0, -65.0),  # Chin\n        (-225.0, 170.0, -135.0),  # Left eye corner\n        (225.0, 170.0, -135.0),   # Right eye corner\n        (-150.0, -150.0, -125.0), # Left mouth corner\n        (150.0, -150.0, -125.0)   # Right mouth corner\n    ], dtype=np.float64)\n\n    # 2D image points (from 106 landmarks)\n    image_points = np.array([\n        landmarks[51],   # Nose tip\n        landmarks[16],   # Chin\n        landmarks[63],   # Left eye corner\n        landmarks[76],   # Right eye corner\n        landmarks[87],   # Left mouth corner\n        landmarks[93]    # Right mouth corner\n    ], dtype=np.float64)\n\n    # Camera matrix\n    h, w = image_shape[:2]\n    focal_length = w\n    center = (w / 2, h / 2)\n    camera_matrix = np.array([\n        [focal_length, 0, center[0]],\n        [0, focal_length, center[1]],\n        [0, 0, 1]\n    ], dtype=np.float64)\n\n    # Solve PnP\n    dist_coeffs = np.zeros((4, 1))\n    success, rotation_vector, translation_vector = cv2.solvePnP(\n        model_points, image_points, camera_matrix, dist_coeffs\n    )\n\n    return rotation_vector, translation_vector\n</code></pre>"},{"location":"modules/landmarks/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_landmarker\n\nlandmarker = create_landmarker()  # Returns Landmark106\n</code></pre>"},{"location":"modules/landmarks/#next-steps","title":"Next Steps","text":"<ul> <li>Attributes - Age, gender, emotion</li> <li>Gaze - Gaze estimation</li> <li>Detection - Face detection with 5-point landmarks</li> </ul>"},{"location":"modules/parsing/","title":"Parsing","text":"<p>Face parsing segments faces into semantic components (skin, eyes, nose, mouth, hair, etc.).</p>"},{"location":"modules/parsing/#available-models","title":"Available Models","text":"Model Backbone Size Classes Best For BiSeNet ResNet18 \u2b50 ResNet18 51 MB 19 Balanced (recommended) BiSeNet ResNet34 ResNet34 89 MB 19 Higher accuracy"},{"location":"modules/parsing/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nfrom uniface.parsing import BiSeNet\nfrom uniface.visualization import vis_parsing_maps\n\n# Initialize parser\nparser = BiSeNet()\n\n# Load face image (cropped)\nface_image = cv2.imread(\"face.jpg\")\n\n# Parse face\nmask = parser.parse(face_image)\nprint(f\"Mask shape: {mask.shape}\")  # (H, W)\n\n# Visualize\nface_rgb = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\nvis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\n# Save result\nvis_bgr = cv2.cvtColor(vis_result, cv2.COLOR_RGB2BGR)\ncv2.imwrite(\"parsed.jpg\", vis_bgr)\n</code></pre>"},{"location":"modules/parsing/#19-facial-component-classes","title":"19 Facial Component Classes","text":"ID Class ID Class 0 Background 10 Ear Ring 1 Skin 11 Nose 2 Left Eyebrow 12 Mouth 3 Right Eyebrow 13 Upper Lip 4 Left Eye 14 Lower Lip 5 Right Eye 15 Neck 6 Eye Glasses 16 Neck Lace 7 Left Ear 17 Cloth 8 Right Ear 18 Hair 9 Hat"},{"location":"modules/parsing/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.parsing import BiSeNet\nfrom uniface.constants import ParsingWeights\n\n# Default (ResNet18)\nparser = BiSeNet()\n\n# Higher accuracy (ResNet34)\nparser = BiSeNet(model_name=ParsingWeights.RESNET34)\n</code></pre> Variant Params Size Notes RESNET18 \u2b50 13.3M 51 MB Recommended RESNET34 24.1M 89 MB Higher accuracy"},{"location":"modules/parsing/#full-pipeline","title":"Full Pipeline","text":""},{"location":"modules/parsing/#with-face-detection","title":"With Face Detection","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.parsing import BiSeNet\nfrom uniface.visualization import vis_parsing_maps\n\ndetector = RetinaFace()\nparser = BiSeNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor i, face in enumerate(faces):\n    # Crop face\n    x1, y1, x2, y2 = map(int, face.bbox)\n    face_crop = image[y1:y2, x1:x2]\n\n    # Parse\n    mask = parser.parse(face_crop)\n\n    # Visualize\n    face_rgb = cv2.cvtColor(face_crop, cv2.COLOR_BGR2RGB)\n    vis_result = vis_parsing_maps(face_rgb, mask, save_image=False)\n\n    # Save\n    vis_bgr = cv2.cvtColor(vis_result, cv2.COLOR_RGB2BGR)\n    cv2.imwrite(f\"face_{i}_parsed.jpg\", vis_bgr)\n</code></pre>"},{"location":"modules/parsing/#extract-specific-components","title":"Extract Specific Components","text":""},{"location":"modules/parsing/#get-single-component-mask","title":"Get Single Component Mask","text":"<pre><code>import numpy as np\n\n# Parse face\nmask = parser.parse(face_image)\n\n# Extract specific component\nSKIN = 1\nHAIR = 18\nLEFT_EYE = 4\nRIGHT_EYE = 5\n\n# Binary mask for skin\nskin_mask = (mask == SKIN).astype(np.uint8) * 255\n\n# Binary mask for hair\nhair_mask = (mask == HAIR).astype(np.uint8) * 255\n\n# Binary mask for eyes\neyes_mask = ((mask == LEFT_EYE) | (mask == RIGHT_EYE)).astype(np.uint8) * 255\n</code></pre>"},{"location":"modules/parsing/#count-pixels-per-component","title":"Count Pixels per Component","text":"<pre><code>import numpy as np\n\nmask = parser.parse(face_image)\n\ncomponent_names = {\n    0: 'Background', 1: 'Skin', 2: 'L-Eyebrow', 3: 'R-Eyebrow',\n    4: 'L-Eye', 5: 'R-Eye', 6: 'Glasses', 7: 'L-Ear', 8: 'R-Ear',\n    9: 'Hat', 10: 'Earring', 11: 'Nose', 12: 'Mouth',\n    13: 'U-Lip', 14: 'L-Lip', 15: 'Neck', 16: 'Necklace',\n    17: 'Cloth', 18: 'Hair'\n}\n\nfor class_id in np.unique(mask):\n    pixel_count = np.sum(mask == class_id)\n    name = component_names.get(class_id, f'Class {class_id}')\n    print(f\"{name}: {pixel_count} pixels\")\n</code></pre>"},{"location":"modules/parsing/#applications","title":"Applications","text":""},{"location":"modules/parsing/#face-makeup","title":"Face Makeup","text":"<p>Apply virtual makeup using component masks:</p> <pre><code>import cv2\nimport numpy as np\n\ndef apply_lip_color(image, mask, color=(180, 50, 50)):\n    \"\"\"Apply lip color using parsing mask.\"\"\"\n    result = image.copy()\n\n    # Get lip mask (upper + lower lip)\n    lip_mask = ((mask == 13) | (mask == 14)).astype(np.uint8)\n\n    # Create color overlay\n    overlay = np.zeros_like(image)\n    overlay[:] = color\n\n    # Blend with original\n    lip_region = cv2.bitwise_and(overlay, overlay, mask=lip_mask)\n    non_lip = cv2.bitwise_and(result, result, mask=1 - lip_mask)\n\n    # Combine with alpha blending\n    alpha = 0.4\n    result = cv2.addWeighted(result, 1 - alpha * lip_mask[:,:,np.newaxis] / 255,\n                             lip_region, alpha, 0)\n\n    return result.astype(np.uint8)\n</code></pre>"},{"location":"modules/parsing/#background-replacement","title":"Background Replacement","text":"<pre><code>def replace_background(image, mask, background):\n    \"\"\"Replace background using parsing mask.\"\"\"\n    # Create foreground mask (everything except background)\n    foreground_mask = (mask != 0).astype(np.uint8)\n\n    # Resize background to match image\n    background = cv2.resize(background, (image.shape[1], image.shape[0]))\n\n    # Combine\n    result = image.copy()\n    result[foreground_mask == 0] = background[foreground_mask == 0]\n\n    return result\n</code></pre>"},{"location":"modules/parsing/#hair-segmentation","title":"Hair Segmentation","text":"<pre><code>def get_hair_mask(mask):\n    \"\"\"Extract clean hair mask.\"\"\"\n    hair_mask = (mask == 18).astype(np.uint8) * 255\n\n    # Clean up with morphological operations\n    kernel = np.ones((5, 5), np.uint8)\n    hair_mask = cv2.morphologyEx(hair_mask, cv2.MORPH_CLOSE, kernel)\n    hair_mask = cv2.morphologyEx(hair_mask, cv2.MORPH_OPEN, kernel)\n\n    return hair_mask\n</code></pre>"},{"location":"modules/parsing/#visualization-options","title":"Visualization Options","text":"<pre><code>from uniface.visualization import vis_parsing_maps\n\n# Default visualization\nvis_result = vis_parsing_maps(face_rgb, mask)\n\n# With different parameters\nvis_result = vis_parsing_maps(\n    face_rgb,\n    mask,\n    save_image=False,  # Don't save to file\n)\n</code></pre>"},{"location":"modules/parsing/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_face_parser\n\nparser = create_face_parser()  # Returns BiSeNet\n</code></pre>"},{"location":"modules/parsing/#next-steps","title":"Next Steps","text":"<ul> <li>Gaze - Gaze estimation</li> <li>Privacy - Face anonymization</li> <li>Detection - Face detection</li> </ul>"},{"location":"modules/privacy/","title":"Privacy","text":"<p>Face anonymization protects privacy by blurring or obscuring faces in images and videos.</p>"},{"location":"modules/privacy/#available-methods","title":"Available Methods","text":"Method Description Use Case pixelate Blocky pixelation News media standard gaussian Smooth blur Natural appearance blackout Solid color fill Maximum privacy elliptical Oval-shaped blur Natural face shape median Edge-preserving blur Artistic effect"},{"location":"modules/privacy/#quick-start","title":"Quick Start","text":""},{"location":"modules/privacy/#one-line-anonymization","title":"One-Line Anonymization","text":"<pre><code>from uniface.privacy import anonymize_faces\nimport cv2\n\nimage = cv2.imread(\"group_photo.jpg\")\nanonymized = anonymize_faces(image, method='pixelate')\ncv2.imwrite(\"anonymized.jpg\", anonymized)\n</code></pre>"},{"location":"modules/privacy/#blurface-class","title":"BlurFace Class","text":"<p>For more control, use the <code>BlurFace</code> class:</p> <pre><code>from uniface import RetinaFace\nfrom uniface.privacy import BlurFace\nimport cv2\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian', blur_strength=5.0)\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\nanonymized = blurrer.anonymize(image, faces)\n\ncv2.imwrite(\"anonymized.jpg\", anonymized)\n</code></pre>"},{"location":"modules/privacy/#blur-methods","title":"Blur Methods","text":""},{"location":"modules/privacy/#pixelate","title":"Pixelate","text":"<p>Blocky pixelation effect (common in news media):</p> <pre><code>blurrer = BlurFace(method='pixelate', pixel_blocks=10)\n</code></pre> Parameter Default Description <code>pixel_blocks</code> 10 Number of blocks (lower = more pixelated)"},{"location":"modules/privacy/#gaussian","title":"Gaussian","text":"<p>Smooth, natural-looking blur:</p> <pre><code>blurrer = BlurFace(method='gaussian', blur_strength=3.0)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity (higher = more blur)"},{"location":"modules/privacy/#blackout","title":"Blackout","text":"<p>Solid color fill for maximum privacy:</p> <pre><code>blurrer = BlurFace(method='blackout', color=(0, 0, 0))\n</code></pre> Parameter Default Description <code>color</code> (0, 0, 0) Fill color (BGR format)"},{"location":"modules/privacy/#elliptical","title":"Elliptical","text":"<p>Oval-shaped blur matching natural face shape:</p> <pre><code>blurrer = BlurFace(method='elliptical', blur_strength=3.0, margin=20)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity <code>margin</code> 20 Margin around face"},{"location":"modules/privacy/#median","title":"Median","text":"<p>Edge-preserving blur with artistic effect:</p> <pre><code>blurrer = BlurFace(method='median', blur_strength=3.0)\n</code></pre> Parameter Default Description <code>blur_strength</code> 3.0 Blur intensity"},{"location":"modules/privacy/#in-place-processing","title":"In-Place Processing","text":"<p>Modify image directly (faster, saves memory):</p> <pre><code>blurrer = BlurFace(method='pixelate')\n\n# In-place modification\nresult = blurrer.anonymize(image, faces, inplace=True)\n# 'image' and 'result' point to the same array\n</code></pre>"},{"location":"modules/privacy/#real-time-anonymization","title":"Real-Time Anonymization","text":""},{"location":"modules/privacy/#webcam","title":"Webcam","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='pixelate')\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n\n    cv2.imshow('Anonymized', frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/privacy/#video-file","title":"Video File","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian')\n\ncap = cv2.VideoCapture(\"input_video.mp4\")\nfps = cap.get(cv2.CAP_PROP_FPS)\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('output_video.mp4', fourcc, fps, (width, height))\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n    out.write(frame)\n\ncap.release()\nout.release()\n</code></pre>"},{"location":"modules/privacy/#selective-anonymization","title":"Selective Anonymization","text":""},{"location":"modules/privacy/#exclude-specific-faces","title":"Exclude Specific Faces","text":"<pre><code>def anonymize_except(image, all_faces, exclude_embeddings, recognizer, threshold=0.6):\n    \"\"\"Anonymize all faces except those matching exclude_embeddings.\"\"\"\n    faces_to_blur = []\n\n    for face in all_faces:\n        # Get embedding\n        embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n        # Check if should be excluded\n        should_exclude = False\n        for ref_emb in exclude_embeddings:\n            similarity = np.dot(embedding, ref_emb.T)[0][0]\n            if similarity &gt; threshold:\n                should_exclude = True\n                break\n\n        if not should_exclude:\n            faces_to_blur.append(face)\n\n    # Blur remaining faces\n    return blurrer.anonymize(image, faces_to_blur)\n</code></pre>"},{"location":"modules/privacy/#confidence-based","title":"Confidence-Based","text":"<pre><code>def anonymize_low_confidence(image, faces, blurrer, confidence_threshold=0.8):\n    \"\"\"Anonymize faces below confidence threshold.\"\"\"\n    faces_to_blur = [f for f in faces if f.confidence &lt; confidence_threshold]\n    return blurrer.anonymize(image, faces_to_blur)\n</code></pre>"},{"location":"modules/privacy/#comparison","title":"Comparison","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nmethods = ['pixelate', 'gaussian', 'blackout', 'elliptical', 'median']\n\nfor method in methods:\n    blurrer = BlurFace(method=method)\n    result = blurrer.anonymize(image.copy(), faces)\n    cv2.imwrite(f\"anonymized_{method}.jpg\", result)\n</code></pre>"},{"location":"modules/privacy/#command-line-tool","title":"Command-Line Tool","text":"<pre><code># Anonymize image with pixelation\npython tools/face_anonymize.py --source photo.jpg\n\n# Real-time webcam\npython tools/face_anonymize.py --source 0 --method gaussian\n\n# Custom blur strength\npython tools/face_anonymize.py --source photo.jpg --method gaussian --blur-strength 5.0\n</code></pre>"},{"location":"modules/privacy/#next-steps","title":"Next Steps","text":"<ul> <li>Anonymize Stream Recipe - Video pipeline</li> <li>Detection - Face detection options</li> <li>Batch Processing Recipe - Process multiple files</li> </ul>"},{"location":"modules/recognition/","title":"Recognition","text":"<p>Face recognition extracts embeddings for identity verification and face search.</p>"},{"location":"modules/recognition/#available-models","title":"Available Models","text":"Model Backbone Size Embedding Dim Best For ArcFace MobileNet/ResNet 8-166 MB 512 General use (recommended) MobileFace MobileNet V2/V3 1-10 MB 512 Mobile/Edge SphereFace Sphere20/36 50-92 MB 512 Research"},{"location":"modules/recognition/#arcface","title":"ArcFace","text":"<p>State-of-the-art recognition using additive angular margin loss.</p>"},{"location":"modules/recognition/#basic-usage","title":"Basic Usage","text":"<pre><code>from uniface import RetinaFace, ArcFace\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Detect face\nfaces = detector.detect(image)\n\n# Extract embedding\nif faces:\n    embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n    print(f\"Embedding shape: {embedding.shape}\")  # (1, 512)\n</code></pre>"},{"location":"modules/recognition/#model-variants","title":"Model Variants","text":"<pre><code>from uniface import ArcFace\nfrom uniface.constants import ArcFaceWeights\n\n# Lightweight (default)\nrecognizer = ArcFace(model_name=ArcFaceWeights.MNET)\n\n# High accuracy\nrecognizer = ArcFace(model_name=ArcFaceWeights.RESNET)\n</code></pre> Variant Backbone Size Use Case MNET \u2b50 MobileNet 8 MB Balanced (recommended) RESNET ResNet50 166 MB Maximum accuracy"},{"location":"modules/recognition/#mobileface","title":"MobileFace","text":"<p>Lightweight recognition for resource-constrained environments.</p>"},{"location":"modules/recognition/#basic-usage_1","title":"Basic Usage","text":"<pre><code>from uniface import MobileFace\n\nrecognizer = MobileFace()\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n</code></pre>"},{"location":"modules/recognition/#model-variants_1","title":"Model Variants","text":"<pre><code>from uniface import MobileFace\nfrom uniface.constants import MobileFaceWeights\n\n# Ultra-lightweight\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_025)\n\n# Balanced (default)\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_V2)\n\n# Higher accuracy\nrecognizer = MobileFace(model_name=MobileFaceWeights.MNET_V3_LARGE)\n</code></pre> Variant Params Size LFW Use Case MNET_025 0.36M 1 MB 98.8% Ultra-lightweight MNET_V2 \u2b50 2.29M 4 MB 99.6% Mobile/Edge MNET_V3_SMALL 1.25M 3 MB 99.3% Mobile optimized MNET_V3_LARGE 3.52M 10 MB 99.5% Balanced mobile"},{"location":"modules/recognition/#sphereface","title":"SphereFace","text":"<p>Recognition using angular softmax loss (A-Softmax).</p>"},{"location":"modules/recognition/#basic-usage_2","title":"Basic Usage","text":"<pre><code>from uniface import SphereFace\nfrom uniface.constants import SphereFaceWeights\n\nrecognizer = SphereFace(model_name=SphereFaceWeights.SPHERE20)\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n</code></pre> Variant Params Size LFW Use Case SPHERE20 24.5M 50 MB 99.7% Research SPHERE36 34.6M 92 MB 99.7% Research"},{"location":"modules/recognition/#face-comparison","title":"Face Comparison","text":""},{"location":"modules/recognition/#compute-similarity","title":"Compute Similarity","text":"<pre><code>from uniface import compute_similarity\nimport numpy as np\n\n# Extract embeddings\nemb1 = recognizer.get_normalized_embedding(image1, landmarks1)\nemb2 = recognizer.get_normalized_embedding(image2, landmarks2)\n\n# Method 1: Using utility function\nsimilarity = compute_similarity(emb1, emb2)\n\n# Method 2: Direct computation\nsimilarity = np.dot(emb1, emb2.T)[0][0]\n\nprint(f\"Similarity: {similarity:.4f}\")\n</code></pre>"},{"location":"modules/recognition/#threshold-guidelines","title":"Threshold Guidelines","text":"Threshold Decision Use Case &gt; 0.7 Very high confidence Security-critical &gt; 0.6 Same person General verification 0.4 - 0.6 Uncertain Manual review needed &lt; 0.4 Different people Rejection"},{"location":"modules/recognition/#face-alignment","title":"Face Alignment","text":"<p>Recognition models require aligned faces. UniFace handles this internally:</p> <pre><code># Alignment is done automatically\nembedding = recognizer.get_normalized_embedding(image, landmarks)\n\n# Or manually align\nfrom uniface import face_alignment\n\naligned_face = face_alignment(image, landmarks)\n# Returns: 112x112 aligned face image\n</code></pre>"},{"location":"modules/recognition/#building-a-face-database","title":"Building a Face Database","text":"<pre><code>import numpy as np\nfrom uniface import RetinaFace, ArcFace\n\ndetector = RetinaFace()\nrecognizer = ArcFace()\n\n# Build database\ndatabase = {}\nfor person_id, image_path in person_images.items():\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n\n    if faces:\n        embedding = recognizer.get_normalized_embedding(image, faces[0].landmarks)\n        database[person_id] = embedding\n\n# Save for later use\nnp.savez('face_database.npz', **database)\n\n# Load database\ndata = np.load('face_database.npz')\ndatabase = {key: data[key] for key in data.files}\n</code></pre>"},{"location":"modules/recognition/#face-search","title":"Face Search","text":"<p>Find a person in a database:</p> <pre><code>def search_face(query_embedding, database, threshold=0.6):\n    \"\"\"Find best match in database.\"\"\"\n    best_match = None\n    best_similarity = -1\n\n    for person_id, db_embedding in database.items():\n        similarity = np.dot(query_embedding, db_embedding.T)[0][0]\n\n        if similarity &gt; best_similarity and similarity &gt; threshold:\n            best_similarity = similarity\n            best_match = person_id\n\n    return best_match, best_similarity\n\n# Usage\nquery_embedding = recognizer.get_normalized_embedding(query_image, landmarks)\nmatch, similarity = search_face(query_embedding, database)\n\nif match:\n    print(f\"Found: {match} (similarity: {similarity:.4f})\")\nelse:\n    print(\"No match found\")\n</code></pre>"},{"location":"modules/recognition/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_recognizer\n\nrecognizer = create_recognizer('arcface')\n</code></pre>"},{"location":"modules/recognition/#next-steps","title":"Next Steps","text":"<ul> <li>Landmarks - 106-point landmarks</li> <li>Face Search Recipe - Complete search system</li> <li>Thresholds - Calibration guide</li> </ul>"},{"location":"modules/spoofing/","title":"Anti-Spoofing","text":"<p>Face anti-spoofing detects whether a face is real (live) or fake (photo, video replay, mask).</p>"},{"location":"modules/spoofing/#available-models","title":"Available Models","text":"Model Size Notes MiniFASNet V1SE 1.2 MB Squeeze-and-Excitation variant MiniFASNet V2 \u2b50 1.2 MB Improved version (recommended)"},{"location":"modules/spoofing/#basic-usage","title":"Basic Usage","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\nimage = cv2.imread(\"photo.jpg\")\nfaces = detector.detect(image)\n\nfor face in faces:\n    result = spoofer.predict(image, face.bbox)\n\n    label = \"Real\" if result.is_real else \"Fake\"\n    print(f\"{label}: {result.confidence:.1%}\")\n</code></pre>"},{"location":"modules/spoofing/#output-format","title":"Output Format","text":"<pre><code>result = spoofer.predict(image, face.bbox)\n\n# SpoofingResult dataclass\nresult.is_real     # True = real, False = fake\nresult.confidence  # 0.0 to 1.0\n</code></pre>"},{"location":"modules/spoofing/#model-variants","title":"Model Variants","text":"<pre><code>from uniface.spoofing import MiniFASNet\nfrom uniface.constants import MiniFASNetWeights\n\n# Default (V2, recommended)\nspoofer = MiniFASNet()\n\n# V1SE variant\nspoofer = MiniFASNet(model_name=MiniFASNetWeights.V1SE)\n</code></pre> Variant Size Scale Factor V1SE 1.2 MB 4.0 V2 \u2b50 1.2 MB 2.7"},{"location":"modules/spoofing/#confidence-thresholds","title":"Confidence Thresholds","text":"<p>The default threshold is 0.5. Adjust for your use case:</p> <pre><code>result = spoofer.predict(image, face.bbox)\n\n# High security (fewer false accepts)\nHIGH_THRESHOLD = 0.7\nif result.confidence &gt; HIGH_THRESHOLD:\n    print(\"Real (high confidence)\")\nelse:\n    print(\"Suspicious\")\n\n# Balanced\nif result.is_real:  # Uses default 0.5 threshold\n    print(\"Real\")\nelse:\n    print(\"Fake\")\n</code></pre>"},{"location":"modules/spoofing/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef draw_spoofing_result(image, face, result):\n    \"\"\"Draw spoofing result on image.\"\"\"\n    x1, y1, x2, y2 = map(int, face.bbox)\n\n    # Color based on result\n    color = (0, 255, 0) if result.is_real else (0, 0, 255)\n    label = \"Real\" if result.is_real else \"Fake\"\n\n    # Draw bounding box\n    cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n\n    # Draw label\n    text = f\"{label}: {result.confidence:.1%}\"\n    cv2.putText(image, text, (x1, y1 - 10),\n                cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    return image\n\n# Usage\nfor face in faces:\n    result = spoofer.predict(image, face.bbox)\n    image = draw_spoofing_result(image, face, result)\n\ncv2.imwrite(\"spoofing_result.jpg\", image)\n</code></pre>"},{"location":"modules/spoofing/#real-time-liveness-detection","title":"Real-Time Liveness Detection","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.spoofing import MiniFASNet\n\ndetector = RetinaFace()\nspoofer = MiniFASNet()\n\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    for face in faces:\n        result = spoofer.predict(frame, face.bbox)\n\n        # Draw result\n        x1, y1, x2, y2 = map(int, face.bbox)\n        color = (0, 255, 0) if result.is_real else (0, 0, 255)\n        label = f\"{'Real' if result.is_real else 'Fake'}: {result.confidence:.0%}\"\n\n        cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(frame, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    cv2.imshow(\"Liveness Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"modules/spoofing/#use-cases","title":"Use Cases","text":""},{"location":"modules/spoofing/#access-control","title":"Access Control","text":"<pre><code>def verify_liveness(image, face, spoofer, threshold=0.6):\n    \"\"\"Verify face is real for access control.\"\"\"\n    result = spoofer.predict(image, face.bbox)\n\n    if result.is_real and result.confidence &gt; threshold:\n        return True, result.confidence\n    return False, result.confidence\n\n# Usage\nis_live, confidence = verify_liveness(image, face, spoofer)\nif is_live:\n    print(f\"Access granted (confidence: {confidence:.1%})\")\nelse:\n    print(f\"Access denied - possible spoof attempt\")\n</code></pre>"},{"location":"modules/spoofing/#multi-frame-verification","title":"Multi-Frame Verification","text":"<p>For higher security, verify across multiple frames:</p> <pre><code>def verify_liveness_multiframe(frames, detector, spoofer, min_real=3):\n    \"\"\"Verify liveness across multiple frames.\"\"\"\n    real_count = 0\n\n    for frame in frames:\n        faces = detector.detect(frame)\n        if not faces:\n            continue\n\n        result = spoofer.predict(frame, faces[0].bbox)\n        if result.is_real:\n            real_count += 1\n\n    return real_count &gt;= min_real\n\n# Collect frames and verify\nframes = []\nfor _ in range(5):\n    ret, frame = cap.read()\n    if ret:\n        frames.append(frame)\n\nis_verified = verify_liveness_multiframe(frames, detector, spoofer)\n</code></pre>"},{"location":"modules/spoofing/#attack-types-detected","title":"Attack Types Detected","text":"<p>MiniFASNet can detect various spoof attacks:</p> Attack Type Detection Printed photos \u2705 Screen replay \u2705 Video replay \u2705 Paper masks \u2705 3D masks Limited <p>Limitations</p> <ul> <li>High-quality 3D masks may not be detected</li> <li>Performance varies with lighting and image quality</li> <li>Always combine with other verification methods for high-security applications</li> </ul>"},{"location":"modules/spoofing/#command-line-tool","title":"Command-Line Tool","text":"<pre><code># Image\npython tools/spoofing.py --source photo.jpg\n\n# Webcam\npython tools/spoofing.py --source 0\n</code></pre>"},{"location":"modules/spoofing/#factory-function","title":"Factory Function","text":"<pre><code>from uniface import create_spoofer\n\nspoofer = create_spoofer()  # Returns MiniFASNet\n</code></pre>"},{"location":"modules/spoofing/#next-steps","title":"Next Steps","text":"<ul> <li>Privacy - Face anonymization</li> <li>Detection - Face detection</li> <li>Recognition - Face recognition</li> </ul>"},{"location":"recipes/anonymize-stream/","title":"Anonymize Stream","text":"<p>Blur faces in real-time video streams for privacy protection.</p>"},{"location":"recipes/anonymize-stream/#webcam","title":"Webcam","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='pixelate')\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    frame = blurrer.anonymize(frame, faces, inplace=True)\n\n    cv2.imshow('Anonymized', frame)\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/anonymize-stream/#video-file","title":"Video File","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.privacy import BlurFace\n\ndetector = RetinaFace()\nblurrer = BlurFace(method='gaussian')\n\ncap = cv2.VideoCapture(\"input.mp4\")\nfps = cap.get(cv2.CAP_PROP_FPS)\nw, h = int(cap.get(3)), int(cap.get(4))\n\nout = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n\nwhile cap.read()[0]:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n    blurrer.anonymize(frame, faces, inplace=True)\n    out.write(frame)\n\ncap.release()\nout.release()\n</code></pre>"},{"location":"recipes/anonymize-stream/#one-liner","title":"One-Liner","text":"<pre><code>from uniface.privacy import anonymize_faces\nimport cv2\n\nimage = cv2.imread(\"photo.jpg\")\nresult = anonymize_faces(image, method='pixelate')\ncv2.imwrite(\"anonymized.jpg\", result)\n</code></pre>"},{"location":"recipes/anonymize-stream/#blur-methods","title":"Blur Methods","text":"Method Code Pixelate <code>BlurFace(method='pixelate', pixel_blocks=10)</code> Gaussian <code>BlurFace(method='gaussian', blur_strength=3.0)</code> Blackout <code>BlurFace(method='blackout', color=(0,0,0))</code> Elliptical <code>BlurFace(method='elliptical', margin=20)</code> Median <code>BlurFace(method='median', blur_strength=3.0)</code>"},{"location":"recipes/batch-processing/","title":"Batch Processing","text":"<p>Process multiple images efficiently.</p>"},{"location":"recipes/batch-processing/#basic-batch-processing","title":"Basic Batch Processing","text":"<pre><code>import cv2\nfrom pathlib import Path\nfrom uniface import RetinaFace\nfrom uniface.visualization import draw_detections\n\ndetector = RetinaFace()\n\ndef process_directory(input_dir, output_dir):\n    \"\"\"Process all images in a directory.\"\"\"\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    # Supported image formats\n    extensions = ['*.jpg', '*.jpeg', '*.png', '*.bmp']\n    image_files = []\n    for ext in extensions:\n        image_files.extend(input_path.glob(ext))\n        image_files.extend(input_path.glob(ext.upper()))\n\n    print(f\"Found {len(image_files)} images\")\n\n    results = {}\n\n    for image_path in image_files:\n        print(f\"Processing {image_path.name}...\")\n\n        image = cv2.imread(str(image_path))\n        if image is None:\n            print(f\"  Failed to load {image_path.name}\")\n            continue\n\n        faces = detector.detect(image)\n        print(f\"  Found {len(faces)} face(s)\")\n\n        # Store results\n        results[image_path.name] = {\n            'num_faces': len(faces),\n            'faces': [\n                {\n                    'bbox': face.bbox.tolist(),\n                    'confidence': float(face.confidence)\n                }\n                for face in faces\n            ]\n        }\n\n        # Visualize and save\n        if faces:\n            draw_detections(\n                image=image,\n                bboxes=[f.bbox for f in faces],\n                scores=[f.confidence for f in faces],\n                landmarks=[f.landmarks for f in faces]\n            )\n\n        output_file = output_path / image_path.name\n        cv2.imwrite(str(output_file), image)\n\n    return results\n\n# Usage\nresults = process_directory(\"input_images/\", \"output_images/\")\nprint(f\"\\nProcessed {len(results)} images\")\n</code></pre>"},{"location":"recipes/batch-processing/#parallel-processing","title":"Parallel Processing","text":"<p>Use multiprocessing for faster batch processing:</p> <pre><code>import cv2\nfrom pathlib import Path\nfrom concurrent.futures import ProcessPoolExecutor, as_completed\nfrom uniface import RetinaFace\n\ndef process_single_image(image_path, output_dir):\n    \"\"\"Process a single image (runs in worker process).\"\"\"\n    # Create detector in each process\n    detector = RetinaFace()\n\n    image = cv2.imread(str(image_path))\n    if image is None:\n        return image_path.name, {'error': 'Failed to load'}\n\n    faces = detector.detect(image)\n\n    result = {\n        'num_faces': len(faces),\n        'faces': [\n            {\n                'bbox': face.bbox.tolist(),\n                'confidence': float(face.confidence)\n            }\n            for face in faces\n        ]\n    }\n\n    # Save result\n    output_path = Path(output_dir) / image_path.name\n    cv2.imwrite(str(output_path), image)\n\n    return image_path.name, result\n\ndef batch_process_parallel(input_dir, output_dir, max_workers=4):\n    \"\"\"Process images in parallel.\"\"\"\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    image_files = list(input_path.glob(\"*.jpg\")) + list(input_path.glob(\"*.png\"))\n\n    results = {}\n\n    with ProcessPoolExecutor(max_workers=max_workers) as executor:\n        futures = {\n            executor.submit(process_single_image, img, output_dir): img\n            for img in image_files\n        }\n\n        for future in as_completed(futures):\n            name, result = future.result()\n            results[name] = result\n            print(f\"Completed: {name} - {result.get('num_faces', 'error')} faces\")\n\n    return results\n\n# Usage\nresults = batch_process_parallel(\"input_images/\", \"output_images/\", max_workers=4)\n</code></pre>"},{"location":"recipes/batch-processing/#progress-tracking","title":"Progress Tracking","text":"<p>Use tqdm for progress bars:</p> <pre><code>from tqdm import tqdm\n\ndef process_with_progress(input_dir, output_dir):\n    \"\"\"Process with progress bar.\"\"\"\n    detector = RetinaFace()\n\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    image_files = list(input_path.glob(\"*.jpg\")) + list(input_path.glob(\"*.png\"))\n\n    results = {}\n\n    for image_path in tqdm(image_files, desc=\"Processing images\"):\n        image = cv2.imread(str(image_path))\n        if image is None:\n            continue\n\n        faces = detector.detect(image)\n        results[image_path.name] = len(faces)\n\n        cv2.imwrite(str(output_path / image_path.name), image)\n\n    return results\n\n# Usage\nresults = process_with_progress(\"input/\", \"output/\")\nprint(f\"Total faces found: {sum(results.values())}\")\n</code></pre>"},{"location":"recipes/batch-processing/#batch-embedding-extraction","title":"Batch Embedding Extraction","text":"<p>Extract embeddings for a face database:</p> <pre><code>import numpy as np\nfrom pathlib import Path\nfrom uniface import RetinaFace, ArcFace\n\ndef extract_embeddings(image_dir):\n    \"\"\"Extract embeddings from all faces.\"\"\"\n    detector = RetinaFace()\n    recognizer = ArcFace()\n\n    embeddings = {}\n\n    for image_path in Path(image_dir).glob(\"*.jpg\"):\n        image = cv2.imread(str(image_path))\n        if image is None:\n            continue\n\n        faces = detector.detect(image)\n\n        if faces:\n            # Use first face\n            embedding = recognizer.get_normalized_embedding(\n                image, faces[0].landmarks\n            )\n            embeddings[image_path.stem] = embedding\n            print(f\"Extracted: {image_path.stem}\")\n\n    return embeddings\n\ndef save_embeddings(embeddings, output_path):\n    \"\"\"Save embeddings to file.\"\"\"\n    np.savez(output_path, **embeddings)\n    print(f\"Saved {len(embeddings)} embeddings to {output_path}\")\n\ndef load_embeddings(input_path):\n    \"\"\"Load embeddings from file.\"\"\"\n    data = np.load(input_path)\n    return {key: data[key] for key in data.files}\n\n# Usage\nembeddings = extract_embeddings(\"faces/\")\nsave_embeddings(embeddings, \"embeddings.npz\")\n\n# Later...\nloaded = load_embeddings(\"embeddings.npz\")\n</code></pre>"},{"location":"recipes/batch-processing/#csv-output","title":"CSV Output","text":"<p>Export results to CSV:</p> <pre><code>import csv\nfrom pathlib import Path\n\ndef export_to_csv(results, output_path):\n    \"\"\"Export detection results to CSV.\"\"\"\n    with open(output_path, 'w', newline='') as f:\n        writer = csv.writer(f)\n        writer.writerow(['filename', 'face_id', 'x1', 'y1', 'x2', 'y2', 'confidence'])\n\n        for filename, data in results.items():\n            for i, face in enumerate(data['faces']):\n                bbox = face['bbox']\n                writer.writerow([\n                    filename, i,\n                    bbox[0], bbox[1], bbox[2], bbox[3],\n                    face['confidence']\n                ])\n\n    print(f\"Exported to {output_path}\")\n\n# Usage\nresults = process_directory(\"input/\", \"output/\")\nexport_to_csv(results, \"detections.csv\")\n</code></pre>"},{"location":"recipes/batch-processing/#memory-efficient-processing","title":"Memory-Efficient Processing","text":"<p>For large batches, process in chunks:</p> <pre><code>def process_in_chunks(image_files, chunk_size=100):\n    \"\"\"Process images in memory-efficient chunks.\"\"\"\n    detector = RetinaFace()\n\n    all_results = {}\n\n    for i in range(0, len(image_files), chunk_size):\n        chunk = image_files[i:i + chunk_size]\n        print(f\"Processing chunk {i//chunk_size + 1}/{(len(image_files)-1)//chunk_size + 1}\")\n\n        for image_path in chunk:\n            image = cv2.imread(str(image_path))\n            if image is None:\n                continue\n\n            faces = detector.detect(image)\n            all_results[image_path.name] = len(faces)\n\n            # Free memory\n            del image\n\n        # Optional: force garbage collection\n        import gc\n        gc.collect()\n\n    return all_results\n</code></pre>"},{"location":"recipes/batch-processing/#error-handling","title":"Error Handling","text":"<p>Robust batch processing with error handling:</p> <pre><code>import logging\nfrom pathlib import Path\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef robust_batch_process(input_dir, output_dir):\n    \"\"\"Batch process with error handling.\"\"\"\n    detector = RetinaFace()\n\n    input_path = Path(input_dir)\n    output_path = Path(output_dir)\n    output_path.mkdir(parents=True, exist_ok=True)\n\n    image_files = list(input_path.glob(\"*.[jJ][pP][gG]\"))\n\n    success_count = 0\n    error_count = 0\n\n    for image_path in image_files:\n        try:\n            image = cv2.imread(str(image_path))\n            if image is None:\n                raise ValueError(\"Failed to load image\")\n\n            faces = detector.detect(image)\n\n            cv2.imwrite(str(output_path / image_path.name), image)\n            success_count += 1\n            logger.info(f\"Processed {image_path.name}: {len(faces)} faces\")\n\n        except Exception as e:\n            error_count += 1\n            logger.error(f\"Error processing {image_path.name}: {e}\")\n\n    logger.info(f\"Completed: {success_count} success, {error_count} errors\")\n    return success_count, error_count\n</code></pre>"},{"location":"recipes/batch-processing/#next-steps","title":"Next Steps","text":"<ul> <li>Video &amp; Webcam - Real-time processing</li> <li>Face Search - Search through embeddings</li> <li>Image Pipeline - Full analysis pipeline</li> </ul>"},{"location":"recipes/custom-models/","title":"Custom Models","text":"<p>Add your own ONNX models to UniFace.</p>"},{"location":"recipes/custom-models/#add-detection-model","title":"Add Detection Model","text":"<pre><code>from uniface.detection.base import BaseDetector\nfrom uniface.onnx_utils import create_onnx_session\nfrom uniface.types import Face\nimport numpy as np\n\nclass MyDetector(BaseDetector):\n    def __init__(self, model_path: str, confidence_threshold: float = 0.5):\n        self.session = create_onnx_session(model_path)\n        self.threshold = confidence_threshold\n\n    def detect(self, image: np.ndarray) -&gt; list[Face]:\n        # Preprocess\n        input_tensor = self._preprocess(image)\n\n        # Inference\n        outputs = self.session.run(None, {'input': input_tensor})\n\n        # Postprocess\n        faces = self._postprocess(outputs, image.shape)\n        return faces\n\n    def _preprocess(self, image):\n        # Your preprocessing logic\n        pass\n\n    def _postprocess(self, outputs, shape):\n        # Your postprocessing logic\n        pass\n</code></pre>"},{"location":"recipes/custom-models/#add-recognition-model","title":"Add Recognition Model","text":"<pre><code>from uniface.recognition.base import BaseRecognizer\nfrom uniface.onnx_utils import create_onnx_session\nfrom uniface import face_alignment\nimport numpy as np\n\nclass MyRecognizer(BaseRecognizer):\n    def __init__(self, model_path: str):\n        self.session = create_onnx_session(model_path)\n\n    def get_normalized_embedding(self, image: np.ndarray, landmarks: np.ndarray) -&gt; np.ndarray:\n        # Align face\n        aligned = face_alignment(image, landmarks)\n\n        # Preprocess\n        input_tensor = self._preprocess(aligned)\n\n        # Inference\n        embedding = self.session.run(None, {'input': input_tensor})[0]\n\n        # Normalize\n        embedding = embedding / np.linalg.norm(embedding)\n        return embedding\n\n    def _preprocess(self, image):\n        # Your preprocessing logic\n        pass\n</code></pre>"},{"location":"recipes/custom-models/#register-weights","title":"Register Weights","text":"<p>Add to <code>uniface/constants.py</code>:</p> <pre><code>class MyModelWeights(str, Enum):\n    DEFAULT = \"my_model\"\n\nMODEL_URLS[MyModelWeights.DEFAULT] = 'https://...'\nMODEL_SHA256[MyModelWeights.DEFAULT] = 'sha256hash...'\n</code></pre>"},{"location":"recipes/custom-models/#use-custom-model","title":"Use Custom Model","text":"<pre><code>from my_module import MyDetector\n\ndetector = MyDetector(\"path/to/model.onnx\")\nfaces = detector.detect(image)\n</code></pre>"},{"location":"recipes/face-search/","title":"Face Search","text":"<p>Build a face search system for finding people in images.</p>"},{"location":"recipes/face-search/#build-face-database","title":"Build Face Database","text":"<pre><code>import numpy as np\nimport cv2\nfrom pathlib import Path\nfrom uniface import RetinaFace, ArcFace\n\nclass FaceDatabase:\n    def __init__(self):\n        self.detector = RetinaFace()\n        self.recognizer = ArcFace()\n        self.embeddings = {}\n        self.metadata = {}\n\n    def add_face(self, person_id, image, metadata=None):\n        \"\"\"Add a face to the database.\"\"\"\n        faces = self.detector.detect(image)\n\n        if not faces:\n            raise ValueError(f\"No face found for {person_id}\")\n\n        # Use highest confidence face\n        face = max(faces, key=lambda f: f.confidence)\n        embedding = self.recognizer.get_normalized_embedding(image, face.landmarks)\n\n        self.embeddings[person_id] = embedding\n        self.metadata[person_id] = metadata or {}\n\n        return True\n\n    def add_from_directory(self, directory):\n        \"\"\"Add faces from a directory (filename = person_id).\"\"\"\n        dir_path = Path(directory)\n\n        for image_path in dir_path.glob(\"*.jpg\"):\n            person_id = image_path.stem\n            image = cv2.imread(str(image_path))\n\n            try:\n                self.add_face(person_id, image, {'source': str(image_path)})\n                print(f\"Added: {person_id}\")\n            except ValueError as e:\n                print(f\"Skipped {person_id}: {e}\")\n\n    def search(self, image, threshold=0.6):\n        \"\"\"Search for faces in an image.\"\"\"\n        faces = self.detector.detect(image)\n        results = []\n\n        for face in faces:\n            embedding = self.recognizer.get_normalized_embedding(image, face.landmarks)\n\n            best_match = None\n            best_similarity = -1\n\n            for person_id, db_embedding in self.embeddings.items():\n                similarity = np.dot(embedding, db_embedding.T)[0][0]\n\n                if similarity &gt; best_similarity:\n                    best_similarity = similarity\n                    best_match = person_id\n\n            results.append({\n                'bbox': face.bbox,\n                'confidence': face.confidence,\n                'match': best_match if best_similarity &gt;= threshold else None,\n                'similarity': best_similarity,\n                'metadata': self.metadata.get(best_match, {})\n            })\n\n        return results\n\n    def save(self, path):\n        \"\"\"Save database to file.\"\"\"\n        np.savez(\n            path,\n            embeddings=dict(self.embeddings),\n            metadata=self.metadata\n        )\n        print(f\"Saved database to {path}\")\n\n    def load(self, path):\n        \"\"\"Load database from file.\"\"\"\n        data = np.load(path, allow_pickle=True)\n        self.embeddings = data['embeddings'].item()\n        self.metadata = data['metadata'].item()\n        print(f\"Loaded {len(self.embeddings)} faces from {path}\")\n\n# Usage\ndb = FaceDatabase()\n\n# Add faces from directory\ndb.add_from_directory(\"known_faces/\")\n\n# Save for later\ndb.save(\"face_database.npz\")\n\n# Search for person\nquery_image = cv2.imread(\"group_photo.jpg\")\nresults = db.search(query_image)\n\nfor r in results:\n    if r['match']:\n        print(f\"Found: {r['match']} (similarity: {r['similarity']:.3f})\")\n    else:\n        print(f\"Unknown face (best similarity: {r['similarity']:.3f})\")\n</code></pre>"},{"location":"recipes/face-search/#visualization","title":"Visualization","text":"<pre><code>import cv2\n\ndef visualize_search_results(image, results):\n    \"\"\"Draw search results on image.\"\"\"\n    for r in results:\n        x1, y1, x2, y2 = map(int, r['bbox'])\n\n        if r['match']:\n            color = (0, 255, 0)  # Green for match\n            label = f\"{r['match']} ({r['similarity']:.2f})\"\n        else:\n            color = (0, 0, 255)  # Red for unknown\n            label = f\"Unknown ({r['similarity']:.2f})\"\n\n        cv2.rectangle(image, (x1, y1), (x2, y2), color, 2)\n        cv2.putText(image, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n    return image\n\n# Usage\nresults = db.search(image)\nannotated = visualize_search_results(image.copy(), results)\ncv2.imwrite(\"search_result.jpg\", annotated)\n</code></pre>"},{"location":"recipes/face-search/#real-time-search","title":"Real-Time Search","text":"<pre><code>import cv2\n\ndef realtime_search(db):\n    \"\"\"Real-time face search from webcam.\"\"\"\n    cap = cv2.VideoCapture(0)\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        results = db.search(frame, threshold=0.5)\n\n        for r in results:\n            x1, y1, x2, y2 = map(int, r['bbox'])\n\n            if r['match']:\n                color = (0, 255, 0)\n                label = r['match']\n            else:\n                color = (0, 0, 255)\n                label = \"Unknown\"\n\n            cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n            cv2.putText(frame, label, (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n\n        cv2.imshow(\"Face Search\", frame)\n\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            break\n\n    cap.release()\n    cv2.destroyAllWindows()\n\n# Usage\ndb = FaceDatabase()\ndb.load(\"face_database.npz\")\nrealtime_search(db)\n</code></pre>"},{"location":"recipes/face-search/#top-k-search","title":"Top-K Search","text":"<p>Find top K matches instead of best match only:</p> <pre><code>def search_top_k(self, embedding, k=5):\n    \"\"\"Find top K matches for an embedding.\"\"\"\n    similarities = []\n\n    for person_id, db_embedding in self.embeddings.items():\n        similarity = np.dot(embedding, db_embedding.T)[0][0]\n        similarities.append((person_id, similarity))\n\n    # Sort by similarity (descending)\n    similarities.sort(key=lambda x: x[1], reverse=True)\n\n    return similarities[:k]\n\n# Usage\nquery_embedding = recognizer.get_normalized_embedding(image, face.landmarks)\ntop_matches = search_top_k(query_embedding, k=3)\n\nfor person_id, similarity in top_matches:\n    print(f\"{person_id}: {similarity:.4f}\")\n</code></pre>"},{"location":"recipes/face-search/#batch-search","title":"Batch Search","text":"<p>Search through multiple query images:</p> <pre><code>from pathlib import Path\n\ndef batch_search(db, query_dir, threshold=0.6):\n    \"\"\"Search for faces in multiple images.\"\"\"\n    all_results = {}\n\n    for image_path in Path(query_dir).glob(\"*.jpg\"):\n        image = cv2.imread(str(image_path))\n        results = db.search(image, threshold)\n\n        matches = [r['match'] for r in results if r['match']]\n        all_results[image_path.name] = matches\n\n        print(f\"{image_path.name}: {matches}\")\n\n    return all_results\n\n# Usage\nresults = batch_search(db, \"query_images/\")\n</code></pre>"},{"location":"recipes/face-search/#find-person-in-group-photo","title":"Find Person in Group Photo","text":"<pre><code>def find_person_in_group(db, person_id, group_image, threshold=0.6):\n    \"\"\"Find a specific person in a group photo.\"\"\"\n    if person_id not in db.embeddings:\n        raise ValueError(f\"Person {person_id} not in database\")\n\n    reference_embedding = db.embeddings[person_id]\n    faces = db.detector.detect(group_image)\n\n    best_match = None\n    best_similarity = -1\n\n    for face in faces:\n        embedding = db.recognizer.get_normalized_embedding(\n            group_image, face.landmarks\n        )\n        similarity = np.dot(embedding, reference_embedding.T)[0][0]\n\n        if similarity &gt; best_similarity:\n            best_similarity = similarity\n            best_match = face\n\n    if best_match and best_similarity &gt;= threshold:\n        return {\n            'found': True,\n            'face': best_match,\n            'similarity': best_similarity\n        }\n\n    return {'found': False, 'similarity': best_similarity}\n\n# Usage\ngroup = cv2.imread(\"group_photo.jpg\")\nresult = find_person_in_group(db, \"john_doe\", group)\n\nif result['found']:\n    print(f\"Found with similarity: {result['similarity']:.3f}\")\n    # Draw the found face\n    x1, y1, x2, y2 = map(int, result['face'].bbox)\n    cv2.rectangle(group, (x1, y1), (x2, y2), (0, 255, 0), 3)\n    cv2.imwrite(\"found.jpg\", group)\n</code></pre>"},{"location":"recipes/face-search/#update-database","title":"Update Database","text":"<p>Add or update faces:</p> <pre><code>def update_face(db, person_id, new_image):\n    \"\"\"Update a person's face in the database.\"\"\"\n    faces = db.detector.detect(new_image)\n\n    if not faces:\n        print(f\"No face found in new image for {person_id}\")\n        return False\n\n    face = max(faces, key=lambda f: f.confidence)\n    new_embedding = db.recognizer.get_normalized_embedding(\n        new_image, face.landmarks\n    )\n\n    if person_id in db.embeddings:\n        # Average with existing embedding\n        old_embedding = db.embeddings[person_id]\n        db.embeddings[person_id] = (old_embedding + new_embedding) / 2\n        # Re-normalize\n        db.embeddings[person_id] /= np.linalg.norm(db.embeddings[person_id])\n        print(f\"Updated: {person_id}\")\n    else:\n        db.embeddings[person_id] = new_embedding\n        print(f\"Added: {person_id}\")\n\n    return True\n\n# Usage\nupdate_face(db, \"john_doe\", cv2.imread(\"john_new.jpg\"))\ndb.save(\"face_database.npz\")\n</code></pre>"},{"location":"recipes/face-search/#next-steps","title":"Next Steps","text":"<ul> <li>Anonymize Stream - Privacy protection</li> <li>Batch Processing - Process multiple files</li> <li>Recognition Module - Model details</li> </ul>"},{"location":"recipes/image-pipeline/","title":"Image Pipeline","text":"<p>A complete pipeline for processing images with detection, recognition, and attribute analysis.</p>"},{"location":"recipes/image-pipeline/#basic-pipeline","title":"Basic Pipeline","text":"<pre><code>import cv2\nfrom uniface import RetinaFace, ArcFace, AgeGender\nfrom uniface.visualization import draw_detections\n\n# Initialize models\ndetector = RetinaFace()\nrecognizer = ArcFace()\nage_gender = AgeGender()\n\ndef process_image(image_path):\n    \"\"\"Process a single image through the full pipeline.\"\"\"\n    # Load image\n    image = cv2.imread(image_path)\n\n    # Step 1: Detect faces\n    faces = detector.detect(image)\n    print(f\"Found {len(faces)} face(s)\")\n\n    results = []\n\n    for i, face in enumerate(faces):\n        # Step 2: Extract embedding\n        embedding = recognizer.get_normalized_embedding(image, face.landmarks)\n\n        # Step 3: Predict attributes\n        attrs = age_gender.predict(image, face.bbox)\n\n        results.append({\n            'face_id': i,\n            'bbox': face.bbox,\n            'confidence': face.confidence,\n            'embedding': embedding,\n            'gender': attrs.sex,\n            'age': attrs.age\n        })\n\n        print(f\"  Face {i+1}: {attrs.sex}, {attrs.age} years old\")\n\n    # Visualize\n    draw_detections(\n        image=image,\n        bboxes=[f.bbox for f in faces],\n        scores=[f.confidence for f in faces],\n        landmarks=[f.landmarks for f in faces]\n    )\n\n    return image, results\n\n# Usage\nresult_image, results = process_image(\"photo.jpg\")\ncv2.imwrite(\"result.jpg\", result_image)\n</code></pre>"},{"location":"recipes/image-pipeline/#using-faceanalyzer","title":"Using FaceAnalyzer","text":"<p>For convenience, use the built-in <code>FaceAnalyzer</code>:</p> <pre><code>from uniface import FaceAnalyzer\nimport cv2\n\n# Initialize with desired modules\nanalyzer = FaceAnalyzer(\n    detect=True,\n    recognize=True,\n    attributes=True\n)\n\n# Process image\nimage = cv2.imread(\"photo.jpg\")\nfaces = analyzer.analyze(image)\n\n# Access enriched Face objects\nfor face in faces:\n    print(f\"Confidence: {face.confidence:.2f}\")\n    print(f\"Embedding: {face.embedding.shape}\")\n    print(f\"Age: {face.age}, Gender: {face.sex}\")\n</code></pre>"},{"location":"recipes/image-pipeline/#full-analysis-pipeline","title":"Full Analysis Pipeline","text":"<p>Complete pipeline with all modules:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface import (\n    RetinaFace, ArcFace, AgeGender, FairFace,\n    Landmark106, MobileGaze\n)\nfrom uniface.parsing import BiSeNet\nfrom uniface.spoofing import MiniFASNet\nfrom uniface.visualization import draw_detections, draw_gaze\n\nclass FaceAnalysisPipeline:\n    def __init__(self):\n        # Initialize all models\n        self.detector = RetinaFace()\n        self.recognizer = ArcFace()\n        self.age_gender = AgeGender()\n        self.fairface = FairFace()\n        self.landmarker = Landmark106()\n        self.gaze = MobileGaze()\n        self.parser = BiSeNet()\n        self.spoofer = MiniFASNet()\n\n    def analyze(self, image):\n        \"\"\"Run full analysis pipeline.\"\"\"\n        faces = self.detector.detect(image)\n        results = []\n\n        for face in faces:\n            result = {\n                'bbox': face.bbox,\n                'confidence': face.confidence,\n                'landmarks_5': face.landmarks\n            }\n\n            # Recognition embedding\n            result['embedding'] = self.recognizer.get_normalized_embedding(\n                image, face.landmarks\n            )\n\n            # Attributes\n            ag_result = self.age_gender.predict(image, face.bbox)\n            result['age'] = ag_result.age\n            result['gender'] = ag_result.sex\n\n            # FairFace attributes\n            ff_result = self.fairface.predict(image, face.bbox)\n            result['age_group'] = ff_result.age_group\n            result['race'] = ff_result.race\n\n            # 106-point landmarks\n            result['landmarks_106'] = self.landmarker.get_landmarks(\n                image, face.bbox\n            )\n\n            # Gaze estimation\n            x1, y1, x2, y2 = map(int, face.bbox)\n            face_crop = image[y1:y2, x1:x2]\n            if face_crop.size &gt; 0:\n                gaze_result = self.gaze.estimate(face_crop)\n                result['gaze_pitch'] = gaze_result.pitch\n                result['gaze_yaw'] = gaze_result.yaw\n\n            # Face parsing\n            if face_crop.size &gt; 0:\n                result['parsing_mask'] = self.parser.parse(face_crop)\n\n            # Anti-spoofing\n            spoof_result = self.spoofer.predict(image, face.bbox)\n            result['is_real'] = spoof_result.is_real\n            result['spoof_confidence'] = spoof_result.confidence\n\n            results.append(result)\n\n        return results\n\n# Usage\npipeline = FaceAnalysisPipeline()\nresults = pipeline.analyze(cv2.imread(\"photo.jpg\"))\n\nfor i, r in enumerate(results):\n    print(f\"\\nFace {i+1}:\")\n    print(f\"  Gender: {r['gender']}, Age: {r['age']}\")\n    print(f\"  Race: {r['race']}, Age Group: {r['age_group']}\")\n    print(f\"  Gaze: pitch={np.degrees(r['gaze_pitch']):.1f}\u00b0\")\n    print(f\"  Real: {r['is_real']} ({r['spoof_confidence']:.1%})\")\n</code></pre>"},{"location":"recipes/image-pipeline/#visualization-pipeline","title":"Visualization Pipeline","text":"<pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, AgeGender, MobileGaze\nfrom uniface.visualization import draw_detections, draw_gaze\n\ndef visualize_analysis(image_path, output_path):\n    \"\"\"Create annotated visualization of face analysis.\"\"\"\n    detector = RetinaFace()\n    age_gender = AgeGender()\n    gaze = MobileGaze()\n\n    image = cv2.imread(image_path)\n    faces = detector.detect(image)\n\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n\n        # Draw bounding box\n        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # Age and gender\n        attrs = age_gender.predict(image, face.bbox)\n        label = f\"{attrs.sex}, {attrs.age}y\"\n        cv2.putText(image, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n        # Gaze\n        face_crop = image[y1:y2, x1:x2]\n        if face_crop.size &gt; 0:\n            gaze_result = gaze.estimate(face_crop)\n            draw_gaze(image, face.bbox, gaze_result.pitch, gaze_result.yaw)\n\n        # Confidence\n        conf_label = f\"{face.confidence:.0%}\"\n        cv2.putText(image, conf_label, (x1, y2 + 20),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n\n    cv2.imwrite(output_path, image)\n    print(f\"Saved to {output_path}\")\n\n# Usage\nvisualize_analysis(\"input.jpg\", \"output.jpg\")\n</code></pre>"},{"location":"recipes/image-pipeline/#json-output","title":"JSON Output","text":"<p>Export results to JSON:</p> <pre><code>import json\nimport numpy as np\n\ndef results_to_json(results):\n    \"\"\"Convert analysis results to JSON-serializable format.\"\"\"\n    output = []\n\n    for r in results:\n        item = {\n            'bbox': r['bbox'].tolist(),\n            'confidence': float(r['confidence']),\n            'age': int(r['age']) if r.get('age') else None,\n            'gender': r.get('gender'),\n            'race': r.get('race'),\n            'is_real': r.get('is_real'),\n            'gaze': {\n                'pitch_deg': float(np.degrees(r['gaze_pitch'])) if 'gaze_pitch' in r else None,\n                'yaw_deg': float(np.degrees(r['gaze_yaw'])) if 'gaze_yaw' in r else None\n            }\n        }\n        output.append(item)\n\n    return output\n\n# Usage\nresults = pipeline.analyze(image)\njson_data = results_to_json(results)\n\nwith open('results.json', 'w') as f:\n    json.dump(json_data, f, indent=2)\n</code></pre>"},{"location":"recipes/image-pipeline/#next-steps","title":"Next Steps","text":"<ul> <li>Batch Processing - Process multiple images</li> <li>Video &amp; Webcam - Real-time processing</li> <li>Face Search - Build a search system</li> </ul>"},{"location":"recipes/video-webcam/","title":"Video &amp; Webcam","text":"<p>Real-time face analysis for video streams.</p>"},{"location":"recipes/video-webcam/#webcam-detection","title":"Webcam Detection","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.visualization import draw_detections\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\nprint(\"Press 'q' to quit\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Detect faces\n    faces = detector.detect(frame)\n\n    # Draw results\n    draw_detections(\n        image=frame,\n        bboxes=[f.bbox for f in faces],\n        scores=[f.confidence for f in faces],\n        landmarks=[f.landmarks for f in faces]\n    )\n\n    cv2.imshow(\"Face Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#video-file-processing","title":"Video File Processing","text":"<pre><code>import cv2\nfrom uniface import RetinaFace\nfrom uniface.visualization import draw_detections\n\ndef process_video(input_path, output_path):\n    \"\"\"Process a video file.\"\"\"\n    detector = RetinaFace()\n\n    cap = cv2.VideoCapture(input_path)\n\n    # Get video properties\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n\n    # Setup output video\n    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n    out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n\n    frame_count = 0\n\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n\n        # Detect and draw\n        faces = detector.detect(frame)\n        draw_detections(\n            image=frame,\n            bboxes=[f.bbox for f in faces],\n            scores=[f.confidence for f in faces],\n            landmarks=[f.landmarks for f in faces]\n        )\n\n        out.write(frame)\n\n        frame_count += 1\n        if frame_count % 100 == 0:\n            print(f\"Processed {frame_count}/{total_frames} frames\")\n\n    cap.release()\n    out.release()\n    print(f\"Saved to {output_path}\")\n\n# Usage\nprocess_video(\"input.mp4\", \"output.mp4\")\n</code></pre>"},{"location":"recipes/video-webcam/#fps-counter","title":"FPS Counter","text":"<p>Add frame rate display:</p> <pre><code>import cv2\nimport time\nfrom uniface import RetinaFace\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\nprev_time = time.time()\nfps = 0\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Calculate FPS\n    curr_time = time.time()\n    fps = 1 / (curr_time - prev_time)\n    prev_time = curr_time\n\n    # Detect faces\n    faces = detector.detect(frame)\n\n    # Draw FPS\n    cv2.putText(frame, f\"FPS: {fps:.1f}\", (10, 30),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n    cv2.putText(frame, f\"Faces: {len(faces)}\", (10, 70),\n                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n\n    # Draw detections\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    cv2.imshow(\"Face Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#skip-frames-for-performance","title":"Skip Frames for Performance","text":"<p>Process every N frames for better performance:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\nPROCESS_EVERY_N = 3  # Process every 3rd frame\nframe_count = 0\nlast_faces = []\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    frame_count += 1\n\n    # Only detect every N frames\n    if frame_count % PROCESS_EVERY_N == 0:\n        last_faces = detector.detect(frame)\n\n    # Draw last detection results\n    for face in last_faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    cv2.imshow(\"Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#full-analysis-pipeline","title":"Full Analysis Pipeline","text":"<p>Real-time detection with age/gender:</p> <pre><code>import cv2\nfrom uniface import RetinaFace, AgeGender\n\ndetector = RetinaFace()\nage_gender = AgeGender()\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n\n        # Draw box\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n        # Predict age/gender\n        result = age_gender.predict(frame, face.bbox)\n        label = f\"{result.sex}, {result.age}y\"\n\n        cv2.putText(frame, label, (x1, y1 - 10),\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n\n    cv2.imshow(\"Age/Gender Detection\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#gaze-tracking","title":"Gaze Tracking","text":"<p>Real-time gaze estimation:</p> <pre><code>import cv2\nimport numpy as np\nfrom uniface import RetinaFace, MobileGaze\nfrom uniface.visualization import draw_gaze\n\ndetector = RetinaFace()\ngaze = MobileGaze()\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n        face_crop = frame[y1:y2, x1:x2]\n\n        if face_crop.size &gt; 0:\n            result = gaze.estimate(face_crop)\n\n            # Draw box\n            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            # Draw gaze arrow\n            draw_gaze(frame, face.bbox, result.pitch, result.yaw)\n\n            # Display angles\n            label = f\"P:{np.degrees(result.pitch):.0f} Y:{np.degrees(result.yaw):.0f}\"\n            cv2.putText(frame, label, (x1, y1 - 10),\n                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    cv2.imshow(\"Gaze Estimation\", frame)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#recording-output","title":"Recording Output","text":"<p>Record processed video:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\n\ndetector = RetinaFace()\ncap = cv2.VideoCapture(0)\n\n# Get camera properties\nfps = 30\nwidth = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\nheight = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n\n# Setup recording\nfourcc = cv2.VideoWriter_fourcc(*'mp4v')\nout = cv2.VideoWriter('recording.mp4', fourcc, fps, (width, height))\n\nis_recording = False\n\nprint(\"Press 'r' to start/stop recording, 'q' to quit\")\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    faces = detector.detect(frame)\n\n    # Draw detections\n    for face in faces:\n        x1, y1, x2, y2 = map(int, face.bbox)\n        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n    # Recording indicator\n    if is_recording:\n        cv2.circle(frame, (30, 30), 10, (0, 0, 255), -1)\n        out.write(frame)\n\n    cv2.imshow(\"Detection\", frame)\n\n    key = cv2.waitKey(1) &amp; 0xFF\n    if key == ord('r'):\n        is_recording = not is_recording\n        print(f\"Recording: {is_recording}\")\n    elif key == ord('q'):\n        break\n\ncap.release()\nout.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#multi-camera","title":"Multi-Camera","text":"<p>Process multiple cameras:</p> <pre><code>import cv2\nfrom uniface import RetinaFace\n\ndetector = RetinaFace()\n\n# Open multiple cameras\ncaps = [\n    cv2.VideoCapture(0),\n    cv2.VideoCapture(1)  # Second camera\n]\n\nwhile True:\n    frames = []\n\n    for i, cap in enumerate(caps):\n        ret, frame = cap.read()\n        if ret:\n            faces = detector.detect(frame)\n\n            for face in faces:\n                x1, y1, x2, y2 = map(int, face.bbox)\n                cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n\n            frames.append(frame)\n\n    # Display side by side\n    if len(frames) == 2:\n        combined = cv2.hconcat(frames)\n        cv2.imshow(\"Multi-Camera\", combined)\n\n    if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n        break\n\nfor cap in caps:\n    cap.release()\ncv2.destroyAllWindows()\n</code></pre>"},{"location":"recipes/video-webcam/#next-steps","title":"Next Steps","text":"<ul> <li>Anonymize Stream - Privacy in video</li> <li>Face Search - Identity search</li> <li>Image Pipeline - Full analysis</li> </ul>"}]}